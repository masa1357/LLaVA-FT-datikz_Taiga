# Explanation_Tuning (LLM + LoRA + ZeRO)
LLMã®LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### ä½œæˆã™ã‚‹å¿…è¦ã®ã‚ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
- `LLaVa-FT-datikz/results`
```
cd LLaVa-FT-datikz
mkdir results
```

### ğŸ”§ ç’°å¢ƒå¤‰æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã¯ `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ã¦ç’°å¢ƒå¤‰æ•°ã‚’ç®¡ç†ã—ã¦ã„ã¾ã™ï¼

```
cp .env.example .env
```
.env ã‚’ç·¨é›†ã—ã¦ã€å¿…è¦ãªå€¤ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚


### Train 
```
bash scripts/train_DDP.sh
```
sbatchã§ã®å®Ÿè¡Œ
```
sbatch sbatch_scripts/sbatch_train.sh
```
