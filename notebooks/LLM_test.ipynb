{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ce1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import pprint as pp\n",
    "from logging import DEBUG\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e593d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.gradepred_data import GradePredictionDataset, GradePredictionCollator\n",
    "from src.util import set_logger, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b4c41",
   "metadata": {},
   "source": [
    "### データセット -> Collator までの可視化・分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fec6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 18:15:27,558 : LLM_test : INFO : 58 : Test_message\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "logger = set_logger(name=\"LLM_test\", level=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc699deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path.cwd().parent\n",
    "DATA_PATH = BASE_PATH / \"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e3f913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 18:15:30,781 : LLM_test : INFO : 142 : Dataset init done. users=377, qs=[1], concat=False\n",
      "2025-06-15 18:15:30,784 : LLM_test : INFO : 214 : Dataset build done: total=377, \n"
     ]
    }
   ],
   "source": [
    "# data = GradePredictionDataset(dataset_path=DATA_PATH, logger=logger)\n",
    "Q1_data = GradePredictionDataset(\n",
    "    dataset_path=DATA_PATH,\n",
    "    question_filter=[1],\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70aa6cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377\n",
      "dict_keys(['userid', 'labels', 'grades', 'L1', 'L2', 'L3', 'L4', 'L5', 'L6', 'L7', 'L8', 'L9', 'L10', 'L11', 'L12', 'L13', 'L14', 'L15'])\n",
      "Elements:\n",
      "===========================\n",
      "Element 0:\n",
      "(\"{'userid': 'C-2021-1_U1', 'labels': 4, 'grades': 'F', 'L1': {'Q1': \"\n",
      " \"'情報伝達には声、モールス信号など様々な形があり、人類の進化と共により高度に精度が高められてきた。'}, 'L2': {'Q1': \"\n",
      " \"'情報源符号化とは、情報を黒と白の○で表すことで、複合とは符号化によって得られたものを元に戻す作業のことである。それらの過程の中で、平均符号語長を短くすることが目指される。そこで一意符号可能性と瞬時復号可能性を持つ語頭符号を用いる。 \"\n",
      " \"エントロピーとは平均符号語長の下限のことである。'}, 'L3': {'Q1': \"\n",
      " \"'生起確率pの事象の生起を知ったことによる曖昧さの減少量は情報量と一致し、生起確率pが高いほど情報量は現状しまた逆もしかりである。またエントロピーは情報量の期待値と一致する。相互情報量とは、ある事象2つが相互に関連して生起した場合に得られる情報量のことである。'}, \"\n",
      " \"'L4': {'Q1': \"\n",
      " \"'通信の際に符号の誤りを検出・訂正する仕組みについて学んだ。符号の誤りは、符号語同士がs+1以上離れていればs個検出でき、2t+1離れていればt個訂正することができる。また、その距離をハミング距離と言う。'}, \"\n",
      " \"'L5': {'Q1': \"\n",
      " \"'コンピューターサイエンスは計算機科学であるともされ、全ての科学技術分野の基盤となりうるものである。コンピューター上で行われている全てのことは「計算」であり、コンピューターサイエンスはそれを制御・理解するための科学である。 \"\n",
      " \"アルゴリズムとは、関数の計算方法を示す手続きのことを言い、プログラムとはアルゴリズムをコンピューターが分かる言葉で記述したものである。アルゴリズムを上手く考えることができれば、「偽コイン発見」の問題のように作業を効率化・最適化することができる。大きな数の因数分解は難しいため、ユークリッドの互除法を用いれば容易に計算できる。'}, \"\n",
      " \"'L6': {'Q1': \"\n",
      " \"'計算ステップ数とは、アルゴリズムが終了するまでに行われる演算の数である。最大公約数の問題では剰余を考える。バブルソートとは、隣り合う要素の大小を比較しながら昇順もしくは降順に整列していくことである。2進木のソートを読むときには通りがけ順で、左の子→自分→右の子、の順番で読む。2進木の一種にヒープがある。'}, \"\n",
      " \"'L7': {'Q1': \"\n",
      " \"'マージソートとは、入力数列を分割していき、分割したものを整列して統合(マージ)するアルゴリズムである。ヒープソートと同様にO(nlogn)回の比較で要素を整列する。2進木を使用しないためよりシンプルである。最悪時の比較回数は決定木の高さに等しい。 \"\n",
      " \"線形探索が数列を先頭から探索していくのに対し、二部探索は真ん中の要素から比較して前半部分・後半部分のどちらか選んで探索していくものである。なので、後者の方が圧倒的に速い。'}, \"\n",
      " \"'L8': {'Q1': 'NaN'}, 'L9': {'Q1': 'NaN'}, 'L10': {'Q1': 'NaN'}, 'L11': \"\n",
      " \"{'Q1': 'NaN'}, 'L12': {'Q1': 'NaN'}, 'L13': {'Q1': 'NaN'}, 'L14': {'Q1': \"\n",
      " \"'NaN'}, 'L15': {'Q1': 'NaN'}}\")\n",
      "===========================\n",
      "Element 1:\n",
      "(\"{'userid': 'C-2021-1_U10', 'labels': 1, 'grades': 'B', 'L1': {'Q1': \"\n",
      " \"'高校時代で習った情報だけでは現代の進化についていけない。 情報はどんな形であっても〇と●で表され、その情報を符号化したり暗号化したりして相手に送る。 \"\n",
      " \"昔から文字や言葉を伝える様々な方法が考えられてきた。'}, 'L2': {'Q1': 'どのように情報を短く表現するか \"\n",
      " \"符号をを一意に素早く戻すためにはどのような符号にすればよいか'}, 'L3': {'Q1': '情報の量を表すためにはどのようにすればよいか。 \"\n",
      " \"2つの現象が起こった場合、相互の情報量はどのようになるか。'}, 'L4': {'Q1': '誤りの自動検出と自動訂正が可能なハミング距離 \"\n",
      " \"符号化効率とブロック誤り率'}, 'L5': {'Q1': '基礎科学としてのコンピューターサイエンス \"\n",
      " \"コンピューターサイエンスにおける計算と問題とは何か アルゴリズムとは何か'}, 'L6': {'Q1': \"\n",
      " \"'色々なソートの種類とその計算時間量や比較回数'}, 'L7': {'Q1': 'マージソートと2分探索法 比較ベースのソートアルゴリズムのまとめ'}, \"\n",
      " \"'L8': {'Q1': 'データとは何か データの予測、発見、分類・グルーピング'}, 'L9': {'Q1': \"\n",
      " \"'人工知能(AI)、特化型AIとは何か 身の回りの人工知能 人工知能の機械学習 人工知能にできないこと'}, 'L10': {'Q1': \"\n",
      " \"'非構造化データの処理 パターン認識とは何か パターン認識と機械学習'}, 'L11': {'Q1': 'データ収集とバイアス 個人情報とデータ \"\n",
      " \"オープンデータについて'}, 'L12': {'Q1': 'ベクトルによるデータ表現、ベクトルとは何か ベクトルとデータ分析の関係 \"\n",
      " \"距離や類似度とは何か、距離や類似度の種類 距離や類似度を利用したデータ分析'}, 'L13': {'Q1': 'データの可視化とは 色々な可視化方法 \"\n",
      " \"多次元データの可視化'}, 'L14': {'Q1': '相関、データの広がりと分散 統計的検定とは何か 画像処理と解析(フィルタ処理)'}, \"\n",
      " \"'L15': {'Q1': '期末テスト 授業アンケート'}}\")\n",
      "===========================\n",
      "Element 2:\n",
      "(\"{'userid': 'C-2021-1_U100', 'labels': 2, 'grades': 'C', 'L1': {'Q1': \"\n",
      " \"'情報の発展の仕方 授業の参加の仕方'}, 'L2': {'Q1': \"\n",
      " \"'情報源符号化によって情報が伝えられるとき、より軽く、瞬時に伝わるような仕組み。'}, 'L3': {'Q1': 'エントロピー、情報量について'}, \"\n",
      " \"'L4': {'Q1': '通信路符号化、ビット反転によってノイズが生じる仕組み、自動訂正自動検出について。'}, 'L5': {'Q1': \"\n",
      " \"'計算とアルゴリズムについて、効率のよい計算'}, 'L6': {'Q1': 'いろいろなソートについて。'}, 'L7': {'Q1': \"\n",
      " \"'マージソート、2分探索法について'}, 'L8': {'Q1': 'データについて'}, 'L9': {'Q1': 'AIについての基礎情報'}, \"\n",
      " \"'L10': {'Q1': '非構造データとはパターン認識とは'}, 'L11': {'Q1': \"\n",
      " \"'データの収集にはどのような方法があるのか、調査方法、バイアスとは。個人情報について。'}, 'L12': {'Q1': \"\n",
      " \"'ベクトルについて、いろいろな距離について、データにおける類似とは'}, 'L13': {'Q1': 'データの可視化について。'}, 'L14': \"\n",
      " \"{'Q1': 'データの間の関係性、相関、統計的検定について。画像解析、フィルタ処理について。'}, 'L15': {'Q1': '期末テスト'}}\")\n",
      "===========================\n",
      "Element 3:\n",
      "(\"{'userid': 'C-2021-1_U101', 'labels': 1, 'grades': 'B', 'L1': {'Q1': \"\n",
      " \"'電子教科書BookRoll、学習支援ダッシュボードの使い方の説明と、デジタル情報がどのように伝わっているかの説明、また、情報伝達の方法がどのように変化してきたか。'}, \"\n",
      " \"'L2': {'Q1': \"\n",
      " \"'情報とは、情報源を符号化し、さらに符号化した情報源を復号することによって得られるものであるということを知った。情報源の符号化のやり方によっては、元の記号列に戻すときにちゃんと戻らなかったり(一意複号可能でないもの)、瞬時複号可能性を持つ記号とそうで無い記号があることを知った。また符号の長さが平均符号長というもので表される事を知った。'}, \"\n",
      " \"'L3': {'Q1': \"\n",
      " \"'質問を繰り返して曖昧さ(選択肢)が減っていく事を「得られた情報の量」と表現し、曖昧さをU(M)で表す。また、確率Pの事象の生起を知ることにより、曖昧さが-logpになる。この事を得られる情報量という。また、得られる情報量の期待値はエントロピーに一致する'}, \"\n",
      " \"'L4': {'Q1': \"\n",
      " \"'通信路とノイズの関係について学んだ。情報をビット化して通信路に乗せて運んでいるときにノイズというものが乗ることがある。このノイズの影響でビットが反転して正しい情報が送れなくなってしまう。その対策としては1物理的にお金をかけてノイズをのりにくくすると2自動誤り検出と自動誤り訂正を用いる方法がある。誤り検出に関しては符号語同士がS+1以上離れているならばsの誤りについて検出可能であり誤り訂正に関しては符号語同士が2t+1以上離れているならばtこの誤りについて訂正可能である。'}, \"\n",
      " \"'L5': {'Q1': \"\n",
      " \"'計算機科学の対象となる問題は曖昧でない問題である。また関数の計算手続きをし目下手続きをアルゴリズムと言い、正しいアルゴリズムとは計算が有限時間内に終了するもののことを言う。また、アルゴリズムを最適化することを考える。モデルとして3のk乗枚あるうちの1枚の偽のコインを発見するためにはk回の操作が必要であるといったことがある。'}, \"\n",
      " \"'L6': {'Q1': \"\n",
      " \"'バブルソートとは隣り合う要素の大小を端から比較しながら数値の順序を並べ替える方法である。この時、整列をしたい数列の長さをnとすると、要素の比較回数は常にn(n-1)/2回であり要素の交換回数は高々n(n-1)/2回である。選択ソートとは、最大の要素を探し出し、それと最後の要素を入れ替えながら順序を並び替えることである。要素の比較回数は常にn(n-1)/2回であり最大値の更新回数は高々n(n-1)/2回であり要素の交換回数は高々n-1回である。'}, \"\n",
      " \"'L7': {'Q1': \"\n",
      " \"'マージソートとは1つの数列を数個に分割し、整列させてから、その数列を先頭から順に比較し、小さい方から順に新たな数列に格納して整列させる方法である。線形探索とはある数列に要素xが含まれているかを調べる際に、数列の先頭から順に調べる方法である。二分探索とは、ある数列に要素xが含まれているかを調べる際に、昇順または降順に並べ替えた数列の中央の値とxを比較することで要素xが全体の前半分にあるか後ろ半分にあるかを判別し、要素xが存在しない側を探索範囲から外して存在する側を新たな探索範囲としてxと中央値を比較する・・これを繰り返すことで要素xが存在するかしないかを判別する方法である。探索方法においては、線形探索より二分探索が圧倒的に早い。'}, \"\n",
      " \"'L8': {'Q1': \"\n",
      " \"'データとは物事を推測する上で、その根拠となる事実もしくはコンピューターにおいてプログラムを使った処理の対象となる記号化・数字化された資料のことを言う。データにはいろいろな種類のものがあるが、一般的に4つの形に分類できる。まず、量的データに値する比率データ・間隔データ、次に量的データに値する順位データ・カテゴリデータである。データの種類に応じて演算方法は異なる。また、データ分析は主に3つのタスクがある。予測・発見・グルーピングである。予測は過去のデータを準備・予測データをすることで行われる。しかし、予測は絶対でない。データ分析に関して相関分析という方法がある。'}, \"\n",
      " \"'L9': {'Q1': '現在使われているAIは特化型AIと呼ばれ、特定の知能だけを人工的に実現するA \"\n",
      " \"Iです。このため、特化型AIは弱いAIとも呼ばれる。現在実用化されていないAIは汎用AIといい、我々の知能と同じ柔軟さと多機能性を持つ強いAIのことです。現在私たちの身近にあるAIの例として、Siri、チャットボット、顔認証などがあります。AIを活用したビジネスとして、シェアリングエコノミー、リコメンド、サーベイランス、DXなどがある。機械学習によって、予測・認識・分類・生成などが可能になるが、AIは万能ではなく、AIには、フレーム問題や判断根拠が不明確などの欠点がある。'}, \"\n",
      " \"'L10': {'Q1': \"\n",
      " \"'構造化データとは表形式で表されるデータのことで非構造化データとは表形式で表せない画像や音などのデータのことを言う。非構造データの処理方法は色々な種類があり、言語データを処理する言語処理、画像データを処理する画像処理、音を処理する音楽処理である。また、パターン認識は、コンピューターには難しく、人間には簡単である。コンピューターによるパターン認識の基本原理は、コンピューターにあらかじめ登録し、それに似ている画像を探すことで行われる。'}, \"\n",
      " \"'L11': {'Q1': \"\n",
      " \"'調査のためのデータ収集には様々な方法があり、場合によってその方法を使い分けることが重要である。まず1つ目の方法として全数調査がある。この方法は最も理想的な方法であるが、母集団の数が多い時にこの調査方法を使うことは適していない。次に方法として、標本調査があるが、これは独自判断で行われるため標本が母集団を代表していないことが多いというデメリットがある。このことを標本選択バイアスという。メディアは街頭調査などで恣意的に標本選択バイアスなどをかけることがあるので注意しよう。また、適切に守られるデータとして個人情報がある。個人情報は名前のみならず、個人が特定されうる情報ならなんでも個人情報である。個人情報が第3者に公開される場合として、個人情報を加工した匿名加工情報がある。公開されるべきデータとして、オープンデータがあり、行政など様々なところで活用されている。'}, \"\n",
      " \"'L12': {'Q1': \"\n",
      " \"'ベクトルとは、数字のかたまりのことで、その並び順に意味を持つ。文書など様々なものをベクトルで表現することが可能である。ベクトルで表現されたデータの解析には線形代数がよく使われる。また、「距離」には様々な種類がある。最も有名な距離は「ユークリッド距離」で直線距離を表す。そのほかには道のりを表す「マンハッタン距離」、max距離。特にデータ分析における距離は自由度が高い。また、距離は条件を満たせば勝手に距離を作ることが可能である。距離は近い方が似ているが、この反対に遠い方が似ているという定義を持つ「類似度」がある。この「距離」や「類似度」に基づいてデータ解析が行われ、クラスタリングや系統分析などが行われる。長さの同じ2系列間の距離を表す「ハミング距離」、系列の長さが違っても大丈夫な「編集距離」などがある。'}, \"\n",
      " \"'L13': {'Q1': \"\n",
      " \"'今日の授業は可視化というデータを直感的に理解できるような図にする方法について学んだ。可視化の手法を選ぶ際の注意点は目的と条件によって適切なものを選択する必要があるということである。まず方法の一つ目として棒グラフがあり、数値データの比較を行う際に有効である。次にエラーバーというものがありデータの散らばり具合を表す。次にヒストグラムというものがあり頻度を表す。次に箱髭図というものがありデータの分布を比較するのに有効である。次にパイチャートというものがありデータ全体に対して各データの割合を示す際に有効である。次に折れ線グラフというものがありデータの点の間の傾向を把握する際に有効である。これらの表現方法はどれも有効なものであるが各データ間の距離を変えたらいすることによって思うようにデータの見せ方を変えることができてしまうことが欠点なので、適切なデータの取り方をすることが大事である。また、有向グラフや無向グラフなどがあり、これらは友達関係の表し方や人の動線を表す。散布図やヒートマップなどというものもある。また、多次元データを可視化するためにはデータを削減した上で表示する'}, \"\n",
      " \"'L14': {'Q1': \"\n",
      " \"'相関とは二つの量の関係性を表すものである。分布とはどんなデータがどのくらいあるか示すものであり、例として成績分布や顧客分布などがある。分布からは、最大・最小や平均・分布などがあり、分布から集団の性質がわかる。分散とは、データの散らばり具合を表す。相関には、正の相関、負の相関、夢想感があり、相関係数が正の時は正の相関、負の時は負の相関、0の時は無相関である。統計的評価とは統計的に差を評価する仕組みである。また、画像とはデータの塊である。画像は日常生活のいろいろな場面で活用されている。物体認識や物体検出、領域分割、三次元再構成などである。また、画像をフィルタ処理する方法などもある。'}, \"\n",
      " \"'L15': {'Q1': '前回の補足でした'}}\")\n",
      "===========================\n",
      "Element 4:\n",
      "(\"{'userid': 'C-2021-1_U102', 'labels': 1, 'grades': 'B', 'L1': {'Q1': \"\n",
      " \"'情報科学という学問とその中の情報に関する点についてのイントロダクション。'}, 'L2': {'Q1': \"\n",
      " \"'情報をどのようにすれば効率的に伝えることができるかについて、また、その為には数学の確立論などの知識が必要不可欠になってくることについての授業。'}, \"\n",
      " \"'L3': {'Q1': '情報量についての話とそれに絡めた確率やエントロピーの話'}, 'L4': {'Q1': \"\n",
      " \"'ノイズなどによるビット反転等の誤りを検出・訂正するためのシステムの成り立ちと仕組みついての説明。'}, 'L5': {'Q1': \"\n",
      " \"'情報をどのようにして数式で表し、さらにそれをどうやって制御するかという話。'}, 'L6': {'Q1': \"\n",
      " \"'ユークリッドの互除法の例を用いた計算量の評価とコンピュータを使って数字を特定の並びに並び直す方法をいくつかの種類紹介された。'}, 'L7': \"\n",
      " \"{'Q1': \"\n",
      " \"'前回同様何個かの種類のソートについての説明と多くのデータから特定のデータをコンピュータがどのようにして検索しているのかということに仕組みの入りの部分。'}, \"\n",
      " \"'L8': {'Q1': \"\n",
      " \"'データ分析について、まずデータ分析を大きく「予測」、「発見」、「グルーピング」の三つに分けてそれぞれについて具体例と共に説明された。'}, 'L9': \"\n",
      " \"{'Q1': 'AIに関する話で、特化型AIの基本的な仕組みとその進化について、そして現在の問題点となぜ汎用型AIの実現が困難なのかの説明。'}, \"\n",
      " \"'L10': {'Q1': \"\n",
      " \"'データには大きく分けて二種類あり、一つは構造化データともう一つは非構造化データである。今回は主に非構造化データの具体例とそれぞれのデータがどのような形でコンピュータに捉えられているのかということの説明とその応用例についての説明があった。'}, \"\n",
      " \"'L11': {'Q1': 'データを収集する際の何個かの方法とそれぞれのメリット・デメリットの説明・個人情報や著作権などの保護の話'}, 'L12': \"\n",
      " \"{'Q1': \"\n",
      " \"'データはベクトルを使って表すことができる。また、そのように表すことによってデータ同士を数学的に距離で比べることができ、その距離を比較することでデータ同士の類似度などを知ることができる。それを応用することで画像認識などの技術が発展した。'}, \"\n",
      " \"'L13': {'Q1': \"\n",
      " \"'今回はこれまで扱ってきたデータをどのようにして直感的にわかりやすくする(可視化する)ことができるかということと、その可視化の種類・それぞれの注意点などについて学んだ。'}, \"\n",
      " \"'L14': {'Q1': 'データの、相関関係の話やフィルタの話、またそれに絡めた数学の話が中心だった。'}, 'L15': {'Q1': \"\n",
      " \"'今回はこれまでの内容の総まとめの期末テストだった。春学期の最初から長かったが、テスト勉強を機にいい復習になったのでよかったと思う。'}}\")\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(Q1_data.__len__())\n",
    "pp.pprint(Q1_data.__getitem__(0).keys())\n",
    "print(\"Elements:\")\n",
    "print(\"===========================\")\n",
    "for i in range(5):\n",
    "    print(f\"Element {i}:\")\n",
    "    pp.pprint(str(Q1_data.__getitem__(i)))\n",
    "    print(\"===========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f40752e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = GradePredictionDataset(\n",
    "    dataset_path=DATA_PATH,\n",
    "    question_filter=[1, 2, 3, 4, 5],\n",
    "    concatenate=False,\n",
    "    mode=\"valid\",\n",
    "    division=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfaa991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval dataset:\n",
      "5700\n",
      "Eval dataset elements:\n",
      "Element 0:\n",
      "(\"{'userid': 'C-2021-2_U128', 'labels': 1, 'grades': 'B', 'input_text': \"\n",
      " \"'L1-Q1: \"\n",
      " \"本日の講義において「情報」は人間のコミュニケーションを円滑に行う道具として役に立っていることを学んだ。例えば、音声を伝えることに関しては私たちが小さいころしていた糸電話や昭和時代に使用されていた音をアナログ電気信号として伝えるアナログ式電話そして時は流れ私たちが使用しているデジタル式電話など情報を伝える方法は変化を遂げている。また情報通信機器では、文字や写真などの情報が0と1の並びとして表されている。また情報を伝えるうえで私たちが快適に情報を伝えるために情報源符号化、通信路符号化、暗号化などの機能もある。また情報を伝える手段を幅広く見ていくと遠くに伝えるためには大昔はほら貝やたいこなどを用いて合図を送り、7千年前~5千年前で世界四大文明が文字を発明した。日本の江戸時代は合図のほか伝書バトや飛脚で文章を送る方法や松明通信や腕木信号や手旗信号やモールス電信やバケツリレー式で情報を伝える手段は時が経つにつれて増加してきたことが分かる。増加してきたことが分かる。そして、私たちの時代には、携帯電話やファックスなどが普及する。このように情報は人々とのコミュニケーションを円滑に行う道具であり、またその情報を伝える方法も場所や時代の変化に伴い増加している。'}\")\n",
      "===========================\n",
      "Element 1:\n",
      "(\"{'userid': 'C-2021-2_U128', 'labels': 1, 'grades': 'B', 'input_text': \"\n",
      " \"'L1-Q2: \"\n",
      " \"今回の講義において、情報を伝える方法がとても多いことに驚きました。、モールス信号はスタジオジブリの「崖に上のポニョ」で知っていましたが、まだまだ自分の未知なるものがあることが分かりました。また主観的評価になってしまいますが、情報を伝える手段がどのようなものかを理解できたかと思います。これからも情報に関しての理解を深めていきたいと思います。'}\")\n",
      "===========================\n",
      "Element 2:\n",
      "(\"{'userid': 'C-2021-2_U128', 'labels': 1, 'grades': 'B', 'input_text': \"\n",
      " \"'L1-Q3: \"\n",
      " \"分からなかったことはありませんでしたが、モールス信号の演習課題がまだ完璧にできていないです。これはきちんと勉強して全部覚えられるようにしたいと思います。'}\")\n",
      "===========================\n",
      "Element 3:\n",
      "(\"{'userid': 'C-2021-2_U128', 'labels': 1, 'grades': 'B', 'input_text': \"\n",
      " \"'L1-Q4: \"\n",
      " \"情報通信機器では、文字や写真などの情報が0と1の並びとして表されているとありましたが、これは言葉が0と1という暗号に変換されたという解釈でよいのでしょうか?(確認みたいになってすみません。私はこのように解釈しましたが、間違えていたら正しい解釈をお願いいたします。)'}\")\n",
      "===========================\n",
      "Element 4:\n",
      "(\"{'userid': 'C-2021-2_U128', 'labels': 1, 'grades': 'B', 'input_text': \"\n",
      " \"'L1-Q5: \"\n",
      " \"最初は情報と聞いたらよくドラマやアニメであるような天才ハッカーがパソコンを打つときに画面に表示される英語や数字が羅列しているような(プログラミング?みたいな)ことを思いつき、そのようなことをするのかと思いついていけるか不安でしたが、今回の授業はそのような不安を除いてくれたとても親しみやすい授業でした。とはいうのもまだまだパソコンは苦手なので、先生には手間をかけるかもしれませんが私も上手く外部ツールを使いこなせるように頑張りたいと思いますのでよろしくお願い致します。'}\")\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "print(\"Eval dataset:\")\n",
    "pp.pprint(eval_dataset.__len__())\n",
    "print(\"Eval dataset elements:\")\n",
    "for i in range(5):\n",
    "    print(f\"Element {i}:\")\n",
    "    pp.pprint(str(eval_dataset.__getitem__(i)))\n",
    "    print(\"===========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe52fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GradePredictionDataset(\n",
    "    dataset_path=DATA_PATH,\n",
    "    question_filter=[1],\n",
    "    concatenate=True,\n",
    "    mode=\"train\",\n",
    ")\n",
    "eval_dataset = GradePredictionDataset(\n",
    "    dataset_path=DATA_PATH,\n",
    "    question_filter=[1],\n",
    "    concatenate=True,\n",
    "    mode=\"valid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9398667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "301\n",
      "Eval dataset:\n",
      "76\n",
      "Train dataset elements:\n",
      "Element 0:\n",
      "(\"{'userid': 'C-2022-1_U80', 'labels': 2, 'grades': 'C', 'input_text': \"\n",
      " \"'L01-Q1: 情報伝達の歴史について \"\n",
      " '〇or✖のようなシンプルな情報伝達から取り決めによってより具体的に事物を伝えられるようになり、機械の発達で高速化した。\\\\nL02-Q1: '\n",
      " '語頭符号について\\\\nL03-Q1: ハミング距離の理論によってビットの誤りを修正できる。\\\\nL04-Q1: '\n",
      " '情報漏洩を防ぐために暗号化は大事\\\\nL05-Q1: アルゴリズムについて アルゴリズムとは問題を解決するための手順や計算方法のこと '\n",
      " 'アルゴリズム体操もおなじ手順を繰り返している!!!\\\\nL06-Q1: より少ない手順で符号を並び替える方法とその大切さ\\\\nL07-Q1: '\n",
      " 'ソートについて\\\\nL08-Q1: 情報の検索について\\\\nL09-Q1: データの整理と生活とコンピュータの共同、かかわり\\\\nL10-Q1: '\n",
      " '言語などの構造化されていないデータの処理に行いて\\\\nL11-Q1: 情報の様々な抽出方法 '\n",
      " '情報をオープンデータにするためには、クリエイティブ・コモンズ・ライセンスをつけて公開しなければいけない。\\\\nL12-Q1: '\n",
      " 'データの解析において様々な距離の種類があり、用途が異なる 距離と類似度は反対の概念\\\\nL13-Q1: '\n",
      " 'データを数値化、可視化することの利点と表現方法の注意 3Dパイチャートはあまりよくない\\\\nL14-Q1: さまざまなフィルタ処理で画質を良くしている '\n",
      " \"画像処理と芸工で学ぶ技術は密接に関係している\\\\nL15-Q1: NaN'}\")\n",
      "===========================\n",
      "Element 1:\n",
      "(\"{'userid': 'C-2021-1_U81', 'labels': 3, 'grades': 'D', 'input_text': \"\n",
      " \"'L01-Q1: イントロダクション 情報通信方法\\\\nL02-Q1: NaN\\\\nL03-Q1: NaN\\\\nL04-Q1: \"\n",
      " 'NaN\\\\nL05-Q1: 効率のよいアルゴリズム 計算時間\\\\nL06-Q1: ユークリッドの互除法 ソートについて\\\\nL07-Q1: '\n",
      " 'マージソートとはどのようなアルゴリズムか 二分深索法とは\\\\nL08-Q1: NaN\\\\nL09-Q1: NaN\\\\nL10-Q1: '\n",
      " '非構造データ(文字、画像、音声など)の解析 パターン認識\\\\nL11-Q1: 調査 個人情報 オープンデータ\\\\nL12-Q1: '\n",
      " 'NaN\\\\nL13-Q1: 可視化についてどのグラフが適しているか、また表し方によるわかりにくさなど\\\\nL14-Q1: '\n",
      " \"分布、分散、相関係数について統計的検定とは統計差を評価する枠組みのことである。 画像のフィルター\\\\nL15-Q1: 画像のエッジに関して テスト'}\")\n",
      "===========================\n",
      "Element 2:\n",
      "(\"{'userid': 'C-2021-2_U2', 'labels': 4, 'grades': 'F', 'input_text': 'L01-Q1: \"\n",
      " 'NaN\\\\nL02-Q1: NaN\\\\nL03-Q1: NaN\\\\nL04-Q1: NaN\\\\nL05-Q1: 比較\\\\nL06-Q1: '\n",
      " 'NaN\\\\nL07-Q1: ヒープソート マージソート\\\\nL08-Q1: 二分探索法 データ解析\\\\nL09-Q1: AI 機械学習 '\n",
      " 'データ解析\\\\nL10-Q1: 非構造化データ解析 パターン認識\\\\nL11-Q1: データ収集 個人情報保護\\\\nL12-Q1: ベクトル 距離 '\n",
      " \"類似度\\\\nL13-Q1: NaN\\\\nL14-Q1: NaN\\\\nL15-Q1: NaN'}\")\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset:\")\n",
    "pp.pprint(train_dataset.__len__())\n",
    "print(\"Eval dataset:\")\n",
    "pp.pprint(eval_dataset.__len__())\n",
    "print(\"Train dataset elements:\")\n",
    "for i in range(3):\n",
    "    print(f\"Element {i}:\")\n",
    "    pp.pprint(str(train_dataset.__getitem__(i)))\n",
    "    print(\"===========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283ce70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d541c300dac42998fce1292fd7e38e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loading processor (use_fast=True)...\n",
      "✅ Model and processor loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\"elyza/Llama-3-ELYZA-JP-8B\", if_ZeRO=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec92d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 15:10:04,850 : CollateTrain : INFO : 58 : Test_message\n",
      "2025-06-15 15:10:04,851 : CollateEval : INFO : 58 : Test_message\n"
     ]
    }
   ],
   "source": [
    "train_logger = set_logger(name=\"CollateTrain\", level=DEBUG)\n",
    "eval_logger = set_logger(name=\"CollateEval\", level=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7163b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 15:10:04,920 : CollateTrain : INFO : 431 : Questions: [1], Q1:今日の内容を自分なりの言葉で説明してみてください\n",
      "\n",
      "2025-06-15 15:10:04,922 : CollateTrain : INFO : 445 : preamble set:\n",
      "================\n",
      "あなたは大学の教授であり、学生の成績を決定する役割を担っています。以下に示す学生の講義後アンケートを読み、成績を高い順に A、B、C、D、F のいずれかに分類してください。\n",
      "成績は出力例のような形式で出力してください。\n",
      "入力文のL は講義回、Q は質問番号を示します（例: L1-Q1）。\n",
      "アンケートの質問文は、\n",
      "Q1:今日の内容を自分なりの言葉で説明してみてください\n",
      "です。回答が NaN の場合は未回答です。\n",
      "出力には A、B、C、D、F のいずれかを含めてください。\n",
      "出力例:\n",
      "この学生の成績は、Aです。\n",
      "アンケート内容：\n",
      "================\n",
      "\n",
      "2025-06-15 15:10:04,923 : CollateEval : INFO : 431 : Questions: [1], Q1:今日の内容を自分なりの言葉で説明してみてください\n",
      "\n",
      "2025-06-15 15:10:04,924 : CollateEval : INFO : 445 : preamble set:\n",
      "================\n",
      "あなたは大学の教授であり、学生の成績を決定する役割を担っています。以下に示す学生の講義後アンケートを読み、成績を高い順に A、B、C、D、F のいずれかに分類してください。\n",
      "成績は出力例のような形式で出力してください。\n",
      "入力文のL は講義回、Q は質問番号を示します（例: L1-Q1）。\n",
      "アンケートの質問文は、\n",
      "Q1:今日の内容を自分なりの言葉で説明してみてください\n",
      "です。回答が NaN の場合は未回答です。\n",
      "出力には A、B、C、D、F のいずれかを含めてください。\n",
      "出力例:\n",
      "この学生の成績は、Aです。\n",
      "アンケート内容：\n",
      "================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_collator = GradePredictionCollator(\n",
    "    tokenizer,\n",
    "    max_tokens=4096,\n",
    "    include_target=True,\n",
    "    logger=train_logger,\n",
    "    question_filter=[1],\n",
    ")\n",
    "eval_collator = GradePredictionCollator(\n",
    "    tokenizer,\n",
    "    max_tokens=4096,\n",
    "    include_target=False,\n",
    "    logger=eval_logger,\n",
    "    question_filter=[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f278da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 15:10:05,036 : CollateTrain : DEBUG : 479 : prompt sample:\n",
      "[INST] あなたは大学の教授であり、学生の成績を決定する役割を担っています。以下に示す学生の講義後アンケートを読み、成績を高い順に A、B、C、D、F のいずれかに分類してください。\n",
      "成績は出力例のような形式で出力してください。\n",
      "入力文のL は講義回、Q は質問番号を示します（例: L1-Q1）。\n",
      "アンケートの質問文は、\n",
      "Q1:今日の内容を自分なりの言葉で説明してみてください\n",
      "です。回答が NaN の場合は未回答です。\n",
      "出力には A、B、C、D、F のいずれかを含めてください。\n",
      "出力例:\n",
      "この学生の成績は、Aです。\n",
      "アンケート内容：\n",
      "L01-Q1: これまで高等学校では情報教育において多くの改善がなされてきたがサイエンスとしての情報がないがしろにされている。大学ではこれまでと違い理系科目として「情報科学」を学んでいく。 情報伝達には情報源符号化、通信路符号化、暗号化がある。アナログからデジタルまで情報伝達の手段は大昔から変遷をたどってきたが、その根本にあるものはいつもかわらない。\n",
      "L02-Q1: 平均符号語長の出来るだけ短い符号を設計することを目的として情報源符号化を行う。ただし一\n",
      "2025-06-15 15:10:05,037 : CollateTrain : DEBUG : 480 : grade sample:  この学生の成績は、Bです。\n",
      "2025-06-15 15:10:05,050 : CollateTrain : DEBUG : 499 : Tokenization Done\n",
      "2025-06-15 15:10:05,052 : CollateTrain : DEBUG : 545 : Collating Done\n",
      "2025-06-15 15:10:05,052 : CollateTrain : DEBUG : 547 : Collate: 4 samples, max_tokens=4096\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:777\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 777\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:739\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3243 at dim 1 (got 553)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# collatorの動作を確認\u001b[39;00m\n\u001b[1;32m      2\u001b[0m example_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      3\u001b[0m     train_dataset, collate_fn\u001b[38;5;241m=\u001b[39mtrain_collator, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )  \u001b[38;5;66;03m# -> Batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/masa1357/Dockerdata/gitfile/LLaVA-FT-datikz_Taiga/notebooks/../src/gradepred_data.py:553\u001b[0m, in \u001b[0;36mGradePredictionCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollating Done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollate: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m samples, max_tokens=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28mlen\u001b[39m(features),\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens,\n\u001b[1;32m    551\u001b[0m )\n\u001b[0;32m--> 553\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: attn_mask,\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;66;03m# \"grade_str\": grades,  # デバッグ用にtarget文字列も返す\u001b[39;00m\n\u001b[1;32m    568\u001b[0m }\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3388\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3385\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3386\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:241\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    237\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:793\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    790\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    791\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    792\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "# collatorの動作を確認\n",
    "example_loader = DataLoader(\n",
    "    train_dataset, collate_fn=train_collator, batch_size=4, shuffle=True\n",
    ")\n",
    "batch = next(iter(example_loader))\n",
    "logger.info(\n",
    "    f\"Batch keys: {batch.keys()}\"\n",
    ")  # -> Batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
    "for k, v in batch.items():\n",
    "    logger.info(\n",
    "        f\"Batch key: {k}, value shape: {v.shape if isinstance(v, torch.Tensor) else type(v)}\"\n",
    "    )\n",
    "# batch内のinput_idsをでコードしてプロンプトを確認\n",
    "input_text = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
    "logger.info(f\"Input prompt: {input_text}\")\n",
    "logger.debug(f\"\\n{batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e83d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Input IDs: tensor([128000,     27,     91,   9125,     91,     29,  30591, '\n",
      " '112568,  15682,\\n'\n",
      " '        102667,  16144, 116891, 116787,   5486, 106718,  16144,  13153, '\n",
      " '116486,\\n'\n",
      " '         30512, 122074,  54926, 107734, 111017,  30512, 106529,  76947,  '\n",
      " '61689,\\n'\n",
      " '          1811, 128009, 128006,    882, 128007,  88852,  20230,  20379,  '\n",
      " '17663,\\n'\n",
      " '        106718,  16144, 114341, 104577,  74482, 113164, 102663,  84477,  '\n",
      " '30512,\\n'\n",
      " '         98499,  64121,   5486,  13153, 116486,  30512, 126574, 110480,  '\n",
      " '20230,\\n'\n",
      " '           362,   5486,     33,   5486,     34,   5486,     35,   5486,     '\n",
      " '37,\\n'\n",
      " '         97718,  16995, 101860,  33121, 111201,  17620, 104770,  39926,  '\n",
      " '72315,\\n'\n",
      " '          9174,     43, 108295, 114341, 104577,  18904,   5486,     48, '\n",
      " '108295,\\n'\n",
      " '        105284,  99397, 122532,  30512,  20379,  78434,  10110,  27452,     '\n",
      " '25,\\n'\n",
      " '           445,     16,  29342,     16, 123505, 113164, 102663,  84477,  '\n",
      " '16144,\\n'\n",
      " '        105284,  99397,  17161,  15682,   5486,     48,     16,     25, '\n",
      " '110589,\\n'\n",
      " '         16144,  44915,  30512, 105994,  26854, 111331, 112690,  16556, '\n",
      " '122984,\\n'\n",
      " '         39926,  64121,  38144,  72315,    198,  38641,   1811, 113925,  '\n",
      " '29295,\\n'\n",
      " '         33278,  97718, 126513,  39442, 113925,  38641,   9174,  20834,  '\n",
      " '48634,\\n'\n",
      " '        102052,    362,   5486,     33,   5486,     34,   5486,     35,   '\n",
      " '5486,\\n'\n",
      " '            37,  97718,  16995, 101860,  33121,  32149,  30512,  96412, '\n",
      " '107436,\\n'\n",
      " '         72315,   9174, 113164, 102663,  84477,  44915,  49543,     43,   '\n",
      " '1721,\\n'\n",
      " '         29342,     16,     25, 108825, 111629,  71289,   5486, 104500,  '\n",
      " '31958,\\n'\n",
      " '        118597, 104309, 103826, 104680,  26854,  41007,  16556,  11881,    '\n",
      " '242,\\n'\n",
      " '         55031, 114640, 101734,    236,  33035,  30512,  18655, 100472,  '\n",
      " '31431,\\n'\n",
      " '          5486, 108134,  55999,  19361,  30512,  39926, 105898,   9174,     '\n",
      " '43,\\n'\n",
      " '          2437,  29342,     16,     25, 104966,  95221, 104309,  86127, '\n",
      " '104067,\\n'\n",
      " '         29295,  50928, 100934, 108134,  30512, 101828, 111199,  16144,  '\n",
      " '79059,\\n'\n",
      " '             7, 104662, 109709,  16144, 101182,  61304,  18476,      8,  '\n",
      " '20230,\\n'\n",
      " '         22324,  50834, 109033, 105262, 106592,  39404,  18476,  33208,   '\n",
      " '5486,\\n'\n",
      " '         39404,  18476,  33208,  56051, 102944,  30512,  24186,  16144, '\n",
      " '108134,\\n'\n",
      " '         20230, 114941,  17663, 106592, 109095,  18476, 102677,   1811,  '\n",
      " '39404,\\n'\n",
      " '         18476,  33208,  54926, 126101, 108608, 103378,  79059,  16144,   '\n",
      " '9039,\\n'\n",
      " '         30512, 114485,  33503,  17663, 118733,  86127, 104067,  29295, '\n",
      " '108134,\\n'\n",
      " '         30512, 105755, 103856,  47884, 101682,  76622,  18655, 100472,  '\n",
      " '31431,\\n'\n",
      " '          5486,  37705, 100472,  31431,  54926,  30926,  29295, 108608,   '\n",
      " '1811,\\n'\n",
      " '        104662, 109709,  30512,  49792, 101908,  39926,  23187, 109315,  '\n",
      " '30926,\\n'\n",
      " '         16556,  79059,  16144,   9039,  30512, 114485,  33503,  17663,  '\n",
      " '30926,\\n'\n",
      " '          5486,  59739, 109709, 115274,  39404,  18476, 102158, 101544,  '\n",
      " '30512,\\n'\n",
      " '        106649,  47884,  54926,  30926,  29295, 108608, 118855,  15120,  '\n",
      " '37689,\\n'\n",
      " '         20230,   5486, 112661, 126101, 109095,  18476, 106084, 123207,  '\n",
      " '86127,\\n'\n",
      " '        104067,  29295, 108134,  30512,  37656, 106624, 103856,  47884, '\n",
      " '101682,\\n'\n",
      " '         76622,  18655, 110615,   1811, 110924,  16556,  33655, 126832, '\n",
      " '101200,\\n'\n",
      " '        110276, 112973, 102158, 103892,  39404,  18476,      7, 112661,  '\n",
      " '39177,\\n'\n",
      " '        109095,  18476,  88367,  39404,  18476,      8, 116787,   5486,  '\n",
      " '39404,\\n'\n",
      " '         18476,  30512,  98499,  64121, 105721,  58942, 104533, 112661, '\n",
      " '126101,\\n'\n",
      " '        109095,  18476,  54926,  30926,  29295, 108608,   1811, 103192,  '\n",
      " '17129,\\n'\n",
      " '        102800,  39404,  18476, 102158,  15682,  58426,  67645, 112384,  '\n",
      " '16144,\\n'\n",
      " '         34171, 105284,  32977,  69978, 100472, 102944,  16556, 112160,  '\n",
      " '37689,\\n'\n",
      " '        109095,  18476,  88367, 102158,  30512, 113727, 104984,   5486, '\n",
      " '112361,\\n'\n",
      " '        101544,  30813,  16144, 102158, 103892,  39404,  18476, 115991,   '\n",
      " '5486,\\n'\n",
      " '        102158, 103892,  39404,  18476,  55031, 115274,  39404,  18476, '\n",
      " '102158,\\n'\n",
      " '        101544,  16144, 106649,  16995, 102944,  30512, 102647, 103854,  '\n",
      " '81219,\\n'\n",
      " '          5486, 103192,  17129, 102800,  39404,  18476, 102158,  30512,  '\n",
      " '50928,\\n'\n",
      " '         30369,  30926,  29295, 108608,   9174,     43,   2839,  29342,     '\n",
      " '16,\\n'\n",
      " '            25,  49940,     99,  18476,  33208, 103646, 108134,  15682, '\n",
      " '118597,\\n'\n",
      " '         16144, 103188,  39607,  16556, 103826, 104680,  26854,  83799,  '\n",
      " '16556,\\n'\n",
      " '        101335, 122624, 114475, 107441,  91482,  67075,  29295,  95543, '\n",
      " '108436,\\n'\n",
      " '         54926,  30926, 107407,   1811,  91482,  67075,  29295,  95543, '\n",
      " '108436,\\n'\n",
      " '         54926, 105184,  96455,  16144, 106592,  95543, 108436, 105184,  '\n",
      " '96455,\\n'\n",
      " '         19732, 112844,   1811,  91482,  67075,  30512, 118565,  48552,  '\n",
      " '48154,\\n'\n",
      " '         19732,  20230,  17620, 107441,  13828,    245, 101544,  33208,  '\n",
      " '54926,\\n'\n",
      " '         30926,  15682,   5486,  13372, 112772, 101198, 120389, 102750, '\n",
      " '103202,\\n'\n",
      " '        116825, 107441, 105324, 104369, 105933,  30926,  16556,   5486,  '\n",
      " '91482,\\n'\n",
      " '         67075,  16144,  95543, 108436,  29295, 125119,  76947, 102532,  '\n",
      " '32977,\\n'\n",
      " '        115681, 111800,  20834,  19732, 115681, 116952,  37656,  29295,  '\n",
      " '88367,\\n'\n",
      " '        115717, 126172,  47884, 109020,   1811, 103721,    108,  31431, '\n",
      " '104196,\\n'\n",
      " '         15024,  39404,  18476,  15682,   5486,  39404,  18476,  33208,  '\n",
      " '54926,\\n'\n",
      " '        110018,  20230,  15120, 118821,  39404,  18476,  30512, 103721,    '\n",
      " '108,\\n'\n",
      " '         31431, 104196,  15024,  89046, 106307,  16556, 103721,    108,  '\n",
      " '31431,\\n'\n",
      " '        104196,  39926, 109095,  18476,  54926, 126101,  91482,  67075,  '\n",
      " '29295,\\n'\n",
      " '         95543, 108436,  39926, 102532,  32977,  43240,   9039, 104662,  '\n",
      " '16556,\\n'\n",
      " '         39404,  18476,  30512, 116952,  37656, 108608, 102944, 103195, '\n",
      " '119520,\\n'\n",
      " '          5486,  91482,  67075,   9039,  29295,  43240,  47884, 109020, '\n",
      " '103846,\\n'\n",
      " '         37705,  30369, 110115,  82973,  29295,  32149, 126469, 114732,   '\n",
      " '9174,\\n'\n",
      " '            43,   2371,  29342,     16,     25,  49854,    122,  47523, '\n",
      " '112258,\\n'\n",
      " '           113, 109163,  18476, 107118,   5486, 115303, 118821, 112258,    '\n",
      " '113,\\n'\n",
      " '         16556, 108134,  30512,  71289,  31431, 108167,  54926, 102944,  '\n",
      " '16556,\\n'\n",
      " '         57207,  38248,    114,  11972, 109163,  18476,  71289, 110904,  '\n",
      " '15120,\\n'\n",
      " '        109033,  19113, 109163,  18476, 104309,  29295,  96412,  17129, '\n",
      " '104028,\\n'\n",
      " '         29295,   5486, 112258,    113,  30512,  67645, 119775, 105509,  '\n",
      " '20230,\\n'\n",
      " '        101682,  22023,  30046,  20230, 110968,  17663,  32149, 102677, '\n",
      " '112258,\\n'\n",
      " '           113,  55487,  37705, 109606,  29295,  21990, 100204,  30369,   '\n",
      " '1811,\\n'\n",
      " '        103359,  39926,  66776, 103359,  25666, 112258,    113, 109163,  '\n",
      " '18476,\\n'\n",
      " '         15682, 109163,  18476,  33208,  19732, 109095,  18476,  33208,  '\n",
      " '16556,\\n'\n",
      " '        108901, 109020, 112258,    113,  30512,  38129,  54926,  42634, '\n",
      " '125135,\\n'\n",
      " '        120950, 102944, 103195,   1811, 101682,  22023,  30046,  15682, '\n",
      " '115364,\\n'\n",
      " '        121298, 112258,    113,  30512, 121298,  39926,  33334,  47884,   '\n",
      " '1811,\\n'\n",
      " '         37705,  22023,  30046,  15682, 101682,  22023, 112919, 121298, '\n",
      " '112258,\\n'\n",
      " '           113,  16556, 109163,  18476,  33208,  39926, 101682,  22023,  '\n",
      " '30046,\\n'\n",
      " '         20230,  37705,  30369,   1811,  74664, 109163,  18476,  15682,   '\n",
      " '5486,\\n'\n",
      " '        119869,  16144, 121298, 112258,    113, 109163,  18476, 116787,   '\n",
      " '5486,\\n'\n",
      " '        105104,  28713, 118821,  27384, 112495,  72238,   9039,  16144,  '\n",
      " '89783,\\n'\n",
      " '        111331, 102616,  30512,  78698, 105262, 113637, 109163,  18476,  '\n",
      " '30512,\\n'\n",
      " '         50338,  98499,  54926, 102052, 109243,  28741, 112258,    113,  '\n",
      " '30512,\\n'\n",
      " '         17701,  46034,  21105,    103, 118145, 105110, 100604,   1811, '\n",
      " '110022,\\n'\n",
      " '         40265, 115926, 121551,  71289,   5486,     45,  16144,  63212,   '\n",
      " '9039,\\n'\n",
      " '         17620,  50338,  30512,  32018, 109315,  41007, 104309, 107407,  '\n",
      " '29295,\\n'\n",
      " '          5486, 108008,  20230,  82973,  29295,  32149, 117295,  97999, '\n",
      " '121298,\\n'\n",
      " '        112258,    113,     45,  16144, 127268, 109904,  82973,    340,     '\n",
      " '43,\\n'\n",
      " '          2304,  29342,     16,     25,  68042, 118613,  72369, 120770,  '\n",
      " '17905,\\n'\n",
      " '         16556,  23039,  78183, 115032, 109335,   5486, 122617, 122282, '\n",
      " '116787,\\n'\n",
      " '          5486, 122282,  30512, 113954,  39926,  30297,  17129,  47884, '\n",
      " '107740,\\n'\n",
      " '         54926, 113009, 101182, 112973, 109713,  70563,  72369, 108337, '\n",
      " '109054,\\n'\n",
      " '         76739, 107446, 103195,   1811,  80071,    237, 104482, 107118, '\n",
      " '118687,\\n'\n",
      " '        112535,  16906,    111,  28741,  20230,  23187, 104577, 103646,  '\n",
      " '97518,\\n'\n",
      " '          9039,  16144, 100909, 103195,   1811, 122282, 107118,   5486, '\n",
      " '109606,\\n'\n",
      " '         30926,  27452,  29295,  58318,  58942, 124195,  58426, 103899,  '\n",
      " '58942,\\n'\n",
      " '         30512,  32018, 109315, 100909, 103195,   1811, 102741, 117675,  '\n",
      " '94550,\\n'\n",
      " '        107118,  88435,  16144, 105360, 103854, 109913,  58942,  16144, '\n",
      " '109606,\\n'\n",
      " '         16144, 100909, 103195,   1811, 113853,   9039,  16144, 122282,  '\n",
      " '41007,\\n'\n",
      " '         30512,  20379,  17663,  46034, 106307,  16144, 106592, 107908, '\n",
      " '115054,\\n'\n",
      " '         78525,  91062,   5486, 109713,  70563,  72369, 108337,  29295,  '\n",
      " '78183,\\n'\n",
      " '        117295, 112690,  16556, 107908, 115054,  78525,  91062, 123410, '\n",
      " '106592,\\n'\n",
      " '         57326,  77750,  32131,  91062,  19732, 120267,   5486, 114754, '\n",
      " '109713,\\n'\n",
      " '         70563,  72369, 108337,  20230, 100927,  58942, 104526, 104004,   '\n",
      " '1811,\\n'\n",
      " '        107908, 115054,  78525,  91062,  15682, 102422,  82973,  16556, '\n",
      " '105721,\\n'\n",
      " '         78183,  30369,  32977,  20834, 123207, 105110, 100604,   1811, '\n",
      " '105864,\\n'\n",
      " '           230,  70203, 106826, 103830,   9039, 107118, 107908, 115054,  '\n",
      " '78525,\\n'\n",
      " '         91062,  29295, 105721,  35287,  54926, 103296, 120650, 126832, '\n",
      " '122282,\\n'\n",
      " '         18904,   9039, 103195,   9174,     43,   2705,  29342,     16,     '\n",
      " '25,\\n'\n",
      " '        120795,  84477, 107118, 101182, 110480,  33144,  20230, 123532,  '\n",
      " '76947,\\n'\n",
      " '        105360, 103854, 109913, 105262, 100909, 103195,   1811,  16325,  '\n",
      " '72661,\\n'\n",
      " '         66953, 112193, 102741,  84477,  15682,  37795,     96,  31431,  '\n",
      " '40862,\\n'\n",
      " '         30297,  31634,  72238,  16144,  93132,  30512, 125261,  21105,    '\n",
      " '103,\\n'\n",
      " '         29295,  33503,  64531,  32218,  54926, 100909, 103195,   1811, '\n",
      " '108914,\\n'\n",
      " '        102754,    107, 102741,  84477,  15682, 112366,  16144,  31634,  '\n",
      " '72238,\\n'\n",
      " '         30512, 106767,  15024, 108048,  19732, 114626,  16144,  31634,  '\n",
      " '72238,\\n'\n",
      " '         30512, 125639, 109913, 107290,  64531,  32218,  54926, 100909, '\n",
      " '103195,\\n'\n",
      " '          1811, 102741,  84477, 112649, 117295,  82973,  15682,  17701,  '\n",
      " '48634,\\n'\n",
      " '         16144, 101544,  30813,  20230, 113468, 124500,  30369, 103846,   '\n",
      " '5486,\\n'\n",
      " '         17701,  48634,  16144, 101544,  30813,     77,  16144,  97518,   '\n",
      " '9039,\\n'\n",
      " '        103306,  78698, 105262,   1811,  41920, 103130,  76505, 124905, '\n",
      " '100472,\\n'\n",
      " '        102741,  84477,  15682, 121640,  29295,  76622, 110480,      7, '\n",
      " '107792,\\n'\n",
      " '         28542,  16144, 112603,  30512,  78659,   9458, 105994,   9458,  '\n",
      " '65917,\\n'\n",
      " '         16144, 110480,  16556,  98499, 104004,   1811, 114941, 100472, '\n",
      " '126101,\\n'\n",
      " '         98499, 104004,  25197, 126989,   1811,      8,  16556,  98499, '\n",
      " '104004,\\n'\n",
      " '        119520,   5486,  17701,  48634,   9039,  32218,  16144, 105360, '\n",
      " '102802,\\n'\n",
      " '         24273, 113468,  15682,  41920, 103130,  76505,  29295, 102987, '\n",
      " '115737,\\n'\n",
      " '        107462, 126145,   5486, 125261,  18904,   9039,  29295,  43240,  '\n",
      " '47884,\\n'\n",
      " '        109020, 103846, 122282,  20230,  82973,  29295,  32149, 117295,   '\n",
      " '1811,\\n'\n",
      " '        110924,  16556,   5486, 101182,     17, 118821,  77195,  30512, '\n",
      " '112624,\\n'\n",
      " '         28713,  17663,  41920, 103130,  76505,  30512,  11883, 102334, '\n",
      " '107461,\\n'\n",
      " '        107610, 102741,  84477,  30512,  38129,  54926, 118733,  17701,  '\n",
      " '48634,\\n'\n",
      " '          9039,  32218,  16144, 105360, 102802, 113468,  82973,  29295, '\n",
      " '124500,\\n'\n",
      " '         30369, 109335,  50338,  33420, 107001,   9174,     43,   2589,  '\n",
      " '29342,\\n'\n",
      " '            16,     25, 125263,  14558,  30512, 106154,  31431, 106272, '\n",
      " '107461,\\n'\n",
      " '        107610, 108176, 104466,     17, 103130,  76505, 126694, 107462,  '\n",
      " '50834,\\n'\n",
      " '          5486, 102831,  30512,  19732,  31431, 116497,  39926, 114626,  '\n",
      " '20230,\\n'\n",
      " '        116155,  56051, 105065,  30512, 102831,  16144, 105494, 109156, '\n",
      " '106824,\\n'\n",
      " '        107461, 107610, 108176, 104466,  77195, 104088, 100481,  47884,  '\n",
      " '19967,\\n'\n",
      " '        101275,  30512, 103721,    108,  31431, 104196,  39926,  23039,  '\n",
      " '76947,\\n'\n",
      " '         17701,  48634,  56051,   9039,  32218,  30512, 102741,  84477, '\n",
      " '109768,\\n'\n",
      " '         47884, 102944,  30512, 107461, 107610, 102741,  84477, 102677,   '\n",
      " '1811,\\n'\n",
      " '        119448,  16995, 115844,  16144,  17701,  48634,   9039,  32218,  '\n",
      " '16144,\\n'\n",
      " '         39177,     17, 103130,  76505,  16144, 110219,  16144, 102741,  '\n",
      " '84477,\\n'\n",
      " '        126036,  64889,  61994, 103856,  47884, 109020, 115397, 118821,  '\n",
      " '17701,\\n'\n",
      " '         48634,   9039,  32218,  30512,  31809,  30813,  26854,   9039,  '\n",
      " '32218,\\n'\n",
      " '         20230,  17620, 111017,  39926,  64531,  32218,  15024, 105721,  '\n",
      " '78183,\\n'\n",
      " '        100472, 123090,  68759,  78767,      7, 105156,  40862,      8, '\n",
      " '109768,\\n'\n",
      " '         47884, 102944,  30512,  68759,  78767, 102741,  84477, 102677,   '\n",
      " '1811,\\n'\n",
      " '          5195, 119775, 108134, 111800,  52084,  54926,  39177, 102052, '\n",
      " '102741,\\n'\n",
      " '         84477, 103646, 119526,  16144,  89151,  25827,  16325,  16144,  '\n",
      " '31634,\\n'\n",
      " '         72238,  19732, 125261,  54926, 106592, 103721,    108,  31431, '\n",
      " '104196,\\n'\n",
      " '         39926,  58254,    252, 107462,  47884,  41007,  30512,     17,  '\n",
      " '17620,\\n'\n",
      " '        106767,  52084, 102677,   9174,     43,   2318,  29342,     16,     '\n",
      " '25,\\n'\n",
      " '        110045, 108337, 102052, 118969,  75694,  17129,  26854, 104091, '\n",
      " '104770,\\n'\n",
      " '        107407,   1811,  33857,   9554, 119526, 103306,  57106,  96455, '\n",
      " '119526,\\n'\n",
      " '         15682,  18028,   5486,   4235,   5486, 123052,   5486,     10,  '\n",
      " '29295,\\n'\n",
      " '        108608,   1811,  27452, 116285,  33014,  30358,  71289,   8107, '\n",
      " '110226,\\n'\n",
      " '        104309, 118855,  56965, 113849, 119526,  15682,  18028,   5486, '\n",
      " '123052,\\n'\n",
      " '         15682, 118765,  29295,     10,   5486,   4235,  15682, 108608,   '\n",
      " '1811,\\n'\n",
      " '         27452, 116285,  99046,    224, 105514, 126287,  71289,  61786,  '\n",
      " '88399,\\n'\n",
      " '            99,   8107, 104309,   1811, 105284,   9554, 119526, 103306,   '\n",
      " '5486,\\n'\n",
      " '        110480,  25129, 119526,  15682,   5486,  18028,   5486, 123052,   '\n",
      " '5486,\\n'\n",
      " '            10,   5486,   4235,  15682, 120837, 115552, 119703,  29295, '\n",
      " '110904,\\n'\n",
      " '         20230, 105360, 103854,  30369, 109335, 108608,   1811,  27452, '\n",
      " '116285,\\n'\n",
      " '        113164, 102663,  84477,  16144,     20,  38574, 106090, 107624,  '\n",
      " '95001,\\n'\n",
      " '        104309, 118855,   5486, 118455, 119526,  15682, 117384, 122532, '\n",
      " '119775,\\n'\n",
      " '        115707, 112535,  83687,  29295, 105360, 126559, 103378,  16144, '\n",
      " '119526,\\n'\n",
      " '        103195,   1811, 110045, 108337, 106596,  15682,   9080,  40053, '\n",
      " '112535,\\n'\n",
      " '         23039,  26854,  76947, 108086,   5486, 103911, 108867,   5486, '\n",
      " '102404,\\n'\n",
      " '         91774,   5486, 118953,  25005,    242,  94550,  20230,  17620,  '\n",
      " '32149,\\n'\n",
      " '        115032,   9174,     43,   2545,  29342,     16,     25, 115334,  '\n",
      " '33710,\\n'\n",
      " '         25005,    242,  94550,  15682, 119526,  30512,  16995,  47884,  '\n",
      " '59739,\\n'\n",
      " '        116421, 118953, 107610,  20230,  17620, 105784, 100909, 118844, '\n",
      " '123410,\\n'\n",
      " '        118733, 127749, 102178, 111665,  29295, 119237, 108176,   1811, '\n",
      " '107059,\\n'\n",
      " '        105335, 122835,  15682, 104409,  28713, 119526,  30512, 107059, '\n",
      " '115332,\\n'\n",
      " '             7,  32149,  28713, 109709,      8,  20230, 123011, 109315, '\n",
      " '100909,\\n'\n",
      " '          1811, 118953,  25005,    242,  94550,  19732, 108010,  76947, '\n",
      " '104409,\\n'\n",
      " '         28713, 102944,  30512, 123011, 109315, 105879,  31958, 105184,  '\n",
      " '26854,\\n'\n",
      " '         75146, 107253, 120937,   1811, 107059, 115332, 107809, 106442,  '\n",
      " '20230,\\n'\n",
      " '         82912, 103792, 102944,  29295,  58426, 107059, 115332,  16144, '\n",
      " '106691,\\n'\n",
      " '        109904, 102944,  30512, 115552, 103792,   1811, 102362,  49792,  '\n",
      " '53283,\\n'\n",
      " '         27327,   4444,     40,      8,  15682, 114516,  16144,  53283,  '\n",
      " '27327,\\n'\n",
      " '         30512,  89151, 104409, 103792, 101513, 109472,  16144, 100909, '\n",
      " '109807,\\n'\n",
      " '         16325,  72661,  66378,  33208,  25287,  15836,  15682, 112599, '\n",
      " '108285,\\n'\n",
      " '         47884,  33655,  78183, 115032,  15836,  16556,  66378,  23187,  '\n",
      " '16144,\\n'\n",
      " '        121432,  67016,  69978, 107462, 100604, 105879,  13177, 112294,  '\n",
      " '16995,\\n'\n",
      " '         15836,  66383, 127173, 104028,   1811,  21980,    236,  11883,  '\n",
      " '25287,\\n'\n",
      " '         15836,  15682, 114516,  19732, 112361, 115289, 122279,  30813,  '\n",
      " '19732,\\n'\n",
      " '         43240, 121432,  34171, 114792,  59739, 105879,  13177, 104195,  '\n",
      " '16995,\\n'\n",
      " '         15836,  66383, 127173, 104028,   1811,  15836,  15682,  96356,  '\n",
      " '16144,\\n'\n",
      " '         18904, 111331,  16995,  85791, 103171, 110229,  16556,  76706,  '\n",
      " '11883,\\n'\n",
      " '        110731,  97999,  47043,  16144,  44971,  47260,  39850, 102860,  '\n",
      " '71289,\\n'\n",
      " '         65299, 112164,  11972,  25197, 109089, 104309,      8,  15836,  '\n",
      " '32977,\\n'\n",
      " '        114516,  19732, 112361, 104466,  48864, 108710,  54926,   1811,  '\n",
      " '48864,\\n'\n",
      " '        108710,  54926, 110018,  20230, 121771, 115867,  66953,  37823,  '\n",
      " '76739,\\n'\n",
      " '         38248,    115,  78205,  16073,  19732,  33857,  16144, 119526,  '\n",
      " '30512,\\n'\n",
      " '         58318, 105262,  30926,  29295, 107693, 103195,   1811,  15836,  '\n",
      " '15682,\\n'\n",
      " '         97518,   9039, 120950, 102944,  16556, 127091,  17701, 104028,  '\n",
      " '19732,\\n'\n",
      " '        127091, 113702, 125334,  38144, 109749,   1811, 112599,  16144,  '\n",
      " '15836,\\n'\n",
      " '         15682, 112185,  37087,  61994, 108038,  26854, 102944, 111769,   '\n",
      " '9174,\\n'\n",
      " '            43,    605,  29342,     16,     25, 115304, 106391,  67178,  '\n",
      " '33208,\\n'\n",
      " '        119526, 107118,  21405, 115707, 101983, 110104,  83125,  71289, '\n",
      " '115240,\\n'\n",
      " '          5486,  79785, 123160, 119526, 116787,   5486,  78183,  28713,  '\n",
      " '56051,\\n'\n",
      " '         43514,  16144,  96356,  60358,  20230, 121771,  98369, 101182, '\n",
      " '119526,\\n'\n",
      " '        103195,   1811, 102832, 110548,  33710,  15682,  83687,  16144, '\n",
      " '103214,\\n'\n",
      " '         64121,  16144, 100909, 103195,   1811,     18,  33671,  24186,  '\n",
      " '17129,\\n'\n",
      " '         77181, 117734,  16144,  39177, 119775, 124283, 104371,  16556,  '\n",
      " '21405,\\n'\n",
      " '        122101,  29295,  64803,  33671,  24186,  23897, 104989,  15682,  '\n",
      " '21405,\\n'\n",
      " '         72342, 100604,   1811, 115240,  15682, 102832, 110548,  33710,      '\n",
      " '7,\\n'\n",
      " '         83687,  16144, 103214,  64121,      8, 103195,   1811, 102987, '\n",
      " '114050,\\n'\n",
      " '        118331, 108994,  33710, 118823, 102756, 105048,  15682,  99849,  '\n",
      " '33671,\\n'\n",
      " '         24186, 102944, 119526,  30512, 108881,  31431, 103302, 102639, '\n",
      " '100481,\\n'\n",
      " '         76947,   5486,  22398,  80805,  26269,  19732, 102769,  76947,  '\n",
      " '17620,\\n'\n",
      " '        104770,  54926,  25197, 126989,   1811,  67645, 119775, 108881, '\n",
      " '107462,\\n'\n",
      " '         47884,  32149,  15682, 109713,  70563,  72369, 108337,  29295,  '\n",
      " '48864,\\n'\n",
      " '        104127,  16995,  47884,   9174,     43,    806,  29342,     16,     '\n",
      " '25,\\n'\n",
      " '        110045, 108337,  30512, 110226,  43167,  54926, 110018,  15120,  '\n",
      " '87217,\\n'\n",
      " '         22649, 101067, 109904, 112973,  37087,  43167, 104873, 112984, '\n",
      " '103195,\\n'\n",
      " '        113637, 110018,  37087,  38144,  16144, 104873, 112984, 103359,  '\n",
      " '47523,\\n'\n",
      " '         15682, 103760,  43167, 108638,  19732, 127173, 104028,   1811,  '\n",
      " '37087,\\n'\n",
      " '          9039, 104873, 112984,  15682,  50928, 109115, 119526,  29295,  '\n",
      " '89151,\\n'\n",
      " '        103350, 103195, 109335,  39850,  37823,  67075, 109846, 102321, '\n",
      " '103350,\\n'\n",
      " '        112535, 119526,   9039,  29295, 127181,  19732,   8239,    112,  '\n",
      " '48634,\\n'\n",
      " '         29295, 119448, 115737,  31431,   5486,  16937,  88367,  16556,  '\n",
      " '30297,\\n'\n",
      " '        103195,  30926,  29295,  68408,  39850,  37823,  67075, 103195,   '\n",
      " '1811,\\n'\n",
      " '        103760,  43167, 108638, 112028,  34048,  30512, 108167,  20834,  '\n",
      " '39926,\\n'\n",
      " '        104873, 112984,  54926, 106592, 105888,  22656, 104873, 112984, '\n",
      " '102677,\\n'\n",
      " '          1811, 105888,  22656,  16144, 116602,  20834,  41007, 102052, '\n",
      " '104873,\\n'\n",
      " '        112984,  16144,  98897,  55723,  30046,  29295, 106063,  37026,  '\n",
      " '16144,\\n'\n",
      " '        122225,  16556, 116602,  20834,  54926,  19361,  37689, 116602,  '\n",
      " '20834,\\n'\n",
      " '         19732,  93488,  99959,  91062,  20230, 116602,  20834,  54926,  '\n",
      " '43568,\\n'\n",
      " '         19967, 101399, 116602,  20834,  25333, 107407,   1811,  19361,  '\n",
      " '37689,\\n'\n",
      " '        116602,  20834,  25333,  15682, 119526, 110226,  43167,  29295, '\n",
      " '105465,\\n'\n",
      " '        109846, 105888,  22656, 121507, 121960, 115598,  31431,  29295,  '\n",
      " '16556,\\n'\n",
      " '         30297,  30369,   1811,  94632, 101335,  57933,  38248,    115,  '\n",
      " '78205,\\n'\n",
      " '         16073, 107118,  15836,  30512, 107740,  54926, 110018,  20230, '\n",
      " '114516,\\n'\n",
      " '         29295,  50834, 109013, 122984,  16144, 100909,   1811, 121503,  '\n",
      " '17792,\\n'\n",
      " '        108134,  15682, 112424, 112212, 123050, 110731,  29295, 116898,  '\n",
      " '47884,\\n'\n",
      " '         59739,  32149,  27452,  48915, 107407,   1811, 104364, 103283, '\n",
      " '109904,\\n'\n",
      " '        115598,  91774, 107294, 105722,  30369, 105933, 121568, 108134,  '\n",
      " '15682,\\n'\n",
      " '         31634,  55487, 101557,    106, 121568, 108134, 103306, 108167, '\n",
      " '121247,\\n'\n",
      " '        126832,   1811,  13773,    123,  13372, 117041, 108134,  15682, '\n",
      " '121568,\\n'\n",
      " '         30512,  66378,  23187, 118765, 104466, 117041,  56051, 121568, '\n",
      " '108134,\\n'\n",
      " '         16144, 100909,   1811,  90962, 107610, 111670, 108337, 107118, '\n",
      " '107338,\\n'\n",
      " '        101513,  97518,  29295,  33563,  19361, 103792, 127862,  33671, '\n",
      " '107740,\\n'\n",
      " '         88367,  16556,   5486,  43568, 101268,    253,  16556, 107740, '\n",
      " '108608,\\n'\n",
      " '        118522, 119526,  16144, 100909,   1811, 103927, 103202, 122481,  '\n",
      " '20230,\\n'\n",
      " '        107121, 103836,  55031,  19732, 119852,  90962, 107610, 111670, '\n",
      " '108337,\\n'\n",
      " '        102677, 113334, 111769,   1811, 107167,  76739,  25197, 103906,  '\n",
      " '52884,\\n'\n",
      " '          9458,  47260, 102494, 115144,   9458, 102803,  64810, 107446,  '\n",
      " '15682,\\n'\n",
      " '         66776, 112167,  60632, 121606,  16144, 104908,  54926, 107420, '\n",
      " '109531,\\n'\n",
      " '        110548,  16144, 120613,  16556, 127359, 105048,  30512,  11883, '\n",
      " '107991,\\n'\n",
      " '        118733, 119526,  30512,  67645,  33121, 120264, 121298,  39926, '\n",
      " '103424,\\n'\n",
      " '        118544,  30512,  20379,  17663,  30926,  29295, 108608,   9174,     '\n",
      " '43,\\n'\n",
      " '           717,  29342,     16,     25, 115813, 110548,  33710,  15682,  '\n",
      " '83687,\\n'\n",
      " '         16144, 103214,  64121,  16144, 100909,   1811, 102832, 110548,  '\n",
      " '33710,\\n'\n",
      " '         16556, 119526,  30512, 106596,  54926, 118733,  43240,   9039,  '\n",
      " '16144,\\n'\n",
      " '        119526,  56965,  16144, 116690,  29295,  91774, 107290, 109749,   '\n",
      " '1811,\\n'\n",
      " '        119526,  42016, 101559,  29295,  60358, 118544, 110596, 118544, '\n",
      " '104409,\\n'\n",
      " '        125655,  32149, 104409,  38144, 100604,  32149,  16556, 119526,  '\n",
      " '30512,\\n'\n",
      " '        106596,  54926,   1811, 124349,  15682, 119526,  29295,  67645,  '\n",
      " '33121,\\n'\n",
      " '        103378, 106627, 115032,  32149,   1811, 112603,  29295, 119448,  '\n",
      " '16995,\\n'\n",
      " '        108680, 104409,  38144, 100604,   1811,  95543, 103359,  20230, '\n",
      " '104770,\\n'\n",
      " '        104409,  27479,  15682, 119526,  29295,  67645,  33121, 103378, '\n",
      " '104409,\\n'\n",
      " '        100934,  32149,   1811, 112603,  29295, 119448,  16995,  39607, '\n",
      " '104409,\\n'\n",
      " '        100934, 119520, 124349, 108680,  16906,    111,  28741,  20230,  '\n",
      " '23187,\\n'\n",
      " '        104577, 116216,  33334, 121140,   5486,  37656,  16144, 112603,  '\n",
      " '32977,\\n'\n",
      " '        109972,  16144, 112603,  32977,  19732,  30369,   1811, 124349, '\n",
      " '102052,\\n'\n",
      " '        108152, 105048,  37823, 105404, 124349,      7, 117734,  16144,  '\n",
      " '39177,\\n'\n",
      " '        103732,  47884,  20834, 107113, 103605, 105888,  32149,  25827,  '\n",
      " '16144,\\n'\n",
      " '        124349,  16144, 121216,  24273,      8,  19732,   2880, 124349,      '\n",
      " '7,\\n'\n",
      " '        103302,  29295,  30369, 103296,  16144,  74245, 101911,  16144, '\n",
      " '124349,\\n'\n",
      " '             8,  19732,     43, 124349,      7,  68759, 100560,    237,  '\n",
      " '26269,\\n'\n",
      " '        127065, 124349,   2432,   7741,    250,  62004, 109203,  64889,  '\n",
      " '72342,\\n'\n",
      " '        101860,  20230,  71493,  29220,  71493,  29220, 103302,  29295,  '\n",
      " '76947,\\n'\n",
      " '         45893,  30512, 103130, 104127,  16995,  47884,  25197, 126989,      '\n",
      " '8,\\n'\n",
      " '        107407,   1811,   2880, 124349,  15682,     16,  31634,  72238,  '\n",
      " '72661,\\n'\n",
      " '        119448,  47884, 108010, 102518,  33503, 122582, 102971, 106391, '\n",
      " '108010,\\n'\n",
      " '         30297,  19732,  21405,  56051,  16995, 107803,  20230,  33655,  '\n",
      " '30297,\\n'\n",
      " '          1811, 102040, 101198,  94550, 124349,  15682, 111476,     43,   '\n",
      " '1032,\\n'\n",
      " '         29342,     16,     25, 108263, 103984,  33208, 107118, 119526,  '\n",
      " '30512,\\n'\n",
      " '        113954,  15024, 126172,  16995, 111629, 117657, 100909,   1811, '\n",
      " '119526,\\n'\n",
      " '         30512,  31540, 103984,  33208,  54926, 100909, 113468, 119526,  '\n",
      " '16144,\\n'\n",
      " '        101427,    122,  70141,  30512, 102178, 111665,  54926, 105908, '\n",
      " '108608,\\n'\n",
      " '          1811,  61633,  28542, 103306,  15682, 113173,  19732,  77195, '\n",
      " '113468,\\n'\n",
      " '        109431, 102769,  26854, 102944,  30512, 121507,  21105,    103,  '\n",
      " '16995,\\n'\n",
      " '         19732, 125144,  50338,  30512,  21990, 104004, 105933,  21405,  '\n",
      " '15024,\\n'\n",
      " '         24273,  30512,  39926, 114732, 107803, 107407,   1811,  31540, '\n",
      " '103984,\\n'\n",
      " '         33208,  16144,  41007, 102052, 117399, 111134,  52591,   5486, '\n",
      " '107461,\\n'\n",
      " '         71634, 111134,  91062,  74396, 104328, 102639, 111629,   5486,  '\n",
      " '35757,\\n'\n",
      " '        111134,  52591,   5486, 107471,  52927, 111629,   5486, 107461,  '\n",
      " '84477,\\n'\n",
      " '         68759, 103830,   5486, 108881,  33121, 101911, 111134,  52591,   '\n",
      " '5486,\\n'\n",
      " '         43568,  70141,   9458,  19361,  70141, 111134,  52591, 104309, '\n",
      " '118969,\\n'\n",
      " '         75694,  17129,  26854,  41007, 115991,   5486, 125261,  54926, '\n",
      " '103359,\\n'\n",
      " '         47523, 104309, 113468,  33655,  16995,  17620, 105784, 105908,  '\n",
      " '27384,\\n'\n",
      " '        102769, 103195,   9174,     43,    975,  29342,     16,     25,    '\n",
      " '220,\\n'\n",
      " '        115240, 107118,     17,  33671,  24186,  17905,  20230,   9039, '\n",
      " '112603,\\n'\n",
      " '         30512, 106045, 108630, 112535,  55487,  32218,  56051, 119526,  '\n",
      " '16144,\\n'\n",
      " '        100909, 103195,   1811, 115240, 117120,  22649,  71289,  46091,  '\n",
      " '33671,\\n'\n",
      " '         24186,  88356, 106391,  13153,   5486, 108761, 103438,  17620, '\n",
      " '111017,\\n'\n",
      " '          5486,  53953,  33014, 111800,  20834,   9458, 104736, 109375, '\n",
      " '104309,\\n'\n",
      " '         29295, 108608,   1811, 106384,  33710,  47307, 117120,  22649,  '\n",
      " '15682,\\n'\n",
      " '         20834,  48634,  55723,  72238, 112028,  55723,  72238,  30512,  '\n",
      " '32018,\\n'\n",
      " '        109315,  16144, 109156,  48634,  55723,  72238,  16144,  41642, '\n",
      " '111331,\\n'\n",
      " '        108761, 103438,  16144,  55723,  72238, 112603,  30512,  11883, '\n",
      " '107991,\\n'\n",
      " '        127919, 115891,  16144, 104721, 109033,  16144, 117120,  22649,  '\n",
      " '16144,\\n'\n",
      " '        100909,   1811, 101911,  83799, 106384,  33710,  47307,  15682,  '\n",
      " '78699,\\n'\n",
      " '         27929,  48154, 110792, 115856,  76622,  70203,  16144, 110022,  '\n",
      " '34208,\\n'\n",
      " '        117824,  30297, 106384,  33710,  47307, 117120,  22649,  16144, '\n",
      " '100909,\\n'\n",
      " '          1811, 106384,  33710,  47307, 117120,  22649,  77181,  76739,  '\n",
      " '26269,\\n'\n",
      " '         76428, 116602,  20834,  71289,  50211, 113764,  33208,  29295, '\n",
      " '108608,\\n'\n",
      " '          1811, 115274,  33208, 106384,  33710,  47307,  15682, 106384,  '\n",
      " '33710,\\n'\n",
      " '         47307,  20230, 117633,  78183, 115032, 108761, 103438, 123727,  '\n",
      " '55723,\\n'\n",
      " '         72238, 112603,  16144, 115274,  30512,  32018, 109315, 100909,   '\n",
      " '1811,\\n'\n",
      " '        127919, 115891, 104721,  33208,  29295, 113764,  33503,  32149, '\n",
      " '108176,\\n'\n",
      " '         29295,   5486, 115240,  37087,  33014,  29295, 109416,  71289, '\n",
      " '105784,\\n'\n",
      " '        107751, 105352,  50034,  17620, 106384,  33710,  47307,  71289, '\n",
      " '102741,\\n'\n",
      " '         25005,    247,  33710, 106384,  33710,  47307,   5486,  32131, '\n",
      " '125540,\\n'\n",
      " '         57207, 113164, 106384,  33710,  47307, 104309, 107407,   1811,     '\n",
      " '17,\\n'\n",
      " '        112603,  33208, 102677,  88435,  15682, 111199, 112810,  15682,  '\n",
      " '15024,\\n'\n",
      " '         85791, 102677, 105104,  30297,  20230,     17, 112603,  20230, '\n",
      " '115240,\\n'\n",
      " '         30512, 104721, 109033,  54926, 117120,  22649,  32977, 101182, '\n",
      " '109807,\\n'\n",
      " '         39177,  21105,    235,  16995,  88435,  30512,  75146, 107253,  '\n",
      " '20230,\\n'\n",
      " '        111199,  32149, 101828, 116421, 106053,  23187, 117824,  30297,   '\n",
      " '1811,\\n'\n",
      " '        110758,  97518, 107118,  41920, 118821,  33857,  16144, 116690,  '\n",
      " '30512,\\n'\n",
      " '        122984,  54926,  41007, 116787,   5486,  17620, 107471,  15682, '\n",
      " '119526,\\n'\n",
      " '         16144, 108285,  29295,  31431,  77913,  40862,  30512,  21405,  '\n",
      " '17663,\\n'\n",
      " '          1811, 107879,    109,  82420,   9554, 111800,  23187, 107118, '\n",
      " '105184,\\n'\n",
      " '         80195,  20230,  75146, 109669, 102532,  30813,  16144,  19361,  '\n",
      " '43568,\\n'\n",
      " '         30512, 105242, 100204,  30369,  41007, 116787,   5486, 112454,  '\n",
      " '43568,\\n'\n",
      " '        124572, 108323,      7,  58426,  95543, 103359,  15682, 103359,  '\n",
      " '80195,\\n'\n",
      " '        124572, 108323,      8,  30512, 113727,  77693,  29295,  72718,  '\n",
      " '50834,\\n'\n",
      " '         30369, 105184,  96455,  30512, 103188,  86436,  16144, 119526,  '\n",
      " '55031,\\n'\n",
      " '        122282,  15024,   5486,  19361,  37689,  53610, 107253,  19732, '\n",
      " '125261,\\n'\n",
      " '         54926, 102677,  41007,  29295, 108729, 109904, 111090,  68408,  '\n",
      " '39880,\\n'\n",
      " '        103195,   9174,     43,    868,  29342,     16,     25, 117737,  '\n",
      " '78943,\\n'\n",
      " '        108396,  16144, 123011,  62004,   1811, 128009,    198, 128006,  '\n",
      " '78191,\\n'\n",
      " '        128007, 116056, 106718,  16144,  13153, 116486,  15682,   5486,     '\n",
      " '33,\\n'\n",
      " '         38641,   1811, 128009])')\n",
      "('Labels: tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,   -100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   '\n",
      " '-100,\\n'\n",
      " '          -100, 116056, 106718,  16144,  13153, 116486,  15682,   5486,     '\n",
      " '33,\\n'\n",
      " '         38641,   1811, 128009])')\n"
     ]
    }
   ],
   "source": [
    "# batchのサンプルから，input_idsとlabelsを取り出して，全体を表示\n",
    "# テンソルの全体を表示するために、`torch.set_printoptions`を使用\n",
    "torch.set_printoptions(threshold=10000, edgeitems=30)\n",
    "ids = 0\n",
    "input_ids = batch[\"input_ids\"][ids]\n",
    "labels = batch[\"labels\"][ids]\n",
    "\n",
    "pp.pprint(f\"Input IDs: {input_ids}\")\n",
    "pp.pprint(f\"Labels: {labels}\")\n",
    "\n",
    "# 戻す\n",
    "torch.set_printoptions(threshold=1000, edgeitems=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b16d7e4",
   "metadata": {},
   "source": [
    "### LLMの読み込み -> PEFT設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb9d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0446c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 01:00:45,625 : LLM_test : INFO : 4 : Batch size: 4\n",
      "2025-06-07 01:00:45,627 : LLM_test : INFO : 5 : Input IDs shape: torch.Size([4, 3126])\n",
      "2025-06-07 01:00:45,628 : LLM_test : INFO : 6 : Attention mask shape: torch.Size([4, 3126])\n",
      "2025-06-07 01:00:45,629 : LLM_test : INFO : 7 : Labels shape: torch.Size([4, 3126])\n"
     ]
    }
   ],
   "source": [
    "# モデルにbatchを通して、出力を確認\n",
    "# 4サンプルだけ取得\n",
    "batch = {k: v[:4] for k, v in batch.items()}  # 4サンプルに制限\n",
    "logger.info(f\"Batch size: {len(batch['input_ids'])}\")\n",
    "logger.info(f\"Input IDs shape: {batch['input_ids'].shape}\")\n",
    "logger.info(f\"Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "logger.info(f\"Labels shape: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4778393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 01:17:12,011 : LLM_test : INFO : 20 : Decoded output: 、の伝相である、ですか？\n",
      "日本\n",
      "\n",
      "\n",
      "\n",
      "首あは\n",
      "\n",
      "首日本日本日本日本日本日本日本の日日のの日本日本日本日本日本日本日本日本日本はは首の日本日本日本日本日本日本日本は\n",
      "は日本日本日本日本日本日本のの日本日本日本\n",
      "\n",
      "は\n",
      "の日本日本は首首らはは日本日本日日日の日本日本のののの日本日本日本のの日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本\n",
      "日本日本ら日本日本日日日のの日本日本日本日本日本日本日本日本日日日日日日年年日日日の日日日のの日本ののの日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本首の日本を日本日日日日日日日日日日本日本日本日日日日日日日日ののの日本日年://年日日年日日日の日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日本日.aa..://://年日日日のの日日日日日日年日日日日日年://://日日日の年.://://日日日日日日日の日日日日日日の日日日の日本日本日年日日年.aaaaaa://://年日://.://年日日日日日の日本のの日://://aaaaaaaaaaaa...://://年年://://aaaaaa...aaaaaaaaaaaaaaaaaaaaaa://://://aaaaaaaa日aa://aa://aaaa://aaaaaa..://.://日日日日日日日日日日日日日日日日年.年日日日日本日日のの日本日本日日年日日日日日日日日日本日本日本日日本日本日aaaaaaaaaa://://://://aaaa.aaaaaaaa.://年://://://://://://.aaaaaaaaaaaaaaaaaaaaaaaa://://aaaaaaaaaa://aaaaaaaa://日://aa://.aaaaaaaaaa....://://://://.aaaa://://://年日日の日本日本日本日本日本ののののの日日日日日日日日日日日日年年日日日日日本日年年年年://.://://://://://..aa.://日年aaaaaaaaaaaaaaaaaaaaaaaa://aaaaaa..年://年日日日日年://://://.aaaaaaaaaaaaaaaaaaaaaa://://://://aaaaaa://://://://://aaaa.aa://aaaaaaaaaaaaaaaaaaaa://://aaaaaaaaaaaa.://://://年年年日日日年://.://年日日://://://://://://://://://://://://://://://年年://日日://...aaaa.://年日日日日://年日日日年://://://.....://://...年日日日日日日日日日日://://://://aaaa://://://aaaaaaaa://://://://://://aaaaaaaaaa日年年://年年://.aaaa.://://://.aa...aaaaaaaaaaaaaaaa..://://...://://://://.aaaaaaaaaaaa://://://://://://://aaaaaaaaaaaaaa://://://://://://aaaaaaaaaaaaaa.://://://.aaaaaaaaaaaaaa.aaaaaaaaaaaa.://://://://://://年年://://://://://://://.aaaa..://.日日年aa://://://://://://://年年年日日日日日年://年年日年日の日日日の日.aaaaaaaaaaaaaaaaaaaa...aaaaaaaaaa://://aaaa://://://://://://://://://://://://aa://://://://://aaaaaaaaaaaaaaaaOO..://://://年://.://://://.://aaaaaa://://..aa://aaaaaa://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://aaaaaa.日aa://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://aaaa://://://://aaaaaaaaaa://://aaaaaaaaaaaaaaaaaaaaaa...aa://.aaaaaaaaaaaa.://年日日日日日日日日日日年年日日日日日日日日日年://年年://..aa://aaaaaaaaaaaaaaaa....://://://://://aaaaaaaaaaaaaa....://.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://://://://://://://://://://://://://://://://://://://aaaaaaaa://://://://://://://aaaaaaaaaaaaaaaaaaaaaaaa://://://aaaaaaaa.aaaa.aaaaaaaaaaaa://://://://://://://://://://://://aa://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://aaaaaa://aa://://://://://://://://://://://://://://://aaaaaaaa://://://://://://://://://://://://://://aaaaaaaaaaaaaaaa..aaaa.://://://://.aa....aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://日日年://://://://..://://://://://.aaaaaaaaaaaaaaaaaaaaaa.://://://年の日日://..aaaaaaaaaaaa.......aaaaaaaaaaaa..aaaaaaaaaaaaaaaaaa..年://://.://://://aa://://://aaaaaaaaaaaaaaaaaaaaaaaa...aa://://年://の://日aaaaaaaa://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaa://://aaaaaaaa年://日aa://://aa://://://://://://://://://://://://://://://://://://://://aaaaaaaaaaaaaaaaaaaaaa://://://://://://://://://://://://://://aaaaaaaa://://://://://://://://://://://://://://://://://://://://://://://://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://aaaaaaaa://://://aaaaaaaaaaaa.://://://://://://://.://aaaaaaaaaaaaaaaa://aaaaaaaaaaaaaa..aaaaaaaaaaaaaaaa.://年日日日日日日日の日日本日.://://://..://年年年://.aa://://://://.://://://..aa.://.aaaaaaaaaa.aa://日日://aaaaaaaaaaaaaaaaaaaaaaaa..://...://日日日日日日日年://aa.aa://aaaaaaaaaaaa://年年年年年.年://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://aaaaaa.aaaaaaaaaa......aaaaaa://://aaaa://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://://://://://aaaaaa://aa日aaaaaaaa://://://aaaaaaaa://://://aa://://://://://://://://://://aaaaaa://://://://aaaaaa://://://://://aaaaaa://://://://://://://://://://://://://://://://://://://://://aaaa://://://://://aa://://://://://://://aaaaaaaaaaaa://://://aaaaaa://://://://aaaaaaaaaaaaaaaaaaaa://://://aaaaaaaa://://://://://://://://://://://://://://://://://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa.://.....aaaaaa://://aaaaaaaaaa..://://年年://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa....://年年日日本のの日日日日://://.://年年年://://://://://://://.://://://..aa..aaaaaaaaaaaa..aaaaaa://://://..://aaaaaaaaaaaaaaaaaaaaaaaaaa://://://://://://://://://://://://年日日年年://.://://://://.aaaaaaaaaaaaaaaaaaaaaaaaaaaa..aa://://年aa://://.aaaaaaaaOOaaaaaa://://aaaaaaaaaaaaOOOO.://://年年年日://.://://://://...://年年://://..://aaaaaa://://aaaa://://://://://://://://://aaaaaaaa://://://://://://://://://://aaaaaaaaaaaaaaaaaa://://://aaaaaaaaaaaaaaaaaaaa://://aaaaaaaa://://://://aaaaaaaa://://://aaaaaaaaaaaaaaaaaaaaaa.://://://://.://://.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa.://://://..aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://://://://://://://://://://://://://://://://://aaaaaaaaaaaaaaaa://aaaaaaaaaaaaaaaaaaaaaa://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://aaaaaaaaaaaa://://aaaa://://://://://://://://://://aaaaaaaaaa://://aaaa://://aaaaaaaa://://://://://://aa://://://://://aaaaaa://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://aaaaaaaa://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://://://aaaaaaaaaaaaaaaaaaaa...........://.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://aaaaaaaaaaaa://://://://aaaaaa://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://://://://://://://aa://://://://://://://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://aaaaaaaaaaaaaaaaaaaaOO....://://://://年年://://://://://..aaaaaaaaaaaaaaaaaaaaaaOOaa.aa.://.aaaaaaaaaaaaaaaa://://aaaaaaaaaaaaaaaaaa://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa000aa.aaaaaaaaaaaaaaaa://://://://://://://://aaaaaaaaaaaaaaaa://://://aa://://://://://aaaaaa://://://://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaa://aaaaaaaaaaaaaaaaaa://://://://://://://://://://://://://://://://://://://://://://aaaaaa://://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://aaaaaaaaaa://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://aaaaaaaa://aaaaaaaaaaaaaaaaaa...://://://://://....aaaaaaaaaaaaaaaaaaaaaaaaOOOOaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://://://://://://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa.aaaaaaaa://aaaaaaaaaaaaaaaaaaaa://aaaaaaaaaaaaaaaaaa://://://://aaaaaaaa://://aaaaaaaa://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://://://://://://://aaaa://://://://://://://://://://://://://aaaa://://://aaaaaaaaaaaaaaaa://://://://://://://aaaaaaaaaaaaaaaa://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://aaaa://://://://://://://://://aaaaaaaaaaaaaaaaaaaaaaaa://://aaaaaaaa://://://aa://://://://://://://://://://://://://://://://://aaaaaaaaaaaaaaaaaaaaaa://://aaaaaaaaaaaaaaaa://://://://://://://://://://://aaaa://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://aaaaaa://://://://://://://://://://://://://://://://aa://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://aa://aa://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://aaaaaaaaaaaaaaaaaa://://://://aa://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://://aaaaaaaa://://://://://://://://://://://://://://://://://://aa://://://://aaaa://://://aa://://://://://://://aa://://aa://://://://://://://://aaaaaaaaaaaaaaaaaa://://://aaaaaaaaaaaaaaaa://://://://://://://://://://://://://://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://://aaaaaaaaaaaaaaaaaaaaaaaaaaaa://://://\n"
     ]
    }
   ],
   "source": [
    "# \"日本の首都はどこですか？\"の質問に対する回答を生成\n",
    "sample_text = \"日本の首都はどこですか？\"\n",
    "inputs = tokenizer(\n",
    "    sample_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=4096,\n",
    ").to(model.device)\n",
    "outputs = model(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    labels=inputs[\"input_ids\"],  # ラベルとして同じ入力を使用\n",
    ")\n",
    "\n",
    "# 生成されたテキストをデコード\n",
    "decoded_output = tokenizer.decode(\n",
    "    outputs.logits.argmax(dim=-1)[0], skip_special_tokens=True\n",
    ")\n",
    "logger.info(f\"Decoded output: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d06c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "2025-06-07 01:34:18,333 : LLM_test : INFO : 21 : Decoded output: 日本の首都はどこですか？assistant\n",
      "\n",
      "日本の首都は東京都です。\n"
     ]
    }
   ],
   "source": [
    "# \"日本の首都はどこですか？\"の質問に対する回答を生成\n",
    "sample_text = \"日本の首都はどこですか？\"\n",
    "inputs = tokenizer(\n",
    "    sample_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=4096,\n",
    ").to(model.device)\n",
    "# ⚠️ ラベルは `generate()` には不要。除去する！\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_new_tokens=64,  # <- 出力長のみを制限するのが推奨\n",
    "    do_sample=False,  # or True (生成アルゴリズム)\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "logger.info(f\"Decoded output: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63efc1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 01:48:40,207 : LLM_test : INFO : 19 : Decoded output item: 日本の首都はどこですか？assistant\n",
      "\n",
      "日本の首都は東京都です。\n"
     ]
    }
   ],
   "source": [
    "# モデルを評価モードに設定\n",
    "model.eval()\n",
    "# モデルの出力を取得\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=64,  # <- 出力長のみを制限するのが推奨\n",
    "        do_sample=False,  # or True (生成アルゴリズム)\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# モデルの出力をデコードして、プロンプトを確認\n",
    "decoded_output = tokenizer.batch_decode(\n",
    "    outputs, skip_special_tokens=True\n",
    ")  # outputsは生成されたトークンのテンソル\n",
    "for item in decoded_output:\n",
    "    logger.info(f\"Decoded output item: {item}\")\n",
    "# logger.info(f\"Decoded output: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b76735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ① LoRA設定\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",  # Self-Attention系\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",  # MLP（FFN）系\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f58a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads()  #! 追加::入力テンソルに勾配を流せる状態を強制する安全スイッチ\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4ea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 01:51:15,966 : LLM_test : INFO : 1 : Trainable parameters:\n",
      "2025-06-07 01:51:15,978 : LLM_test : INFO : 3 : ✅ LoRA has been successfully applied to the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 01:51:16,419 : LLM_test : DEBUG : 4 : ==========================================================================================\n",
      "Layer (type:depth-idx)                                            Param #\n",
      "==========================================================================================\n",
      "PeftModelForCausalLM                                              --\n",
      "├─LoraModel: 1-1                                                  --\n",
      "│    └─LlamaForCausalLM: 2-1                                      --\n",
      "│    │    └─LlamaModel: 3-1                                       7,525,896,192\n",
      "│    │    └─Linear: 3-2                                           (525,336,576)\n",
      "==========================================================================================\n",
      "Total params: 8,051,232,768\n",
      "Trainable params: 20,971,520\n",
      "Non-trainable params: 8,030,261,248\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Trainable parameters:\")\n",
    "model.print_trainable_parameters()\n",
    "logger.info(\"✅ LoRA has been successfully applied to the model.\")\n",
    "logger.debug(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44a608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 19:09:24,912 : LLM_test : INFO : 12 : Logits shape: torch.Size([4, 1041, 128256])\n",
      "2025-06-06 19:09:24,913 : LLM_test : INFO : 13 : Loss: 1.5673637390136719\n",
      "2025-06-06 19:09:25,176 : LLM_test : INFO : 16 : Decoded output: ['、]のなたは、生研究で、学生の質長を評する権割を担っています。学生の、す学生の成義ノアンケートの分析み、学生績を決定評に並、B、C、D、Fの5ずれかに評類しますください。\\n\\n講ecture、義の数T は質問のです示します。例： L1Q1）。Qケートの内容問とは以下Q1: 本の講は理解でりに言葉で説明してくださいてください。\\nQ。\\n\\nアンは以下 の学生、回答を。\\n\\n以下席は、、B、C、D、F のいずれかを記むください。\\n\\n成ケートのはL1-Q1: 自回講学校の数学くは、系系の指り方放を共目カをと文やの、年数ややの表っているたいするがA01-Q1: 文記、回の、進数で表現います、データで特くからをつけるられる方法するくいろな方法夫がされている。\\nL03-Q1: 二ータの表みりを合を、隔を防見する修正する。\\nどうるくる。\\nータの二進のパ子重まものックで位で管理信。\\n�違いが発ぐために、々を増くするおる。\\nL04-Q1: デ\\nL05-Q1: デンピューターは�イエンスの、機速御するり、学プである。L06-Q1: コ\\nL07-Q1: デフトやいていえばいろな方法がある学えるりL08-Q1: NaN\\nL09-Q1: コータの重類や方法を、の機の学用方法をL10-Q1: NaNに電子の二造化データと非構造化データの分類られる\\n\\n、中でも非構造化データはAI類されるものは多析ははAIターン認識のAIれる方法の分類する識識する方法方法が二をL11-Q1: NaNータの集の方法と、々あるがことをと々情報の守する�つデータ々の情報を集用する方法をという\\nL12-Q1: NaN\\nL13-Q1: データの情報の整視化する方法、をれのような方法な方法方法する視化するいのかを可さんの方法フの種類が\\n\\nL14-Q1: NaN今日や非次元のデータクトルで、画像ピューターは画像表表理することが。\\n、やり、正したりすることが \\n互�相を情報を特をを画像られた結果を可に有の関係分けや使するりする。\\nL15-Q1: NaN\\nLINST]あ学生の成績を、Bです。', '、]のなたは、生研究で、学生の質長を評する権割を担っています。学生の、す学生の成義ノアンケートの分析み、学生績を決定評に並、B、C、D、Fの5ずれかに評類しますください。\\n\\n講ecture、義の数T は質問のです示します。例： L1Q1）。Qケートの内容問とは以下Q1: 本の講は理解でりに言葉で説明してくださいてください。\\nQ。\\n\\nアンは以下 の学生、回答を。\\n\\n以下席は、、B、C、D、F のいずれかを記むください。\\n\\n成ケートのはL1-Q1: 自�業の学われた資料の使い方が間学、くから移動を伝える方法文字々な方法を用案してきた。L01-Q1: �報を伝号化と、くつかの方法り方があるが、中で最くして率の情報かが番に決数的するかが大である\\nL03-Q1: 情報のが減くする、�昧さを減らすことが大切であるL04-Q1: 情\\nL05-Q1: 情ゴリズムのすることでによって、々な問題象を解率的に処かすることが可能る。\\nL06-Q1: 情\\nL07-Q1: 情��々な方法フトのが、最てのす錯数が最少らすことが目としている。\\nL08-Q1: NaNー�ードを索の、ベ可類をL09-Q1: NaNータの分用方法重要たを情報のど事かに分のように事でをL10-Q1: NaN\\n\\n成11-Q1: 情\\nL12-Q1: NaN\\n\\nL13-Q1: NaN\\nL14-Q1: NaN\\nL15-Q1: NaN\\n\\nLINST]あ学生の成績は、Cです。', '、]のなたは、生研究で、学生の質長を評する権割を担っています。学生の、す学生の成義ノアンケートの分析み、学生績を決定評に並、B、C、D、Fの5ずれかに評類しますください。\\n\\n講ecture、義の数T は質問のです示します。例： L1Q1）。Qケートの内容問とは以下Q1: 本の講は理解でりに言葉で説明してくださいてください。\\nQ。\\n\\nアンは以下 の学生、回答を。\\n\\n以下席は、、B、C、D、F のいずれかを記むください。\\n\\n成ケートのはL1-Q1: 自報の概念、の進化するが、情報の変憶化することで点的な変わ�から変わらないないとL01-Q1: 情報の記える手あたっては記雑的ななというという化号長長が短短くすることがった点。\\n、情報率化高る。\\nできる切。\\n。\\n \\n符号語長を短ものとどう調ることが、エントロピーを関係関係を調にしたよい。\\nL03-Q1: 情報を記�昧さを、少という、の正る人になる。\\nL04-Q1: 情イズが情報ットが転が起るが、が、情報ビのビす場合、n出をットをけることで、処がことができる。\\n \\n的に出も自動訂正が可能。\\nどうかは、ードング符を0な。\\nL05-Q1: 情ゴリズムの、ピュータが処手手順をことである。\\n情報ピュータが、の速くがいえない、計算力がのが求めるているかを大�昧な。\\nけない。\\nL06-Q1: 情ゴリズムの計算夫は第で、のかる時間を大化する。\\n計算の並を並び替えや問題を、く方法類がある、をれの方法で適点と欠点がある。\\nL07-Q1: 情ルソートは、つの要のを分けるしことで、計算回の比較で最大ず1つの要を分られる。\\nマつの木索法、要ップで、列の長さが半分になる。\\n、探定大い数列の探ートする場合に有量が少幅に減少。\\nL08-Q1: 掩去の情報を基計、関�つけることが、将の予予測できる。\\n、相関が強つ方が難つりの方法義って。\\nある。\\n相関の予果関係を推見することが困難である。\\nえる問題点�ければならけない。\\nもある。\\nL09-Q1: 相定の問題野で特化したアルつのの々知能は、の超えるているつあるがも�えられたていない分学ら学える、人できない。\\n、人工知能は人間を同く同じ能力考るうことは不の不現でき�うにない。\\nL10-Q1: 人線造化は�数学化する、構の使すことが非えば、や場合、クトル化数の列み合わせ)に表すする。\\n非いを非をかを知識するには、のっては簡と不理識に行であるできるが、人ピュータはっては困しい。\\nL11-Q1: 人ータの分めることがは、数ありが、どしいにや速実性、のぞれの点がある利点がある。データーバンデータをがによって、によって、の特展に、社会頼性の向上期待測される。\\nL12-Q1: 人クトル空、数の要の組み、数字のの基切方法を定めることが、のデータの類似度を測することができる。\\nL13-Q1: 人例つのデータをと、データつのデータを基すベは多つに、々な方法がある。例の�多かりやすくなる。\\n方法現方法を選�味することががあるます表適切な表フを使用ソリ使用を力な表現方法は避はと、解を招起こす。\\nになるうる。\\nL14-Q1: 人類分析表のばらばりを標準関でデータつのデータの関係をわかる。分計学には定はうことで、ないサから有計的有を検価する。\\n統の分ルタリング、ざまな類がありがL15-Q1: 人画像のフィ識する分集するには、、るさ、調激な変化をの避識する、適調をルタをけることでった方法できるわれるれている。\\nLINST]成学生の講績を、Aです。', '、]のなたは、生研究で、学生の質長を評する権割を担っています。学生の、す学生の成義ノアンケートの分析み、学生績を決定評に並、B、C、D、Fの5ずれかに評類しますください。\\n\\n講ecture、義の数T は質問のです示します。例： L1Q1）。Qケートの内容問とは以下Q1: 本の講は理解でりに言葉で説明してくださいてください。\\nQ。\\n\\nアンは以下 の学生、回答を。\\n\\n以下席は、、B、C、D、F のいずれかを記むください。\\n\\n成ケートのはL1-Q1: 自�はが学段使する情報は多くは、と1のデみ合わせでと進数でなるり。\\n \\nの内容義で、2の講のの基の基組みを、重要史を話まかに学んだ。2くから、の電を用したが号伝、や号や電には電ールス信号、経て、のデに信に進階的に進化してきた。ことをわかった。\\nL02-Q1: 報のの号化の、情報源0○×などの組列に表すすることである。情報号化のことでには、情報気信信荷を減減するために、符べく少いする単��な表号化することが重要ましい。\\n、符号化ど意に情報である用すかどう列�する必要がある。\\n配、ハ源符うう化は、情報々な符的方法識を統を必要であるされる。\\nL03-Q1: 情報源符確に伝えるため符夫として学んだ。L04-Q1: 情字を置ぞれのする数字コード割定的に割える方法、情報仮を暗号文する。、暗意ある攻撃者が暗を解読するないようにする暗号化長読を��関時間がかかるようにする夫する。\\nL05-Q1: 情報の通信機関係連を歴組みをおまかに学んだ。\\nL06-Q1: 情イソートのクープソートなどの学んだ。\\nL07-Q1: 情�ープソートとバージソートの違いをバ徴を分探索木方法点をを学んだ。\\nL08-Q1: �分探索ののヒの並を学んだ。\\nL09-Q1: �工知能のI)の学んだ。\\nL10-Q1: 人ータの分析や学んだ。\\nL11-Q1: 人ータの情報情報の学んだ。\\nL12-Q1: 人イズル空行、内似性、学んだ。\\nL13-Q1: 人��々な計算のデータ視化の方法ぞれの特や学んだ。\\nL14-Q1: 可関�学んだ。\\nL15-Q1: 相\\nLINST]あ学生の成績を、Bです。']\n"
     ]
    }
   ],
   "source": [
    "# モデルを評価モードに設定\n",
    "model.eval()\n",
    "# モデルの出力を取得\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=64,  # <- 出力長のみを制限するのが推奨\n",
    "        do_sample=False,  # or True (生成アルゴリズム)\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# モデルの出力をデコードして、プロンプトを確認\n",
    "decoded_output = tokenizer.batch_decode(\n",
    "    outputs, skip_special_tokens=True\n",
    ")  # outputsは生成されたトークンのテンソル\n",
    "for item in decoded_output:\n",
    "    logger.info(f\"Decoded output item: {item}\")\n",
    "# logger.info(f\"Decoded output: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc441ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a852601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c60011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
