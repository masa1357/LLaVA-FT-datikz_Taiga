{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e967348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "421ea8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8be05292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gradeexplanation_data import GradeExplanationDataset, GradeExplanationCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16710087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elyza/Llama-3-ELYZA-JP-8B tokenizerのロード\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"elyza/Llama-3-ELYZA-JP-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81892d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path.cwd().parent\n",
    "DATA_PATH = BASE_PATH / \"data\"\n",
    "data = GradeExplanationDataset(\n",
    "    dataset_path=DATA_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    max_tokens=4096,\n",
    "    )\n",
    "Q1_data = GradeExplanationDataset(\n",
    "    dataset_path=DATA_PATH,\n",
    "    question_filter=[1],\n",
    "    tokenizer=tokenizer,\n",
    "    max_tokens=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b174f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5081305e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userid': 'C-2021-1_U30',\n",
       " 'labels': 0,\n",
       " 'grades': 'A',\n",
       " 'L1': {'Q1': '情報イントロダクション、情報の大まかな仕組み、情報の歴史',\n",
       "  'Q2': '情報は0と1の組み合わせで伝えられていること、モールス信号は作られてから使われるまで時間が空いたこと',\n",
       "  'Q3': '特にないです',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': 'フランスの腕木通信は知らなかったのでおもしろいと思いました。'},\n",
       " 'L2': {'Q1': '情報源とその符号化、望ましい符号',\n",
       "  'Q2': '符号は一意に、素早く戻せるもの、短いものが良い。 語頭符号は一意に、瞬時に戻せる。 平均符号語長がエントロピーを下回ることはない。',\n",
       "  'Q3': '最適符号の定義',\n",
       "  'Q4': '平均符号語長がエントロピーとエントロピーに1を足した値で挟めることは結構すごいことだから、その時の符号を最適符号という、という認識で大丈夫でしょうか。',\n",
       "  'Q5': '前回と比べて、一気にハードになったなと感じました。新しい語句がたくさんでてきて混乱しそうです。'},\n",
       " 'L3': {'Q1': '情報量、曖昧さの減少、情報の期待値、相互情報量',\n",
       "  'Q2': '情報量=曖昧さの減少、起きにくい現象の情報を知った時に得られる情報量は大きい、情報量の期待値はエントロピーに等しい',\n",
       "  'Q3': '曖昧さ=log の証明',\n",
       "  'Q4': '特にないです',\n",
       "  'Q5': 'エントロピーがまだよくわかっていないので、しっかり復習したいです'},\n",
       " 'L4': {'Q1': 'ハミング距離、誤り検出と誤り訂正、繰り返し符号、通信路容量',\n",
       "  'Q2': '誤り訂正と誤り検出ができる場合について、ハミング距離の求め方、',\n",
       "  'Q3': '通信路符号化定理、検査ビット',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': 'エントロピーがここでもでできて、重要な存在なのだなと感じました。通信路符号化定理のところと、最後の検査ビットがでてきたところがいまいちよく分からなかったので、ちゃんと復習したいです。'},\n",
       " 'L5': {'Q1': 'コンピューターサイエンスとは、計算、アルゴリズム',\n",
       "  'Q2': 'アルゴリズムの定義について',\n",
       "  'Q3': '根付き木がまだよく分からなかった。',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': 'また新しい語句が増えたので、しっかり定着させたいと思いました。コインのアルゴリズムのところが面白かったです。'},\n",
       " 'L6': {'Q1': 'バブルソート、選択ソート、2進木ソート、ヒープソート',\n",
       "  'Q2': 'それぞれのソートのやり方',\n",
       "  'Q3': 'ヒープソートの比較回数にlogが出てくるのがいまいちよくわからないです',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': 'ソートがパズルみたいで面白かったです。特に私は2進木ソートが好きです。'},\n",
       " 'L7': {'Q1': 'マージソート、二分探索',\n",
       "  'Q2': 'マージソートのやり方、二分探索のアルゴリズム',\n",
       "  'Q3': 'キーワードの二分探索がちょっと微妙です',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '線形探索と比べて、二分探索の計算量の少なさに驚きました。'},\n",
       " 'L8': {'Q1': 'データとは、データの種類、予測、発見、グルーピング',\n",
       "  'Q2': 'データ分析は身近であること、データを用いた予測、発見、グルーピングのやり方',\n",
       "  'Q3': '特にないです',\n",
       "  'Q4': '特にないです',\n",
       "  'Q5': 'データ分析はもっと専門的な知識を必要とすると思っていたので、こんなにも身近であることに驚きました。'},\n",
       " 'L9': {'Q1': '人工知能とは、人工知能を用いたビジネス、機械学習、今の人工知能にできないこと',\n",
       "  'Q2': '人工知能は思っている以上に万能ではないこと、人工知能を利用したビジネスの詳細',\n",
       "  'Q3': 'オープン戦略のあたりがまだ曖昧です',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '人間の脳がいかに優れているかを実感しました。また、顧客監視のような人工知能を使ったビジネスの中にも知らないものがあったので、新しい知識を身に着けることができてよかったです。'},\n",
       " 'L10': {'Q1': '非構造データ(言語、画像、音声)、パターン認識とその応用',\n",
       "  'Q2': '非構造データとは何か、画像はベクトルで表せること、パターン認識とは何か、パターン認識の応用',\n",
       "  'Q3': 'とくにないです',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '今、線形代数を履修しているので、ベクトル=画像で気を安らげたいと思います。また、深層ニューラルネットワークによるパターン認識が面白いと思いました。'},\n",
       " 'L11': {'Q1': 'データ収集、全数調査、標本調査、個人情報、オープンデータ、著作権',\n",
       "  'Q2': 'データ収集の主なやりかた、全数調査と標本調査のメリット・デメリット、個人情報の扱い方',\n",
       "  'Q3': 'オープンデータのところがまだ少し微妙です。定義はなんとなく理解できましたが、目的の一つである「透明性、信頼性の向上」がまだよく分からないです。',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '個人情報の部分はサイバーセキュリティでやったところと重なっていたのですぐに理解ができました。また、メディアの街頭インタビューのような標本選択バイアスに惑わされないように気をつけようと思いました。'},\n",
       " 'L12': {'Q1': 'ベクトル、距離、類似度、それらの種類や応用',\n",
       "  'Q2': 'ベクトルとは何か、ベクトルを用いたデータ分析のやり方、距離の種類、距離や類似度の応用',\n",
       "  'Q3': 'max距離の使い方とコサイン類似度が微妙です',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '距離と聞くと長さを表す単位(kmやm)が思いつくが、データ解析においても距離を使うのが新鮮で面白かった。'},\n",
       " 'L13': {'Q1': 'データの可視化、そのいろいろなやり方',\n",
       "  'Q2': '棒グラフや折れ線グラフなど、データの可視化の手法について、データの可視化の利点',\n",
       "  'Q3': '多次元データの可視化',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': 'パイチャートやヒートマップなど、初めて知ったデータの表し方があったので、面白かったです。データの可視化をする際には、どのやり方でするのがよいか、だけでなく、軸の数字や点の配置など、考えなければならないことがたくさんあるんだなと感じました。'},\n",
       " 'L14': {'Q1': '相関、分散、相関係数、統計的検定、画像処理、フィルタ処理、エッジ抽出',\n",
       "  'Q2': '相関係数の求め方、相関や分散や統計的検定の特徴',\n",
       "  'Q3': '微分フィルタ、ソーベルフィルタ',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '高校の数学でやったところが多かったので、懐かしい気分になりました。フィルタ処理のところが難しかったです。'},\n",
       " 'L15': {'Q1': '画像処理の補足、期末テスト',\n",
       "  'Q2': 'ラプラシアンフィルタについて',\n",
       "  'Q3': 'NaN',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '期末テストは結構時間がギリギリでした。説明をする記述問題が自信ないです。'}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 31\n",
    "data.__getitem__(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d488c611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userid': 'C-2021-1_U30',\n",
       " 'labels': 0,\n",
       " 'grades': 'A',\n",
       " 'L1': {'Q1': '情報イントロダクション、情報の大まかな仕組み、情報の歴史'},\n",
       " 'L2': {'Q1': '情報源とその符号化、望ましい符号'},\n",
       " 'L3': {'Q1': '情報量、曖昧さの減少、情報の期待値、相互情報量'},\n",
       " 'L4': {'Q1': 'ハミング距離、誤り検出と誤り訂正、繰り返し符号、通信路容量'},\n",
       " 'L5': {'Q1': 'コンピューターサイエンスとは、計算、アルゴリズム'},\n",
       " 'L6': {'Q1': 'バブルソート、選択ソート、2進木ソート、ヒープソート'},\n",
       " 'L7': {'Q1': 'マージソート、二分探索'},\n",
       " 'L8': {'Q1': 'データとは、データの種類、予測、発見、グルーピング'},\n",
       " 'L9': {'Q1': '人工知能とは、人工知能を用いたビジネス、機械学習、今の人工知能にできないこと'},\n",
       " 'L10': {'Q1': '非構造データ(言語、画像、音声)、パターン認識とその応用'},\n",
       " 'L11': {'Q1': 'データ収集、全数調査、標本調査、個人情報、オープンデータ、著作権'},\n",
       " 'L12': {'Q1': 'ベクトル、距離、類似度、それらの種類や応用'},\n",
       " 'L13': {'Q1': 'データの可視化、そのいろいろなやり方'},\n",
       " 'L14': {'Q1': '相関、分散、相関係数、統計的検定、画像処理、フィルタ処理、エッジ抽出'},\n",
       " 'L15': {'Q1': '画像処理の補足、期末テスト'}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1_data.__len__()\n",
    "Q1_data.__getitem__(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e9567b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample C-2021-1_U27 exceeds max_tokens (4661 > 4096). Trimm -565 tokens.\n",
      "Sample C-2021-1_U54 exceeds max_tokens (4424 > 4096). Trimm -328 tokens.\n",
      "Sample C-2021-2_U101 exceeds max_tokens (5071 > 4096). Trimm -975 tokens.\n",
      "Sample C-2021-2_U128 exceeds max_tokens (12354 > 4096). Trimm -8258 tokens.\n",
      "Sample C-2021-2_U142 exceeds max_tokens (4317 > 4096). Trimm -221 tokens.\n",
      "Sample C-2021-2_U3 exceeds max_tokens (7199 > 4096). Trimm -3103 tokens.\n",
      "Sample C-2021-2_U39 exceeds max_tokens (7185 > 4096). Trimm -3089 tokens.\n",
      "Sample C-2021-2_U41 exceeds max_tokens (14887 > 4096). Trimm -10791 tokens.\n",
      "Sample C-2021-2_U56 exceeds max_tokens (6043 > 4096). Trimm -1947 tokens.\n",
      "Sample C-2021-2_U60 exceeds max_tokens (5851 > 4096). Trimm -1755 tokens.\n",
      "Sample C-2021-2_U8 exceeds max_tokens (7763 > 4096). Trimm -3667 tokens.\n",
      "Sample C-2021-2_U9 exceeds max_tokens (9443 > 4096). Trimm -5347 tokens.\n",
      "Sample C-2022-1_U15 exceeds max_tokens (4524 > 4096). Trimm -428 tokens.\n",
      "Sample C-2022-1_U25 exceeds max_tokens (4580 > 4096). Trimm -484 tokens.\n",
      "Sample C-2022-1_U39 exceeds max_tokens (4710 > 4096). Trimm -614 tokens.\n",
      "Sample C-2022-1_U43 exceeds max_tokens (4320 > 4096). Trimm -224 tokens.\n",
      "Sample C-2022-1_U50 exceeds max_tokens (4240 > 4096). Trimm -144 tokens.\n",
      "Sample C-2022-1_U52 exceeds max_tokens (7255 > 4096). Trimm -3159 tokens.\n",
      "Sample C-2022-1_U54 exceeds max_tokens (7007 > 4096). Trimm -2911 tokens.\n",
      "Sample C-2022-1_U58 exceeds max_tokens (9486 > 4096). Trimm -5390 tokens.\n",
      "Sample C-2022-1_U65 exceeds max_tokens (6034 > 4096). Trimm -1938 tokens.\n",
      "Sample C-2022-1_U66 exceeds max_tokens (8057 > 4096). Trimm -3961 tokens.\n",
      "Sample C-2022-1_U77 exceeds max_tokens (5181 > 4096). Trimm -1085 tokens.\n",
      "Sample C-2022-1_U82 exceeds max_tokens (5119 > 4096). Trimm -1023 tokens.\n",
      "Sample C-2022-1_U89 exceeds max_tokens (7081 > 4096). Trimm -2985 tokens.\n"
     ]
    }
   ],
   "source": [
    "# trim_datasetの動作確認(GradePredictionDatasetのメソッド)\n",
    "Q1_data.trim_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f381ca5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377\n"
     ]
    }
   ],
   "source": [
    "print(Q1_data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2bba985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found at index: 343\n"
     ]
    }
   ],
   "source": [
    "# get index userid== C-2022-1_U66\n",
    "for i in range(len(data)):\n",
    "    if data[i][\"userid\"] == \"C-2022-1_U66\":\n",
    "        print(f\"Found at index: {i}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1fc293a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userid': 'C-2022-1_U66',\n",
       " 'labels': 1,\n",
       " 'grades': 'B',\n",
       " 'L1': {'Q1': '授業方針の説明、通信手段の歴史、情報の表し方',\n",
       "  'Q2': '古代から人は様々な手段で情報を伝達していた。腕木通信は初めて知った。',\n",
       "  'Q3': '特になし',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '大学での初めての授業ですこし緊張した。情報を科学的に考えられるように頑張りたい。'},\n",
       " 'L2': {'Q1': '情報源はただ情報の源というわけではなく一定間隔で何かしらの情報を吐き出すものである。情報を伝達するために情報源符号化を行う。情報を0と1であらわし、情報をできるだけ短くするのが目的である。情報源符号化において、一意復号可能性と瞬時復号可能性を考慮する必要がある。ビット列の分解の仕方が複数存在したり、復号に時間がかからないようにするためである。よって符号の評価基準は、一意復号可能であるか、瞬時復号可能であるか、そして平均符号語長という三要素で成り立っている。これを満たすために語頭符号を活用する。語頭符号は一意復号可能性と瞬時復号可能性を満たしている。また、「任意の一意復号可能符号Cに対し、同じ符号語調を持つ語頭符号C’が存在する」という定理から、語頭符号は平均符号語長も最小にすることができる。平均符号語長は情報源と符号語を変えることで変化する。また、情報源のエントロピーは平均符号語長の最小値以下となるので、エントロピーは平均符号語長の下限となる。しかし、エントロピーが最適符号というわけではなく、平均符号語長がエントロピーと、エントロピー+1の間にあれば最適符号となる。',\n",
       "  'Q2': '情報源はただ情報の源というわけではなく一定間隔で何かしらの情報を吐き出すものである。 情報源符号化とは、情報を0と1であらわすことで、情報をできるだけ短くするのが目的。 符号の評価基準は、一意復号可能であるか、瞬時復号可能であるか、平均符号語長の三要素で成り立っている。 一意復号可能性とは、符号の長さが同じ(固定長符号)だったり、末尾の記号を統一することで情報を一意に復号できること。 瞬時復号可能性とは、符号語を読み終えた時点で先読みせずに記号を復号できること。 語頭符号とは、どの符号語も別の符号語の接頭語になっていない符号のこと。語頭符号は一意復号可能で、瞬時復号可能である。 情報源のエントロピーが平均符号語長の下限である。 最適符号はエントロピーとは限らない。',\n",
       "  'Q3': 'NaN',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '新しい単語や定理、公式が登場したのでよく理解しておきたい。'},\n",
       " 'L3': {'Q1': '情報を伝送するとき、物理的または電気的な要因でノイズが生じ、ビットが反転することがある。反転確立が小さくても、ブロックのサイズが大きいとブロック誤り率は大きくなってしまう。対策としてはまず通信路を改善する方法があげられる。しかし物理的、経済的制限により対策が困難な場合がある。そこで二つ目の対策として符号化を工夫することがあげられる。これにより自動誤り検出、自動誤り訂正が可能となる。符号語どうしがs+1以上離れているならば高々s個の誤りを自動検出できる。また符号語どうしが2t+1以上離れているならば高々t個の誤りを自動訂正できる。繰り返し符号でも誤り訂正ができる。n=2k+1からなるn次繰り返し符号では、ビットの反転がk個以下の場合は誤り訂正ができる。nを大きくすればブロック誤り率はいくらでも小さくできるが、伝送速度は低下してしまう。通信路には容量があり、符号化効率の上限である。このとき通信路符号化定理により、符号化効率はいくらでも通信路容量に近づけ、ブロック誤り率もいくらでも小さくできる。',\n",
       "  'Q2': 'ビット列における誤りの数をハミング距離といい、三次繰り返し符号において各符号語の守備範囲をハミング距離1以下にすることで高々1ビットの誤り訂正ができる。 通信路容量は通信路に固有の値で符号化効率の上限である。 通信路符号化定理により、ブロック誤り率をいくらでも小さくできるうえに、符号化効率も通信路容量に近づけれる。',\n",
       "  'Q3': '受信した側は受け取ったビット列が正しいかどうかなんてわからないのに、どうして自動で誤りを検出できるのか不思議に感じた。社員名簿の例だと受信側に似た情報が存在しているから誤りを検出劇るかもしれないが、写真などの情報を一方的に送信する場合は受け取る側に検証の余地はないのではと思った。',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': 'ブロックを冗長化したり、繰り返し符号を使うなど、様々な方法で誤りの検出や訂正ができるとしれてよかった。'},\n",
       " 'L4': {'Q1': '暗号とは悪意ある第三者から通信内容を守る技術で、計算機やネットワークの発達と普及を背景に、暗号の重要性は急速に高まっている。 最古の暗号はシーザー暗号で、紀元前50年頃にできた。古代ローマ人のジュリウス・シーザーが使用したもので、平文の文字をK文字右にずらして暗号化する。 シーザー暗号への攻撃としては、総当たり攻撃がある。文字の種類をMとすると、鍵の候補K=0~M-1をすべて試せばいつか必ず平文が得られる。 シーザー暗号を改良したものに、単一換字暗号がある。単純にK文字ずらすだけでなく、文字の一対一対応がとれていれば、どんな対応関係でも、暗号化の鍵として使え、より複雑な暗号を作ることができる。 単一換字暗号に対する攻撃には、まず総当たり攻撃がある。しかし、文字の種類がMのとき、復号化鍵の候補となる写像の種類はM!であり、現実的ではない。次に、頻度分析攻撃がある。平文の文字の頻度には偏りがあり、その偏りは文字を変換した後も保存されることを利用して、暗号に使われる文字の頻度から平文の文字を特定するというものだ。 シーザー暗号、単一換字暗号は対象鍵暗号といわれ、暗号化と復号の鍵は対称的である。 平文を暗号化する前に、受信者に鍵を伝える必要があり、復号鍵を攻撃者に知られたくないけど、そのためにはまた別の暗号が必要になるという鍵配送問題が発生する。 一方で非対称鍵暗号は暗号化と復号に異なる鍵を使うため鍵配送問題を解決できる。公開鍵と秘密鍵の2種類の鍵を使い、1。受信者は、自分の公開鍵を誰にでもわかるように公開しておく。2。送信者は、受信者の公開鍵で平文を暗号化し、受信者に送る。3。受信者は、暗号文を自分の秘密鍵で復号する。といった手順で使用できる。 RSA暗号とは1997年MITのRivest、Shamir、Adlemanが開発した最初の公開鍵暗号で、単一換字暗号である。RSA暗号ではmodを使う(二つの大きな素数の積の余りを使う)のが特徴である。RSA暗号への攻撃は以下の4つがあげられる。 1。暗号文から秘密鍵を求める攻撃 2。総当たりで秘密鍵を求める攻撃 3。公開鍵から秘密鍵を求める攻撃 4。公開鍵から素数を求める攻撃 しかし、現実的な時間では求めることができず、突破されにくい。',\n",
       "  'Q2': '計算機やネットワークの発達と普及を背景に、暗号の重要性は急速に高まっている。 最古の暗号はシーザー暗号 古代ローマ時代には既に暗号が使われていた。 平文の文字の頻度には偏りがある 暗号には対象鍵暗号と非対象鍵暗号がある。 シーザー暗号、単一換字暗号は対象鍵暗号。 鍵配送問題。。。平文を暗号化する前に、受信者に鍵を伝える必要があり、復号鍵を攻撃者に知られたくないけど、そのためにはまた別の暗号が必要になるというジレンマ。 非対称鍵暗号とはコインロッカーのように暗号化と復号に異なる鍵を使う仕組みの暗号 RSA暗号とは1997年MITのRivest、Shamir、Adlemanが開発した最初の公開鍵暗号のこと。 単一換字暗号である。 RSA暗号ではmodを使う。 公開鍵(E、N)と秘密鍵Dの計算手順 1、Nを求める 2つの素数pとqを準備し、N=p*qとする 2、EとDを求める E*D=k*LCM(p-1、q-1)+1を満たす整数E、D、kを適当に選ぶ 公開鍵(E、N)は公開する 秘密鍵Dと素数p、qは公開しない RSA暗号への攻撃は時間がかかりすぎて現実的ではない',\n",
       "  'Q3': 'NaN',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '紀元前から暗号が使われていることに驚いた。暗号を攻撃するには莫大な時間が必要で、想像以上に暗号を解読するのは不可能だと感じた。'},\n",
       " 'L5': {'Q1': 'コンピュータサイエンスは基礎科学としての計算機科学で、すべての科学技術分野の基盤となりうるものであり、物理などと同様に重要な基礎科学となりうる分野である。 コンピュータは他分野の専門家が使うだけの単なる便利な道具ではなく、他学問と同様に背景には独自の理論体系がある。 コンピュータサイエンスは、計算とうまく付き合い、制御するために、計算が何者なのか理解するための学問である。 計算機科学における問題とは、数学的に厳密に定義された関数のことで、任意の問題事例に対し、答えが一意に定まっていることが前提となる。そのため、問題は厳密に、明確に記述すべきである。計算とは、その問題の答えを求めることである。ただ、コンピュータは問題を与えられただけでは答えを計算することができないので、関数の計算手順を人間が教える必要があり、その関数の計算方法を示す手続きをアルゴリズムという。コンピュータがわかる言葉でアルゴリズムを記述したものがプログラムである。アルゴリズムは有限時間で計算を終了するものである。 アルゴリズムを工夫することで計算ステップ数を減らすことができ、アルゴリズムの実行時間を短くできる。',\n",
       "  'Q2': 'コンピューターサイエンスとは基礎科学としての計算機科学で、すべての科学技術分野の基盤となりうる学問である。 コンピュータサイエンスでは、あらゆる問題、関数の計算問題として数式化(主に文字列で表現)し、問題を解くための効率のいい計算手順を設計する。計算手順の正当性や効率化の限界は証明できる。 計算とうまく付き合い、制御するには、計算が何者なのか理解する必要がある。 計算機科学における問題とは数学的に厳密に定義された関数で、任意の問題事例に対し、答えが一意に定まっていることが前提 計算とは問題事例xが与えられたとき、その答えf(x)を求めること アルゴリズムとは問題を計算するアルゴリズムとは任意の問題事例から答えを求める手続きであり、有限時間で計算を終了するもの コンピュータがわかる言葉でアルゴリズムを記述したものがプログラム 計算ステップ数とはアルゴリズムが終了するまでに行われる基本演算の回数で、計算ステップ数が少ないほどアルゴリズムの実行時間は短くなる',\n",
       "  'Q3': '特になし',\n",
       "  'Q4': '特になし',\n",
       "  'Q5': '普段は何気なくコンピュータを利用しているが、コンピュータが問題を解くためには、実際は我々が手取り足取り計算手順を教えてあげる必要があり、アルゴリズムを構築するのは大変そうだとおもった。'},\n",
       " 'L6': {'Q1': 'ソート問題とはn個のアイテムが与えられたときに、それらをある順序によって並び替えることで、計算機科学におけるもっとも基本的な問題の一つである。 代表的なソートアルゴリズムとして、交換型、選択型、分割統治型などがあり、交換型にはバブルソート選択ソート、選択型には2進木ソート、ヒープソート、分割統治型にはマージソート、クイックソートがありそのほかにもバケツソートや基数ソートといったものもある。 バブルソートとは隣り合う要素の代償を比較しながら整列するソートで、整列したい数列の長さをnとし、要素の比較回数と交換回数で時間計算量を見積もる。 選択ソートとは最大の要素を探し、それと最後の要素を入れ替えるソートで、整列したい数列の長さをnとし、要素の比較回数、最大値の更新回数、要素の交換回数で時間計算量を見積もる。ここで、時間計算量とは入力に対して各手順を実行するのにかかる時間の総和のことで、入力に依存する関数である。入力が大きいとそれだけ時間がかかる。 アルゴリズムの計算量はオーダー記法で求めることができる。 2進木ソートは、以下の手順で整列するソートである。 1まずa1を根に置く 2i=2、3、。。。、nの順に次を行う Iaiを読み込んだら、根から次のIIを繰り返す IIaiを現在の節点の値と比べ、aiのほうが小さかったら左の子へ進み、そうでなければ右の子へ進む。進んだところに節点がなければIIIに進 IIIそこに節点を作り、aiを格納する 3完成した2進木を通りがけ順に読む 欠点として、入力数列の並び方で計算量がかわる(完成した2進木の深さに依存する)ことや、入力数列のサイズをnとすると、最悪の場合、比較回数はn(nー1)/2になることがあげられる。',\n",
       "  'Q2': 'ソート問題とは、n個のアイテムが与えられたときに、それらをある順序によって並び替えることで計算機科学におけるもっとも基本的な問題の一つ。 昇順(小→大) 降順(大→小) 代表的なソートアルゴリズム 交換型(バブルソート、選択ソート)、分割統治型(マージソート、クイックソート)、選択型(2進木ソート、ヒープソート)、その他(バケツソート、基数ソート) バブルソート 隣り合う要素の代償を比較しながら整列 整列したい数列の長さをnとし、要素の比較回数と交換回数で時間計算量を見積もる 比較回数;常にn(n-1)/2回 交換回数;高々n(n-1)/2回 選択ソート 最大の要素を探し、それと最後の要素を入れ替える 整列したい数列の長さをnとし、要素の比較回数、最大値の更新回数、要素の交換回数で時間計算量を見積もる 比較回数;常にn(n-1)/2回 最大値の更新回数;高々n(n+1)/2回 交換回数;高々nー1回 時間計算量 入力に対して各手順を実行するのにかかる時間の総和 入力によって実行する手順が異なる 入力に依存する関数 入力が大きいとそれだけ時間がかかる 比較的短い入力は時間があまりかからないのでどうでもよい 長い入力に対する挙動が見積れたほうが嬉しい 入力が大きい時の関数の上限を考える 最悪計算量 長さnの入力のうち、最も時間のかかる入力に対する計算量 オーダー記法 関数T(n)に対してT(n)がf(n)のオーダーであるとは、ある正定数c、n0が存在し、任意のn>n0に対してT(n)<=cf(n)が成り立つこと この時T(n)=O(f(n))とかく 2進木ソート 1まずa1を根に置く 2i=2、3、。。。、nの順に次を行う Iaiを読み込んだら、根から次のIIを繰り返す IIaiを現在の節点の値と比べ、aiのほうが小さかったら左の子へ進み、そうでなければ右の子へ進む。進んだところに節点がなければIIIに進 IIIそこに節点を作り、aiを格納する 3完成した2進木を通りがけ順に読む 通りがけ順 節点の値を左の子、自分、右の子の順に読む 欠点 入力数列の並び方で計算量がかをる(完成した2進木の深さに依存する) 入力数列のサイズをnとすると、最悪の場合、比較回数はn(nー1)/2になる',\n",
       "  'Q3': '特になし',\n",
       "  'Q4': '特になし',\n",
       "  'Q5': 'ソートアルゴリズムについてはYouTubeなどで見たことがあったが、今回の授業でさらに理解を深めることができてよかった。'},\n",
       " 'L7': {'Q1': 'ヒープとは節点数n、高さhの2進木が、次の条件を満たす木のことである。 1。深さd(0<=d<=h-1)の節点はちょうど2^d個存在し、深さhの節点は左から順にn-(2^h-1)個存在する。深さh-1までの節点の合計数は(2^h-1)個 2。どの節点の値も、自分の親の値以下である 入力数列のサイズがnのとき、ヒープソートの比較回数は3nlog2nを超えず、その 計算量は2進木ソート、バブルソート、選択ソートよりも少ない。 マージソートはヒープソートと同様に、O(n log n)の比較で要素を整列するアルゴルズムで、 2進木を使用しないのでよりシンプルである。入力数列Aをより小さな数列A1とA2に分割し、整列し終わったA1とA2をマージ(統合)することによってAを整列した結果を得る。サイズn1の数列Aとサイズn2の数列Bをマージするためには、高々n1+n2-1回の比較を行えばよい。またマージソートの比較回数は高々n(log2n)である。 比較ソートアルゴリズムの比較回数について、あらゆる比較ベースのソートアルゴリズムは必ずO(n log n)回の比較を要する。要素を比較する順番=アルゴリズムである。比較ソートアルゴリズムの比較回数は、少なくともn!個の葉を持つ2進木の高さの最小値以上である。ソートに必要な比較回数はnlog2nに比例して増える。 情報検索とは大量のデータから目的に合致したものを取り出すことで、ウェブ検索がそれにあたる。情報検索を高速化する方法の1つに、データをあらかじめソートして2分探索を行う方法がある。 線形探索は整数集合Sが整列されてないとき、先頭から整数Xを探す方法で、最悪の場合、n回の要素比較をしなければならない。線形探索の時間計算量はO(n)。一方2分探索アルゴリズムの計算量は各ステップで探索する範囲は必ず1/2以下になる。最悪の場合(最もステップ数が多い場合)でも、探索範囲のサイズが1になったら必ず終了するので、要素数nの集合に対する2分探索のステップ数log2nを超えない。2分探索の時間計算量はO(log n) である。 キーワードの2分探索の計算量、テキストの2分探索の計算量ともに線形探索よりも2分探索のほうが高速。 接尾辞配列を用いてテキストを2分探索する アイディアが発表されたのは1991年で、それ以降テキストデータをはじめ、DNA 配列など様々な実際のデータに対して幅広く応用されている。',\n",
       "  'Q2': 'ヒープとは節点数n、高さhの2進木が、次の条件を満たす木のこと。 1。深さd(0<=d<=h-1)の節点はちょうど2^d個存在し、深さhの節点は左から順にn-(2^h-1)個存在する。深さh-1までの節点の合計数は(2^h-1)個 2。どの節点の値も、自分の親の値以下である ヒープソート第1ステージ 常にヒープになるように2進木を構成していく。 条件2を満たしている間は、左の子、右の子の順に追加していく。 ヒープソート第2ステージ 第1ステージで完成したヒープから、節点を一つずつ削除しながら、数を並べていく。 1。根から数を取り出して並べる。 2。最後に追加した節点を削除し、その節点の数を根に格納する。 3。ヒープの条件2が満たされないならば、その数と左の子または右の子のうち大きい方の数と交換する。これをヒープの条件2が満たされるまで続ける。 ヒープソートの比較回数 入力数列のサイズがnのとき、ヒープソートの比較回数は3nlog2nを超えない 第1ステージで親と比較するごとに1回 第2ステージで左右の子と比較するごとに2回 n個の節点からなるヒープの高さはlog2nを超えない 各ソートアルゴリズムの計算量 ヒープソート:O(n log n) 2進木ソート、バブルソート、選択ソート:O(n^2) マージソート ヒープソートと同様に、O(n log n)の比較で要素を整列するアルゴルズム 2進木を使用しないので、よりシンプル 入力数列Aをより小さな数列A1とA2に分割し、整列し終わったA1とA2をマージ(統合)することによってAを整列した結果を得る。 マージ 既に入力された2つの入力数列から、整列された1つの数列を出力する操作。2つの数列を先頭から比較し、小さい方を新たな配列に格納する。 マージ操作の比較回数 サイズn1の数列Aとサイズn2の数列Bをマージするためには、高々n1+n2-1回の比較を行えばよい 要素比較を1回行うと、必ず1つの要素が出力される 最終的に出力される数列のサイズはn1+n2であり、最後に残った要素は比較相手がいないので、比較回数は高々n1+n2-1回 数列AとBにおいて値が小さい方から交互にあらを割れるとき、最大の比較回数(n1+n2-1回)となる マージソートアルゴリズム 入力数列に対して、その数列のサイズが1より大きければ、 1、2つの数列に分割し、それぞれをマージする 2、ソート後の2つの数列をマージする これを再帰的に繰り返す マージソートの比較回数 入力数列のサイズをn マージソートの各段において、分割された数列の長さの総和は常にn→格段の要素の比較回数の合計はnを超えない マージソートの段数は高々log2n よってマージソートの比較回数は高々n(log2n) 比較ソートアルゴリズムの比較回数 比較ベースのソートアルゴリズム: 2つの要素の比較結果によって、解の候補を絞っていく。 あらゆる比較ベースのソートアルゴリズムは必ずO(n log n)回の比較を要する! 比較ソートアルゴリズムは決定木で表せる。つまり、要素を比較する順番=アルゴリズムである。 入力数列Aに対するアルゴリズムの比較回数は、決定木の根から葉(Aに対する解)までの辺の数に等しい。よって、アルゴリズムの最悪時の比較回数は、決定木の高さに等しい。 比較ソートアルゴリズムの比較回数は、少なくともn!個の葉を持つ2進木の高さの最小値以上である。 決定木Tの高さをhとする。Tは少なくともn!の葉を持つ。 高さhの決定木(2進木)の葉の数は高々2^hなので、2^h≧n! 両辺に対して、2が底の対数をとると、h≧log2n! Stirlingの公式n!>(n/e)^nより h>log2(n/e)^n=nlog2n-nlog2e。ここで、log2e=1。442695040888963387。。。であるから、n≧4ならば、h>(1/2)nlog2nである。よって、ソートに必要な比較回数はnlog2nに比例して増える。 情報検索 大量のデータから目的に合致したものを取り出すこと ex)ウェブ検索 情報検索を高速化する方法の1つに、データをあらかじめソートして2分探索を行う方法がある。 線形探索 整数集合Sが整列されてないとき、先頭から整数Xを探す方法。 最悪の場合、n回の要素比較をしなければならない Xが右端にある時、≠がn-1回、=が1回 XがSに含まれていないとき、≠がn回 線形探索の時間計算量はO(n) 2分探索 Sの要素が整列されている場合、以下の要領でxを探す方法。 1。真ん中の要素yとxを比較する A)x<yならば、残りの前半部分について1を行う B)x>yならば、残りの後半部分について1を行う C)x=yならば、xが見つかった 2分探索アルゴリズムの計算量 各ステップで、探索する範囲は必ず1/2以下になる。 最悪の場合(最もステップ数が多い場合)でも、探索範囲のサイズが1になったら必ず終了する。よって、要素数nの集合に対する2分探索のステップ数log2nを超えない。 各ステップにおいて、高々1回の要素比較を行う。よって、2分探索の時間計算量はO(log n)である。 キーワードの2分探索の計算量 キーワード集合Kの要素数をnとする。2分探索のステップ数はlog2nを超えない。 パターン文字列pの長さをkとする。各ステップにおいて、高々k回の文字比較を行う。したがって、時間計算量はO(klog n)である。 ちなみに、線形探索した場合の時間計算量はO(kn)なので、2分探索のほうが高速。 テキストの2分探索の計算量 テキスト文字列Tの長さをnとする。Tの接尾辞集合Suffix(T)の要素数はn。よって、2分探索のステップ数はlog2nを超えない。 パターン文字列pの長さをkとする。各ステップにおいて、高々k回の文字比較を行う。したがって、時間計算量はO(klog n)である。 ちなみに、素朴にパターンを探索した場合の時間計算量はO(kn)なので、2分探索のほうが高速。 接尾辞配列を用いてテキストを2分探索するアイディアが発表されたのは1991年で、それ以降、テキストデータをはじめ、DNA配列など、様々な実際のデータに対して幅広く応用されている。 ごく最近までこのような比較的シンプルな探索方法すら知られていなかった。',\n",
       "  'Q3': '特になし',\n",
       "  'Q4': '特になし',\n",
       "  'Q5': 'マージソートは比較的シンプルで理解しやすかった。接尾辞配列を用いてテキストを2分探索するアイディアが発表されたのは1991年とあって、比較的最近でも新たな発見がある情報科学分野はまだまだ発展の余地がありそうだとおもった。'},\n",
       " 'L8': {'Q1': 'データには、1。 物事の推論の基礎となる事実。また、参考となる資料・ 情報。2。 コンピューターで、プログラムを使った処理の対象となる記 号化・数字化された資料。という2つの意味がある。データの例として ・測定値 ・メディアデータ ・ラベルデータ ・ネットワーク(関係データ)が挙げられる。 前後関係のあるデータを系列データという。時々刻々と得られる系列データは時系列データといい、動画像、行動、ジェスチャ、歩行、ゲーム操作、音声信号、対話系列などがある。また、時間とは関係のない系列データには、文字列(文章)、DNA系列がある。 データは構造化データと非構造化データに分けることもできる。 表形式にできるものを構造化データといい、文章など表形式にはならないデータを 非構造化データという。 データを一般的に4分類すると、和や差、積、除算ができる比率データ、和と差ができる間隔データ、四則演算はできないが並べることはできる順位データ、そして形式的に数字になっているだけのカテゴリデータがある。比率データと間隔データを量的データといい、順位データとカテゴリデータを質的データという。・ 現在はあらゆる分野でデータ分析の必要性が高まっている。その要因として、・学術的、社会的要請・データからの要請・データ分析技術の進展の三つが挙げられる。 様々な研究でデータ分析は行われているが、研究者に限らず、誰もが無意識にデータを分析しながら生きている。 データ分析の主な3つのタスクは ・予測 ・(傾向や関連の)発見 ・分類・グルーピングである。 予測には、天気予報などの未来の予測や、画像認識、推量・診断、因果推論、推薦がある。予測モデルができると過去になかった状況に対しても予測可能になる。しかしながら、予測することは容易ではない。過去のデータを十分に集められない場合や、予測結果を決める要因が不明な場合など、様々な問題がある。 発見とは大規模なデータの中に潜む傾向を見つける方法で、発見のための代表的手段には、相関分析や頻出パターン発見がある。 相関分析は相関には強さから傾向を見つけることである。疑似相関や、相関と因果関係は違うことに注意する必要がある。 頻出パターン発見にはバスケット解析が用いられる。買い物かごの中身から商品の購入パターンを発見することができる。 グルーピングとは、データをいくつかのグループに分けることあり、グループ単位でみることで、データ全体の状況把握が容易になる。 グループがあらかじめ決まっている場合は組み合わせ可能で、階層化可能である。 グループがあらかじめ決まっていない場合 クラスタリングを行い、似たデータが同じグループになるようにすることで、自動的にグループを構成する。',\n",
       "  'Q2': 'データとは 1。 物事の推論の基礎となる事実。また、参考となる資料・ 情報。 2。 コンピューターで、プログラムを使った処理の対象となる記 号化・数字化された資料。 データの例 ・測定値 体温、体重、消費カロリー、人流 ・メディアデータ 画像(次スライド)、動画像(ビデオ)、音声 ・ラベルデータ 患者の病名、地点名・駅名、生物種 ・ネットワーク(関係データ) 空手クラブメンバーの仲良し関係 メディアデータの代表例:画像 ・カメラ画像 ・文字、文書、記号、標識、ナンバープレート ・顔、指紋、虹彩、耳、唇、掌の静脈 ・CT・MRI・X線などの医用画像 前後関係のあるデータを系列データという 時々刻々と得られる系列データを時系列データといい、 ・動画像 ・行動、ジェスチャ、歩行、ゲーム操作 ・音声信号、対話系列 ・心拍数変化、呼気量変化 ・環境中のNOx濃度変化、気温変化 ・10年ごとに測定した世界人口などがある。 時間とは関係のない系列データには、 ・文字列(文章) ・DNA系列がある。 データの一般的な4分類( ・量的データ ・比率データ 和、差、積、除算ができる。 Ex。体重、年収、長さ ・間隔データ 積や除算に意味がない。ただし和や差はできる。Ex。(華氏・摂氏で測る)温度、西暦年 ・質的データ ・順位データ 四則演算(加減乗除)すべて意味がない。ただし並べることはできる。・Ex。アンケート結果(5:非常によい、4:よい、3:ふつう、2:わるい、1:非常に悪い)、成績順 ・カテゴリデータ 形式的に数字になっているだけ、Ex。「1:女性、2:男性」、電話番号、背番号、バスの系統番号 構造化データと非構造化データ ・構造化データ 表形式のデータを構造化データという ・非構造化データ 文章、画像、音などがある。「表形式」にはならないので「非構造化データ」と呼ばれる。 あらゆる分野でデータ分析の必要性が高まっている。 要因 ・学術的・社会的要請 客観性・再現性のある(=だれがやっても同じになる)根拠が必要 さらにその根拠を数値として表現する必要 ・データからの要請 データが大規模・複雑・多様化→手計算では無理 分析が待たれるオープンデータの蓄積 ・データ分析技術の進展 計算機リソースの大規模化 数値分析法、機械学習(特に深層学習)の進歩 オープンソース化、無料ライブラリ、技術解説サイト データ分析の主な3つのタスク 1。予測 ・未来の予測;試験勉強、スポーツ、買い物、天気予報 ・画像認識、推量・診断、因果推論、推薦 ・データを用いた予測 ・予測モデルができると過去になかった状況に対しても予測可能になる。 ・予測モデルは様々考えられる モデルによって予測結果は違う(精度が違う) 同じ予測モデルでもあてはめ方は様々 ・予測は難しい 過去のデータを十分に集められない場合がある 予測結果を決める要因が不明な場合がある 現時点と予測時点では状況が違う場合がある どの予測モデルを使えばよいかは、自明ではない 2。傾向や関連の発見 発見とは大規模なデータの中に潜む傾向を見つける方法 ・発見のための代表的手段 相関分析、頻出パターン発見 ・相関分析 相関には強さがある→傾向には強さがある 疑似相関に注意 相関と因果関係は違う ・頻出パターン発見 バスケット解析 3。分類、ルーピング ・データをいくつかのグループに分けること ・グループ単位でみることで、データ全体の状況把握が容易に ・グループがあらかじめ決まっている場合は組み合わせ可能、階層化可能 ・グループがあらかじめ決まっていない場合クラスタリングを行い似たデータが同じグループになるようにすることで、自動的にグループを構成 ・クラスタ(cluster)とは似たデ-タのかたまり',\n",
       "  'Q3': '特になし',\n",
       "  'Q4': '特になし',\n",
       "  'Q5': 'データ分析は日常生活でも活用されており、その必要は年々高まっていると知った。具体例をあげることで、データ分析をより身近に感じた。'},\n",
       " 'L9': {'Q1': 'クラスタリングの結果から、以下の3つのことがわかる。 ・できたクラスタの数から、データ全体の多様性がわかる。 ・各クラスタのデータ数で、各クラスタの勢力がわかる ・各グループの代表例を見ることで、全体を概観可能 しかし、実際のクラスタリングは色々考えるべき点も多い。クラスタリングは「似たデータを同じグループ」にする技術だが、クラスタリングには「絶対的な正解」が存在しないことが多く、「似ている具合」の測り方もじゆうである。これが違えば、クラスタリング結果も当然変わってくる。また、クラスタ数の決め方も多種多様である。 人工知能(AI)とは人間の真似をする機械のことである。AIには特化型AIと汎用AIがある。 特化型AIとは特定の知能だけを人工的に実現したAIのことで、いいかえると特定のことしかできないAIということである。現在利用されているすべてのAIは特定型AIである。 汎用AIとは我々の知能と同じ柔軟さと多機能性を持つ「強いAI」のことで、現状ではまだ検討段階で、「どんなAIが強いAIなのか」も定義できていないのが実情である。 既に使われている人工知能には、音声認識、対話AIやチャットボットなどがあり、コンピュータ将棋やAlphaGoといったゲーム用の人工知能もある。 AIはビジネスでも活用されている。例えばシェアリングエコノミーはインターネット上のプラットフォームを介して個人間でシェア(賃借や売買や提供)をしていく新しい経済の動きで、場所、乗り物、モノ、人、スキル、金の5つがシェア対象となっている。ここでのAIの利用例は、いつ・どこで・どのような共有依頼が来るかの予測やシェアすべきリソース(部屋や車)の最適配分また、需要・供給に応じた利用価格の自動決定(ダイナミックプライシング)などである。ほかにも商品の推薦や顧客監視(サーベイランス)、デジタルトランスフォーメーション(DX)などでもAIが活用されている。 人工知能は機械学習によって作られる。AIは最初から賢いわけではないので、「こういう入力には、こう出力しなさい」といった「例」をたくさん用意し、その通り出力するように、AIを調整する必要がある。十分な例でAIを学習できれば、(学習に使ってない)初めて見る入力にも、正しい出力を返すことができる。機械学習にはデータが重要であり、データが十分にないと、学習しても性能が出せず、データがたくさんあっても、似たようなものばかりでは意味がない。機械学習によって・予測・認識、分類・生成が可能になる。 最近主流のAIは深層ニューラルネットワークである。考え方自体は1970年代から存在しており、・予測・認識・生成など、様々な課題に利用できる。大量データと学習(「深層学習」と呼ばれる)によって、非常に高性能なAIを実現し、入力データに対して、様々な数(パラメータ)を掛けたり足したりすることを何回も繰り返すことで、最終的な答えが出る。また学習によりパラメータを適切に設定することができる。 AIの性能が、最近劇的に向上したのには以下の3つの理由がある。 1。深層ニューラルネットワークの利用 2。非常に大規模なデータが手に入るようになったが手に入る 3。コンピュータのパワーアップ また、「3つの理由」をさらに加速させる「オープン戦略」に、オープンソースやオープンデータ、Webから無料で使えるコンピュータなどがある。これにより、無料で、誰でも、AI関係の開発・研究ができる点がブームの背後になった。 現在のAIは全く万能ではない。実現できているのは「特化型AI」のみで、画像認識など、特定のことしかできず、汎用AIはまだ研究途上である。また、十分なデータがなければ正しく動かないが、プライバシーにかかわるデータは集めづらく、大地震などの希少データは収集不可能なので、課題は多い。 他にも次に挙げる問題が存在している。 ・ケース1敵対的事例 ・人間だと絶対起こさないような誤りを起こす ・ケース2フレーム問題 ・特定の範囲のことしか考えが及ばない ・人間がフレーム問題をどう回避しているのかは依然不明 ・ケース3判断根拠が不明確 ・AI(深層ニューラルネット)の判断根拠を見出すのが難しい ・現在この問題を解決するべくexplainable AI(説明可能AI)の研究が数多く行われている',\n",
       "  'Q2': 'クラスタリングの結果からわかること ・できたクラスタの数から、データ全体の多様性がわかる。 ・各クラスタのデータ数で、各クラスタの勢力がわかる ・各グループの代表例を見ることで、全体を概観可能 クラスタリングの実際:色々考えるべき点も多い ・クラスタリングは「似たデータを同じグループ」にする技術 ・データが「似ている」とは →「どこが」似ているか →「どれぐらい」似ているか クラスタリングには「絶対的な正解」が存在しないことが多い ・「似ている具合」の測り方について規定はない ・測り方が違えば、クラスタリング結果も当然変わってくる ・クラスタ数の決め方も多種多様 ・数学に基づいたデータ解析を使えば、ただ一つに定まるというわけではない 人工知能=人間の真似をする機械 特化型AIとは特定の知能だけを人工的に実現したAIのことで、つまり特定のことしかできないAIということである。 ・現在利用されているすべてのAIは特定型AI EX)画像認識AI、対話AI(チャットボット)、将棋AI、天気予報AIなど。 汎用AIとは我々の知能と同じ柔軟さと多機能性を持つ「強いAI」のこと。現状ではまだ検討段階で、「どんなAIが強いAIなのか」も定義できていない。 ・実現するための方法に「全脳シミュレーション」がある。 ・1脳を構成する神経細胞(ニューロン)の個々の動きを再現 ・2それら神経細胞(ニューロン)を繋げて脳全体を再現 既に使われている人工知能 ・音声認識、対話AI・チャットボット・顔認識・画像認識・医療診断・推薦、広告配信 ゲームで使われた人工知能 ・IBM Watson・コンピュータ将棋・AlphaGo AI等を活用したビジネス 1。シェアリングエコノミー ・インターネット上のプラットフォームを介して個人間でシェア(賃借や売買や提供)をしていく新しい経済の動きで、場所、乗り物、モノ、人、スキル、金の5つがシェア対象。 ・AIの利用例 ・いつ・どこで・どのような共有依頼が来るかの予測 ・シェアすべきリソース(部屋や車)の最適配分 ・需要・供給に応じた利用価格の自動決定(ダイナミックプライシング) 2。商品の推薦 3。顧客監視(サーベイランス) ・カメラを店舗や街中に設置して、ヒトの動きやモノの動きを観察するサービス ・防犯や異常検知、顧客の行動把握に利用。河川氾濫や渋滞監視等にも使われる。 4。デジタルトランスフォーメーション(DX) ・AIを含む様々な情報処理技術によりビジネスや様々な組織の効率を向上させること ・DXの例:ロボティックプロセスオートメーション(RPA) ・複数のファイルを解析・統合し、作業に必要なファイルを自動生成画 ・DXの例:名刺管理 ・社内に集まった顧客の名刺を管理、ある商談に適切な顧客を自動抽出 ・今後DXが多用されると考えられているオフィス業務 ・意思決定予測人事、経理ミス発見、業績評価など 機械学習とは「こういう入力には、こう出力しなさい」といった「例」をたくさん用意し、その通り出力するように、AIを調整することで、これを教師あり機械学習と呼ぶ。十分な例でAIを学習できれば、「(学習に使ってない)初めて見る入力にも、正しい出力を返すことができるが、データが十分にないと、学習しても性能が出せず、データがたくさんあっても、似たようなものばかりでは意味がない。 機械学習によって可能になる事 ・予測 ・認識、分類 ・生成 深層ニューラルネットワークとは最近主流のAIで、考え方自体は1970年代から存在している。予測・認識・生成など、様々な課題に利用でき、大量データと学習(「深層学習」と呼ばれる)によって、非常に高性能なAIを実現した。入力データに対して、様々な数(パラメータ)を掛けたり足したりすることを何回も繰り返すことで、最終的な答えが出て、また学習によりパラメータを適切に設定することができる。 AIの性能が、最近劇的に向上した3つの理由 1。深層ニューラルネットワークの利用により、パラメータ数が膨大(数千万~数億個のパラメータを持つ)に=調整の自由度が非常に高い 2。非常に大規模なデータが手に入るようになり、深層ニューラルネットワークの大量のパラメータを調整(=学習)するのに十分なデータが手に入るようになった。 3。コンピュータのパワーアップにより、大量のデータを用いた深層ニューラルネットワークの学習が可能になった 「3つの理由」をさらに加速させる「オープン戦略」 ・オープンソース(ソース=source-code=プログラム) ・github ・深層学習の各種ライブラリー(ライブラリー自分で作ると面倒なプログラムを誰かが使いやすいようにあらかじめ作成し、公開してくれているもの) ・keras、pytorch、tensorflow、。。。 ・オープンデータ ・GAFAによるデータ提供 ・各種コンペティション ・Kaggle ・Webから無料で使えるコンピュータも ・Google Colaboratory(2021年3月現在無料) 無料で、誰でも、AI関係の開発・研究ができる点が ブームの背後に 実現できているのは「特化型AI」のみで、現在のAIは全く万能ではない。画像認識など、特定のことしかできず、汎用AIはまだ研究途上である。また、十分なデータがなければ正しく動かないが、プライバシーにかかわるデータは集めづらく、大地震などの希少データは収集不可能。 ・他にも次に挙げる問題が存在 ・ケース1 敵対的事例 ・人間だと絶対起こさないような誤りを起こす ・ケース2 フレーム問題 ・特定の範囲のことしか考えが及ばない 人間がフレーム問題をどう回避しているのかは依然不明 ・ケース3 判断根拠が不明確 AI(深層ニューラルネット)の判断根拠を見出すのが難しい 現在この問題を解決するべくexplainable AI(説明可能AI)の研究が数多く行われている',\n",
       "  'Q3': 'とくになし',\n",
       "  'Q4': 'とくになし',\n",
       "  'Q5': '自分たちが普段使っている人工知能は万能のように思っていたが、実際は特定の分野にのみ特化しているだけで、汎用AIは完成されていないと知り、AIもまだまだ発展の余地があると思った。'},\n",
       " 'L10': {'Q1': '非構造化データとは文章、画像、音など、表示形式にならないデータのことを言い、非構造データ処理には、言語処理、画像処理、音声・音楽処理の3つがある。 言語処理 言語データは非構造化データの代表例で、文字列で表されるデータのことである。SNSやネットでは、言語データが日々大量に生まれており、自然言語処理(natural language processing)によって頻出言語の分析や翻訳、検索など様々なことができる。また、言語処理で意味解析やパラフレーズ解析、センチメント解析、さらには文章生成もできる。 画像処理 身の回りにある画像データにはカメラ画像や文字、顔、CT、MRI、X線などの医療用画像などがある。画像データに関する様々な分析課題には画像認識やコンピュータビジョン、画像生成などがある。 音声、音楽処理 コンピュータにとっては「音」もデータである。音には ・人間の声である音声 ・音楽 ・雑音や騒音、生活音などの一般的な環境音 の三種類がある。 音声データの分析には音声認識、話者認識、感情認識、音声合成がある。また音楽データの分析には楽曲分析、自動作曲および作曲支援、音響分析などがあり、環境音データの分析には環境音認識、音源分離、音源同定、異常音検出がある。 パターン認識 パターン認識とは画像や音声、テキストなど、様々なデータを対象として、それが何であるかを当てる方法である。パターン認識には病気診断、行動認識、感情認識、環境認識 コンピュータによるパターン認識は、認識できる対象(クラス)が事前に決められているという前提があるので、自由に何でも認識できるわけではない。しかし身近になりつつある画像認識もあり、顔認識、食事認識、物体認識、医療用画像認識などがある。 コンピュータによるパターン認識の基本原理は自分が知っているものと似ているものを探すことである。 最近のパターン認識は大量のデータと機械学習を組み合わせたもので、大量のデータを準備して、コンピュータに境界線を引いてもらうことである。境界線ができれば、あとは、それのどちら側にあるかで、認識が可能となる。 パターン認識の応用例には自動運転などの自動化技術や異常検出がある。',\n",
       "  'Q2': '文章、画像、音など表示形式にならないデータを非構造化データと呼ぶ 非構造データ処理には言語処理、画像処理、音声・音楽処理の3つがある。 1。言語処理 言語データは非構造化データの代表例で、文字列で表されるデータのことである。 自然言語処理(natural language processing)とは、言語データをコンピュータによって分析する技術のことで、頻出言語処理、翻訳、検索、要約など様々なことができる。また意味解析やセンチメント解析、文章生成といった高度なことができる。 2。画像処理 画像データにはカメラ画像、文字、顔、CT・MRI・X線などの医療用画像などがあり、画像認識、コンピュータビジョン、画像処理など様々な分析ができる。 3。音声、音楽処理 時々刻々と変わる音の波の高さを数値化することで「音」もデータにすることができる。 音には・人間の声である音声・音楽・雑音や騒音、生活音などの一般的な環境音といった三種類の音がある。 音声データの分析には音声認識、話者認識、感情認識、音声合成があり、音楽データの分析には楽曲分析、自動作曲および作曲支援、音響分析、音楽認識、楽曲推薦、音楽データ圧縮がある。環境音データの分析には環境音認識、音源分離、音源同定、異常音検出がある。 パターン認識とは画像や音声、テキストなど、様々なデータを対象として、それが何であるかを当てる方法で、病気診断、行動認識、感情認識、環境認識などが例である。人間は画像認識や音声認識を無意識・高精度・高速に実行可能なのでパターン認識は人間には簡単だが、コンピュータは個々の物体等はある程度認識できても、それらの関係、全体的な雰囲気などは理解できないのでコンピュータにとっては難しい。コンピュータによるパターン認識は自由に何でも認識できるわけではなく、認識できる対象(クラス)は、事前に決められている。身近な画像認識には顔認識、食事認識、物体認識、医療用画像認識、音声認識、行動認識、テキストトピック認識、文字認識がある。 コンピュータによるパターン認識の基本原理は自分が知っているものと似ているものを探すことであるがそれも容易ではない。 最近のパターン認識は大量のデータと機械学習を掛け合わせたものである。大量のデータを準備して、コンピュータに境界線を引いてもらうことで認識が可能になる。 パターン認識の応用には自動運転、自動診断、無人店舗・無人工場・植物工場、自動採点といった自動化技術や、人々を対象とした異常検出、食品や生産物の異常検出、機械・建造物・コンピュータシステムの異常検出などの異常検出がある。',\n",
       "  'Q3': '特にありません。',\n",
       "  'Q4': '特にありません。',\n",
       "  'Q5': '文章や音声、音楽を解析するだけではなく、それを利用して文章や音楽の創作も可能になったことで、コンピュータもいよいよ人間味が増してきたと感じた。'},\n",
       " 'L11': {'Q1': '統計調査を使ってできる限り正確なデータに基づいて調査したいとき、データを集める方法には、全数調査と標本調査がある。 理想的状況は全数調査である。全数調査とは調査対象をすべて調べることで、すべての母集団を調査できる環境が整っている場合や母集団の規模が小さいとき、全数調査が使える。 全数調査のメリットは調査で得られたデータは真実であることである。一方、全数調査のデメリットには一般的な統計調査で全数調査は事実上不可能や調査に対する労力が単純に大きいことが挙げられる。 全数調査が無理な場合、標本調査を行う。調査対象となる母集団の一部を取り出して調査することで、標本の抽出方法には優位抽出法と無作為抽出法がある。 有意抽出法は調査の企画者らが独自の判断で標本を抽出することで、有意抽出法の代表例には便宜抽出がある。 有意抽出法のメリットは調査対象を「独自の判断で」絞り込めるので、データ収集が楽なことで、デメリットは独自判断で調査が行われるため、調査結果が母集団全体を代表していないことが多い。これを標本選択バイアスという。独自判断が誤っていれば、真実とは異なる結果が出てくる可能性がある。 なるべく標本選択バイアスを入らないようにデータを収集するために単純純無作為抽出法を用いて母集団から直接ランダムにデータを選べばよい。またランダムにグループを選択し、グループ内でランダムに抽出する、多段抽出法も有効である。 その他、ありがちな標本選択バイアスには既に取得されたデータ(=過去のデータ)から未来を予測しようとするものがある。また、標本選択バイアス以外のバイアスに帰納バイアスがある。帰納バイアスとは利用した方法の性質に依存したバイアスのことである。 個人情報は適切に守られるべきデータで、個人情報保護法によって保護されている。個人情報の適正かつ効果的な利用が、産業の創出等に有用であることを踏まえた上で、個人の権利利益を守ることを目的としており、両者のバランスが重要 個人情報の第三者提供は原則として、データ所有者本人への同意なしには無理であるが、以下の例外に当てはまるときは可能となる。 ・例外1人の生命・身体・財産の保護のために必要がある場合 ・例外2つぎの条件をすべて満たすとき ・本人の意向により「提供停止(オプトアウト)」可能な場合。ただし、要配慮個人情報はオプトアウト可能でも第三者提供NG ・下記を本人に知らせておく ・第三者への提供を利用目的とすること、そのデータ項目、提供の方法 ・オプトアウトできること、さらにオプトアウトの受付方法 ・個人情報保護委員会に届け出 ・例外3匿名加工情報になっている場合 オープンデータ・とは行政機関(国や地方自治体)が保有する公共データのうち 1。二次利用可能 2。機械判読に適している 3。無償で利用できる 公開データのことである。 オープンデータの意義・目的は ・国民参加・官民協働の推進による諸課題の解決、経済の活性化 ・行政の高度化・効率化 ・透明性・信頼性の向上 ・地域の課題の解決 がある。 行政(国や自治体)によるオープンデータ推進により、官民データ活用推進基本法が制定され、地方自治体に対し、自らが保有する官民データについて国民が容易に利用できるよう必要な措置を講じることを義務化した。さらに平成29年に閣議決定された「世界最先端IT国家創造宣言・官民データ活用推進基本計画」において、令和2年度までに地方自治体のオープンデータ取組率100%を目標とすることが明記された。 オープンデータに関する地方自治体の取り組み状況は政府CIOポータルで確認可能で、自治体は防災や観光、子育てのためにオープンデータを活用している。 ホームページに公開しただけではオープンデータとは言えず、オープンデータと言うためには、二次利用のための条件を掲載し、機械判読に適した形式で無償提供される必要がある。',\n",
       "  'Q2': '全数調査とは、調査対象をすべて調べることである。すべての調査対象は「母集団」と呼ばれる。 全数調査が使える場面は国勢調査といったすべての母集団を調査できる環境が整っている場合や、クラス内の学生の成績調査などの母集団の規模が小さいとき。 ・全数調査のメリット ・調査で得られたデータは真実 ・全数調査のデメリット ・一般的な統計調査で全数調査は事実上不可能 ・調査に対する労力が単純に大きい 標本調査とは調査対象となる母集団の一部を取り出して調査すること 有意抽出法とは調査の企画者(ら)が独自の判断(ある一定の法則や何かしらの固定概念)で標本を抽出することで代表例に便宜抽出がある。便宜抽出とは収集しやすい調査対象から標本を抽出すること。 有意抽出法のメリット ・調査対象を「独自の判断で」絞り込めるので、データ収集が楽 ・デメリット ・独自判断で調査が行われるため、調査結果が母集団全体を代表していないことが多い(標本選択バイアスがかかる) ・独自判断が誤っていれば、真実とは異なる結果が出てくる可能性 なるべく標本選択バイアスを入らないようにデータを収集するには、母集団から直接ランダムにデータを選ぶ単純純無作為抽出法や、ランダムにグループを選択しグループ内でランダムに抽出する多段抽出法がある。 その他、ありがちな標本選択バイアスには、 ・既に取得されたデータ(=過去のデータ)から未来を予測 ・入試のテストの点数から、入学後の成績を推定 ・現社員全体のアンケートに基づく、方針決定 ・メディアの街頭インタビュー などがある。 標本選択バイアス以外のバイアスには、 ・帰納バイアス(利用した方法の性質に依存したバイアス) ・アノテーションバイアス(人間がつけたアノテーション(データの説明)にかかる偏り) などがある。 個人情報保護法は、正式名称を「個人情報の保護に関する法律」といい、その名の通り、個人情報の保護を目的とする法律である。ただ単に情報の利用を制限するだけではなく、個人情報の適正かつ効果的な利用が、産業の創出等に有用であることを踏まえた上で、個人の権利利益を守ることを目的としており、両者のバランスが重要 個人情報保護法による「個人情報」の定義は、生存する個人に関する情報であって、以下のいずれかに該当するもの ・氏名、生年月日等特定の個人を識別することができるもの(他の情報と容易に照合することができ、それにより特定個人の識別が可能になるものを含む) ・個人識別符号が含まれるもの ・身体の特徴を電子化したもの(生体情報)→例:DNA、容貌、虹彩、声紋、指紋など ・個人がサービスを利用or商品を購入する際に割り当てられる符号→例:旅券番号、免許証番号、住民票コード、個人番号 個人情報の第三者提供は原則として、データ所有者本人への同意なしには無理である。 ・例外1人の生命・身体・財産の保護のために必要がある場合 ・例外2つぎの条件をすべて満たすとき ・本人の意向により「提供停止(オプトアウト)」可能な場合。ただし、要配慮個人情報はオプトアウト可能でも第三者提供NG ・下記を本人に知らせておく ・第三者への提供を利用目的とすること、そのデータ項目、提供の方法 ・オプトアウトできること、さらにオプトアウトの受付方法 ・個人情報保護委員会に届け出 ・例外3匿名加工情報になっている場合 要配慮個人情報とは、本人の人種、信条、社会的身分、病歴、犯罪歴、犯罪被害の事実及び本人に対する不当な差別偏見が生じる恐れのあると政令で定める要配慮情報のことである。要配慮個人情報は本人の同意を得ない取得が原則禁止で、取り扱いが特に厳重 匿名加工情報とは、個人を特定できないように加工した個人情報で、匿名加工情報の第三者提供する旨の明示・公表等は必要。また、名前を隠すだけではNG オープンデータとは行政機関(国や地方自治体)が保有する公共データ(政府や独立行政法人・自治体だけでなく、公益企業など民間事業者や個人が保有し、二次利用可能な形で公開されるものも含む)のうち 1。二次利用可能 2。機械判読に適している 3。無償で利用できる 公開データのこと オープンデータの意義・目的には以下のものがある。 ・国民参加・官民協働の推進による諸課題の解決、経済の活性化 ・行政の高度化・効率化 ・透明性・信頼性の向上 ・地域の課題の解決 ・政策は、「声の大きな人」の意見で決まるのではなく、データに基づいた客観的な分析に基づいて決まるべき(Evidence-based policy making (EBPM)と呼ばれる) 官民データ活用推進基本法(平成28年法律第103号)第11項の規定により、地方自治体に対し、自らが保有する官民データについて国民が容易に利用できるよう必要な措置を講じることを義務化。さらに平成29年に閣議決定された「世界最先端IT国家創造宣言・官民データ活用推進基本計画」において、令和2年度までに地方自治体のオープンデータ取組率100%を目標とすることが明記された。 オープンデータに関する地方自治体の取り組み状況は政府CIOポータルで確認可能で、自治体オープンデータの民間活用例として、防災、観光、子育てに活用されている。 オープンデータと言うためには、1。二次利用のための条件を掲載し、2。機械判読に適した形式で、3。無償提供する必要がある。 クリエイティブ・コモンズとは、クリエイティブ・コモンズ・ライセンス(CCライセンス)を提供している国際非営利組織とそのプロジェクトの総称 CCライセンスとは、インターネット時代のための新しい著作権ルールのことで、作品を公開する作者の、「この条件を守れば私の作品を自由に使って構いません」という意思表示である。CCライセンス利用で作者は著作権を保持したまま作品を自由に流通させることが可能となり、受け手はライセンス条件の範囲内で再配布やリミックスなどが可能になる。 クリエイティブ・コモンズ・ライセンスのレベル BY: 著作権者の表示 NC: 商用利用不可 ND: 改変不可 SA: 改変版についても、レベルを継承、 CCを付けて公開することで各市町のクレジット(市名、データ名等)を表示すれば、利用者が営利目的を含めて自由にデータを改変、複製、再配布することが可能になる。 CCがないとホームページの情報を利用する際、著作権処理(使用許可等)に手間、時間、費用等がかかる。また、利用に制約があり、利用者は自由に編集・加工ができない CCがあると出典を明記すれば、利用者は、著作者の許可を得ずとも自由にホームページ情報の二次利用が可能となり、利用者は、自由に編集・加工ができるため、他のデータとも組み合わせて利用拡大が見込める',\n",
       "  'Q3': '特にありません。',\n",
       "  'Q4': '特にありません。',\n",
       "  'Q5': '今後オープンデータが広く普及し、活用されることで、日本経済や私たちの暮らしはより良いものになると感じた。'},\n",
       " 'L12': {'Q1': 'ベクトルとは、複数の数値をカタマリにしたもので、()の中にカンマで区切って書く。その順番に意味がある。 ベクトル表現されたデータの分析には、線形代数もよく使われ、行列もデータ表現に使われる。 ベクトルはデータの組み合わせなので、ベクトルでデータ分析すると、1つの組み合わせでは分からないことも、多数のデータを用意することで、データ間の関係が見えてくる データ解析における「距離」はもっと自由である。ここでの距離はデータ間の差異(似てない具合)であり、距離が小さい2データは「似ている」といえる。単位がある場合もない場合もある。 数学的には、非退化性・対称性・三角不等式の3条件を満たすd(x、y)をx、 y の「距離」と呼び、条件を満たすなら、何でも「距離」といえる。 類似度は距離の反対の概念で、大きければ大きいほど似ている(距離は小さいほど似ている)。類似度は距離ほど厳密に定義されておらず、類似度は正も負の値もとる(距離は0以上)。また、三角不等式のような条件もない モノをベクトルで表せば、様々な種類の距離や類似度が使え、距離や類似度に基づいた分析例には、相同性検索・クラスタリング、系統分類・判定・異常検知などがある。 距離は「データ解析の基本」であり、距離は1種類ではない。距離が変われば、データ解析結果は「まるっきり」変わるので、データや解析問題の性質に合致した「距離」を選ぶ必要がある。 最も代表的な距離はユークリッド距離でありユークリッド距離以外には、L1距離(マンハッタン距離)、max距離、ハミング距離、編集距離(edit distance)などがある。 類似度にはJaccard係数(類似度)、コサイン類似度などがある。 距離や類似度を応用してデータ集合のグルーピングやデータの異常度調査、データの「認識」ができる',\n",
       "  'Q2': 'ベクトルとは複数の数値をカタマリにしたもの。()の中にカンマで区切って書き、順番に意味がある ベクトル表現されたデータの分析には、線形代数もよく使われ、行列もデータ表現に使われる。ベクトル(データ)の集まりとしての行列や、対応関係(ネットワーク)の表現としての行列など、線形代数は実はデータ分析で大活躍する ベクトルはデータの組み合わせなので、ベクトルでデータ分析すれば、1つの組み合わせでは分からないことも、多数のデータを用意することで、データ間の関係が見えてくる 「近い/遠い」や「似ている/似てない」はデータ分析の基本的な道具で、データをまとめる、区別することでデータを識別する データ解析における「距離」は日常会話における「距離」よりももっと自由で、データ間の差異(似てない具合)を表している。距離が小さい2データは「似ている」という事で、単位がある場合もない場合もある。 数学的には、次の3条件を満たすd(x、y)をx、 y の「距離」と呼ぶ ・非退化性(同じものだけ距離がゼロ): x = y d(x、y) = 0 ・対称性(「xからyへ」と「yからxへ」の距離は同じ) : d(x、y) = d(y、x) ・三角不等式(寄り道したら遠くなる): d(x、z) + d(z、y)d(x、y) これらは「距離の公理」と呼ばれている。この条件を満たすなら、何でも「距離」といえる。 類似度は距離の反対の概念で、大きければ大きいほど似ている(距離は小さいほど似ている)。類似度は距離ほど厳密に定義されておらず、正も負の値もとる(距離は0以上)。また、三角不等式のような条件もない。 モノをベクトルで表せば、様々な種類の距離や類似度が使え、距離や類似度に基づいた分析例には、相同性検索やクラスタリング、系統分類、判定、異常検知などがある。 距離は「データ解析の基本」であり、距離は1種類ではない。距離が変われば、データ解析結果は「まるっきり」変わるので、データや解析問題の性質に合致した「距離」を選ぶ必要がある。 「データ間の距離」とは、2データがどれぐらい違うか(=離れているか)を意味し、最も代表的な距離にユークリッド距離がある。 ・簡略表現法 xとyの距離の二乗 = (x - y)2 「要素ごとの差の二乗の合計」という意味結果はベクトルではなく、数値 xとyの距離=√((x-〖y)〗^2 ) = || x – y || ユークリッド距離以外にも様々な距離がある。 L1距離(マンハッタン距離)は、碁盤の目状で斜めにいけない街マンハッタンが由来で、市街地距離と呼ばれることもある。 max距離は「1要素でも大きく違ったら、それは結構違うのだ」としたい場合に使う。ただし1要素間でのみの評価になるので、全体的な差異は評価できない ハミング距離は長さの同じ2系列間の距離を表し、違う要素の数が距離となる。 編集距離(edit distance)も2系列間の距離を表すが、系列の長さが違っても大丈夫なのがメリット。置換、挿入、削除の最小回数がわかり、ハミング距離を一般化したものである。Levenshtein距離ともいう。 Jaccard係数(類似度)は「(数学の)集合」の類似度を表す。集合は何かの集まりを表し、入ってる/入ってないだけが重要で、どのくらい共通しているかを測っている。 コサイン類似度は方向性の類似度を測る方法で、長さはどうでもいい時に使う。 距離や類似度を応用してデータ集合のグルーピングをしたり、データの異常度をはかったり、データの「認識」ができる。画像認識やクラスタリング、データから様々な知見を発見するのに利用される。',\n",
       "  'Q3': '特にありません。',\n",
       "  'Q4': '特にありません。',\n",
       "  'Q5': 'データ分析ではベクトルや行列といった数学的要素が多く活用されていると分かって、数学の大切さを実感した。'},\n",
       " 'L13': {'Q1': '可視化とはデータを直感的に理解できる図にすることで、膨大なデータを把握しやすくするために必要である。可視化をするときは目的と条件によって適切なものを選択する必要がある。不適切な可視化は誤解を生んだり、不誠実な印象を招くからだ。 可視化手法は様々あり、 ・データの分布を調べたい→ヒストグラム ・データ分布の比較したい→箱ひげ図 ・数値データの比較したい→棒グラフ ・データ全体に対しての各データの割合を把握したい→パイチャート ・2種類のデータの傾向を同時に把握したい→散布図 ・2種類のデータの分布を調べたい→ヒートマップ ・データ点の間の変化の傾向を把握したい→折れ線グラフ ・データ間の関係性・接続を把握したい→無向/有向グラフ といったように用途に合わせて使い分ける。 データを可視化することで、次に行う分析の方針決定につながる。',\n",
       "  'Q2': '可視化とはデータを直感的に理解できる図にすること 膨大なデータの把握は困難なので可視化が必要 可視化をするときは、不適切な可視化は誤解を生んだり、不誠実な印象を招かないよう各々の可視化手法がどのような効果をもち、どのような場合に使うか把握する 棒グラフは棒の高さで値を表現した図で、数値データの比較を行う場合に有効。エラーバーをつけると標準偏差でデータの散らばり具合を表現できる。棒グラフの縦軸の原点を恣意的に選ぶべきではない。 ヒストグラムは「頻度(回数)」を表す棒グラフのことで、区間ごとにデータが観測された回数を集計した図。データの分布を調べる場合に有効。ビン幅によって見た目が大きく変わることに注意する必要がある。 箱ひげ図はデータ分布を最大値・最小値・四分位数で簡易的に表現した図で、データ分布の比較する場合に有効。 パイチャートは円の面積で割合を表示する図で、データ全体に対しての各データの割合を把握する場合に有効。3Dパイチャートは、立体的になることで、面積の大小関係が異なるように見えるので、誤解しやすい。 折れ線グラフはデータ点の間を直線で結んだ図で、データ点の間の変化の傾向を把握する場合に有効。折れ線グラフの線は観測されなかった点と点の間を補間する意味合いなので、関連のないデータを線で結ぶのは不適当。折れ線グラフを比較する際には縦軸の範囲や幅を同じにしなければならない。 無向グラフは点(ノード)と辺(エッジ)でデータ間の関係を表現した図で、データ間の関係性・接続を把握する場合に有効。点の配置の仕方を工夫して見やすくできる。 有向グラフは辺に向きがあるグラフ。有向グラフに色や地図を組み合わせると、ノードで場所、エッジで人の移動方向を表現し、色で人流の量を表現できる。 散布図は平面上に点の集合としてデータを表現した図で、2種類のデータの傾向を同時に把握したい場合に有効。色をつけることによってたくさんの値を可視化可能。形や点の大きさを変えることでさらに多くの情報を同時に可視化できる。しかしデータ数が膨大になると散布図は不向きになる。 ヒートマップ(2次元ヒストグラム)は色によって数値を表現した図で、2種類のデータでどんな値がどのぐらいあるのかを調べる場合に有効。ヒートマップの色の設定で同じデータでも印象・見方が大きく異なる。地図とくみあわせることで、地理的な情報であれば地図上に重ねると効果的。 1データ=2次元なら散布図がベストだが、3種類以上の要素を持つデータをまとめた散布図でもデータ分布を表現できていない。そこで要素数を削ってデータを2種類の要素で表現できるとしたら可視化できる。散布図行列は、3つ以上の要素を可視化するために、二つずつ要素を組み合わせて、複数の散布図を作ってできる。 可視化されたデータを見ることで、次に行う分析の方針決定につながる。',\n",
       "  'Q3': '特にありません。',\n",
       "  'Q4': '特にありません。',\n",
       "  'Q5': '普段何気なくグラフや図を使っているが、表現方法や用途についてよく考えて使う必要があるとかんじた。'},\n",
       " 'L14': {'Q1': '画像は理系・文系問わず、有用なデータである。 画像解析には画像処理、物体検出・物体認識、領域分割、3次元再構成がある。 基本的な画像解析にはフィルタ処理と2値化がある。 フィルタ処理とは出力画像の1画素の値を求めるために入力画像のある領域内の画素値を用いる濃淡変換の処理で、滑らかな出力画像の1画素の値を入力画像で同じ位置の画素の値とその周辺の画素の値から計算する。フィルタ処理をすることで、平滑化やエッジ抽出ができる。平均化フィルタや重み付き平均化フィルタといった線形フィルタを用いて、画像に含まれる不要な濃淡変動を軽減することができるが、画像全体がぼやけるという欠点もある。そこで非線形フィルタであるエッジ保存平均化フィルタを用いることで、画像がぼやけることなく平滑化できる。エッジ抽出は、微分フィルタやソーベルフィルタ、ラプラシアンフィルタを用いて画像中の明るさが急激に変化する部分(エッジ)を取り出す処理である。 2値化とは、黒(文字)と白(紙)の2値の画像に変換する処理のことで、2値化による文字領域候補の抽出により文書のデータ化ができる。 相関とは2つの量の関係性を説明する方法で、分散とどうようにデータの広がり具合の指標となる。相関は相関係数により数値で表現する(定量化する)ことができ、相関係数pが分かれば、分布の形を少し想像することができる 統計的検定は統計的に差を評価する枠組みで、帰無仮説や対立仮説を用いてデータが得られる確率を評価する。',\n",
       "  'Q2': '画像は理系・文系問わず、有用なデータで、人や物の見た目や形状、環境の様子などを調べたいときに便利 画像処理は画質補正や画像合成など画像に対して行う処理全般のことで、画像を分析しやすくするための処理を含む 物体検出・物体認識は画像中の物体の位置やカテゴリを認識するもので、自動運転やロボットに応用可能 領域分割は画像中から解析したい対象領域を抽出することで、植物の生長モデリングや自動運転システムに応用可能 3次元再構成は2枚以上の画像から3次元情報を復元する技術で、実空間の3次元モデリングやナビゲーションシステム、ARアプリケーションなど様々なことに応用できる。 フィルタ処理とは出力画像の1画素の値を求めるために入力画像のある領域内の画素値を用いる濃淡変換の処理のことで、滑らかな出力画像の1画素の値を入力画像で同じ位置の画素の値とその周辺の画素の値から計算する。 平滑化画像に含まれる不要な濃淡変動を軽減するために用いる手法で、平均化フィルタ、重み付き平均化フィルタ、エッジ保存平均化フィルタなどがある。 平均化フィルタはフィルタによって覆われる領域内の画素値の平均を求めることである。 重み付き平均化単純な平均値ではなく、フィルタの原点に近いほど大きな重みをつけるのが特徴で、重みをガウス分布に近づけたものをガウシアンフィルタという 平均化フィルタを行うと濃淡変動が滑らかになるが、画像全体がぼける。 線形フィルタは画像に含まれるノイズなどの不要な濃淡変動を軽減できるが、画像にもともとあるエッジもなめらかになるという欠点がある。 非線形フィルタは画像のエッジも保存可能で、メディアンフィルタ{領域内の中央値(メディアン)を出力とするフィルタ}などがある。 エッジ抽出とは、画像中の明るさが急激に変化する部分(エッジ)を取り出す処理で、画像の局所的な特徴を抽出する。ことである。画像中から特徴や図形を検出したりするための前処理として利用される。 微分フィルタはデジタル画像では注目画素とその隣接画素との差分を求めることを言う。縦方向と横方向の微分フィルタが必要 ソーベルフィルタはノイズを抑えながらエッジを抽出する方法である。 ラプラシアンフィルタを使うと、方向に依存しないエッジが直接得られる 鮮鋭化とは、エッジ部分の両側で明るい部分をより明るく、暗い部分をより暗くし、エッジの傾斜を急にすることである。 2値化とは黒(文字)と白(紙)の2値の画像に変換する処理のことである。 しきい値の設定は2値化の結果に大きく影響し、しきい値の設定によって文字が欠けたり、紙の領域を文字領域候補としたりしてしまう。 相関とは2つの量の関係性を説明する方法である。 分布とはどんなデータが、どのくらいあるかを表したもので、分布からは最小・最大、平均・分散などが見えてくる。 分散は広がり具合を数値で表現(=定量化)したものである。 相関はもう1つの広がりの指標で、無相関、正の相関、負の相関の3ケースがある。 相関係数によって相関を数値で表現する(定量化する)ことができ、相関係数pが分かれば、分布の形を少し想像することができる 統計的検定とは統計的に差を評価する枠組みである。 統計的検定の基本アイディア 1。「差がない」という仮説(帰無仮説)を考える 2。仮説を信じてみる 3。信じた上でそれが起きる確率を過去のデータから計算する 4。基準とする確率(有意水準)と比較する ・基準より低い ・ → 基準から考えるとあり得ないことが起きている ・ → 仮定した仮説がそもそも間違っていた ・ → 帰無仮説を棄却する ・基準より高い ・ → 基準から考えると起きてもおかしくないことが起きている ・ → どこにも問題はなく、帰無仮説を棄却できない',\n",
       "  'Q3': '特にありません。',\n",
       "  'Q4': '特にありません。',\n",
       "  'Q5': '正規分布表は模試の結果などで見ることはあっても、実際に活用したことはなかったので、今回その活用例がわかって面白かった。'},\n",
       " 'L15': {'Q1': '期末テスト',\n",
       "  'Q2': 'とても不安だったけど、何とか問題を解くことができた',\n",
       "  'Q3': '所々わからない問題や不安な問題があった',\n",
       "  'Q4': '特にありません。',\n",
       "  'Q5': 'この講義で学習した知識を、今後の大学生活で活用できるよう頑張ります。'}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__getitem__(343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2e62d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample len L1 Q1: 22\n",
      "sample len L1 Q2: 34\n",
      "sample len L1 Q3: 4\n",
      "sample len L1 Q4: 3\n",
      "sample len L1 Q5: 40\n",
      "sample len L2 Q1: 490\n",
      "sample len L2 Q2: 336\n",
      "sample len L2 Q3: 3\n",
      "sample len L2 Q4: 3\n",
      "sample len L2 Q5: 29\n",
      "sample len L3 Q1: 451\n",
      "sample len L3 Q2: 158\n",
      "sample len L3 Q3: 140\n",
      "sample len L3 Q4: 3\n",
      "sample len L3 Q5: 51\n",
      "sample len L4 Q1: 953\n",
      "sample len L4 Q2: 486\n",
      "sample len L4 Q3: 3\n",
      "sample len L4 Q4: 3\n",
      "sample len L4 Q5: 63\n",
      "sample len L5 Q1: 495\n",
      "sample len L5 Q2: 432\n",
      "sample len L5 Q3: 4\n",
      "sample len L5 Q4: 4\n",
      "sample len L5 Q5: 92\n",
      "sample len L6 Q1: 734\n",
      "sample len L6 Q2: 954\n",
      "sample len L6 Q3: 4\n",
      "sample len L6 Q4: 4\n",
      "sample len L6 Q5: 60\n",
      "sample len L7 Q1: 1029\n",
      "sample len L7 Q2: 2653\n",
      "sample len L7 Q3: 4\n",
      "sample len L7 Q4: 4\n",
      "sample len L7 Q5: 110\n",
      "sample len L8 Q1: 1150\n",
      "sample len L8 Q2: 1582\n",
      "sample len L8 Q3: 4\n",
      "sample len L8 Q4: 4\n",
      "sample len L8 Q5: 64\n",
      "sample len L9 Q1: 1806\n",
      "sample len L9 Q2: 2473\n",
      "sample len L9 Q3: 5\n",
      "sample len L9 Q4: 5\n",
      "sample len L9 Q5: 88\n",
      "sample len L10 Q1: 927\n",
      "sample len L10 Q2: 1097\n",
      "sample len L10 Q3: 8\n",
      "sample len L10 Q4: 8\n",
      "sample len L10 Q5: 71\n",
      "sample len L11 Q1: 1626\n",
      "sample len L11 Q2: 2803\n",
      "sample len L11 Q3: 8\n",
      "sample len L11 Q4: 8\n",
      "sample len L11 Q5: 52\n",
      "sample len L12 Q1: 769\n",
      "sample len L12 Q2: 1525\n",
      "sample len L12 Q3: 8\n",
      "sample len L12 Q4: 8\n",
      "sample len L12 Q5: 51\n",
      "sample len L13 Q1: 369\n",
      "sample len L13 Q2: 1197\n",
      "sample len L13 Q3: 8\n",
      "sample len L13 Q4: 8\n",
      "sample len L13 Q5: 48\n",
      "sample len L14 Q1: 630\n",
      "sample len L14 Q2: 1571\n",
      "sample len L14 Q3: 8\n",
      "sample len L14 Q4: 8\n",
      "sample len L14 Q5: 59\n",
      "sample len L15 Q1: 5\n",
      "sample len L15 Q2: 25\n",
      "sample len L15 Q3: 19\n",
      "sample len L15 Q4: 8\n",
      "sample len L15 Q5: 34\n",
      "text_length: 30005\n",
      "Token length: 21273\n"
     ]
    }
   ],
   "source": [
    "sample = data.__getitem__(343)\n",
    "# get token_length\n",
    "text = \"\"\n",
    "for c in range(1,16):\n",
    "    cource = f\"L{c}\"\n",
    "    for q in range(1, 6):\n",
    "        question = f\"Q{q}\"\n",
    "        print(f\"sample len {cource} {question}: {len(sample[cource][question])}\")\n",
    "        text += sample[cource][question]\n",
    "\n",
    "print(f\"text_length: {len(text)}\")\n",
    "token_len = len(tokenizer.encode(text, add_special_tokens=False))\n",
    "print(f\"Token length: {token_len}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f40bf14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample C-2021-1_U101 exceeds max_tokens (5341 > 4096). Trimm -1245 tokens.\n",
      "Sample C-2021-1_U105 exceeds max_tokens (4659 > 4096). Trimm -563 tokens.\n",
      "Sample C-2021-1_U23 exceeds max_tokens (5353 > 4096). Trimm -1257 tokens.\n",
      "Sample C-2021-1_U27 exceeds max_tokens (6640 > 4096). Trimm -2544 tokens.\n",
      "Sample C-2021-1_U36 exceeds max_tokens (5818 > 4096). Trimm -1722 tokens.\n",
      "Sample C-2021-1_U49 exceeds max_tokens (4110 > 4096). Trimm -14 tokens.\n",
      "Sample C-2021-1_U54 exceeds max_tokens (5291 > 4096). Trimm -1195 tokens.\n",
      "Sample C-2021-1_U63 exceeds max_tokens (4348 > 4096). Trimm -252 tokens.\n",
      "Sample C-2021-1_U75 exceeds max_tokens (4871 > 4096). Trimm -775 tokens.\n",
      "Sample C-2021-1_U79 exceeds max_tokens (5127 > 4096). Trimm -1031 tokens.\n",
      "Sample C-2021-2_U101 exceeds max_tokens (10843 > 4096). Trimm -6747 tokens.\n",
      "Sample C-2021-2_U108 exceeds max_tokens (6667 > 4096). Trimm -2571 tokens.\n",
      "Sample C-2021-2_U111 exceeds max_tokens (4805 > 4096). Trimm -709 tokens.\n",
      "Sample C-2021-2_U112 exceeds max_tokens (6188 > 4096). Trimm -2092 tokens.\n",
      "Sample C-2021-2_U114 exceeds max_tokens (5850 > 4096). Trimm -1754 tokens.\n",
      "Sample C-2021-2_U117 exceeds max_tokens (5565 > 4096). Trimm -1469 tokens.\n",
      "Sample C-2021-2_U125 exceeds max_tokens (5717 > 4096). Trimm -1621 tokens.\n",
      "Sample C-2021-2_U127 exceeds max_tokens (5953 > 4096). Trimm -1857 tokens.\n",
      "Sample C-2021-2_U128 exceeds max_tokens (16016 > 4096). Trimm -11920 tokens.\n",
      "Sample C-2021-2_U13 exceeds max_tokens (5568 > 4096). Trimm -1472 tokens.\n",
      "Sample C-2021-2_U131 exceeds max_tokens (4491 > 4096). Trimm -395 tokens.\n",
      "Sample C-2021-2_U14 exceeds max_tokens (4413 > 4096). Trimm -317 tokens.\n",
      "Sample C-2021-2_U142 exceeds max_tokens (6452 > 4096). Trimm -2356 tokens.\n",
      "Sample C-2021-2_U154 exceeds max_tokens (5764 > 4096). Trimm -1668 tokens.\n",
      "Sample C-2021-2_U26 exceeds max_tokens (4286 > 4096). Trimm -190 tokens.\n",
      "Sample C-2021-2_U3 exceeds max_tokens (9195 > 4096). Trimm -5099 tokens.\n",
      "Sample C-2021-2_U36 exceeds max_tokens (5897 > 4096). Trimm -1801 tokens.\n",
      "Sample C-2021-2_U39 exceeds max_tokens (8231 > 4096). Trimm -4135 tokens.\n",
      "Sample C-2021-2_U40 exceeds max_tokens (4939 > 4096). Trimm -843 tokens.\n",
      "Sample C-2021-2_U41 exceeds max_tokens (18095 > 4096). Trimm -13999 tokens.\n",
      "Sample C-2021-2_U48 exceeds max_tokens (4622 > 4096). Trimm -526 tokens.\n",
      "Sample C-2021-2_U56 exceeds max_tokens (7774 > 4096). Trimm -3678 tokens.\n",
      "Sample C-2021-2_U60 exceeds max_tokens (6774 > 4096). Trimm -2678 tokens.\n",
      "Sample C-2021-2_U64 exceeds max_tokens (5166 > 4096). Trimm -1070 tokens.\n",
      "Sample C-2021-2_U72 exceeds max_tokens (4118 > 4096). Trimm -22 tokens.\n",
      "Sample C-2021-2_U75 exceeds max_tokens (4992 > 4096). Trimm -896 tokens.\n",
      "Sample C-2021-2_U76 exceeds max_tokens (4463 > 4096). Trimm -367 tokens.\n",
      "Sample C-2021-2_U8 exceeds max_tokens (10270 > 4096). Trimm -6174 tokens.\n",
      "Sample C-2021-2_U85 exceeds max_tokens (4356 > 4096). Trimm -260 tokens.\n",
      "Sample C-2021-2_U89 exceeds max_tokens (5997 > 4096). Trimm -1901 tokens.\n",
      "Sample C-2021-2_U9 exceeds max_tokens (11198 > 4096). Trimm -7102 tokens.\n",
      "Sample C-2022-1_U10 exceeds max_tokens (5573 > 4096). Trimm -1477 tokens.\n",
      "Sample C-2022-1_U11 exceeds max_tokens (5367 > 4096). Trimm -1271 tokens.\n",
      "Sample C-2022-1_U13 exceeds max_tokens (5963 > 4096). Trimm -1867 tokens.\n",
      "Sample C-2022-1_U14 exceeds max_tokens (4764 > 4096). Trimm -668 tokens.\n",
      "Sample C-2022-1_U15 exceeds max_tokens (5534 > 4096). Trimm -1438 tokens.\n",
      "Sample C-2022-1_U24 exceeds max_tokens (4474 > 4096). Trimm -378 tokens.\n",
      "Sample C-2022-1_U25 exceeds max_tokens (5403 > 4096). Trimm -1307 tokens.\n",
      "Sample C-2022-1_U27 exceeds max_tokens (4365 > 4096). Trimm -269 tokens.\n",
      "Sample C-2022-1_U29 exceeds max_tokens (5723 > 4096). Trimm -1627 tokens.\n",
      "Sample C-2022-1_U3 exceeds max_tokens (7574 > 4096). Trimm -3478 tokens.\n",
      "Sample C-2022-1_U30 exceeds max_tokens (6186 > 4096). Trimm -2090 tokens.\n",
      "Sample C-2022-1_U31 exceeds max_tokens (5015 > 4096). Trimm -919 tokens.\n",
      "Sample C-2022-1_U39 exceeds max_tokens (5886 > 4096). Trimm -1790 tokens.\n",
      "Sample C-2022-1_U42 exceeds max_tokens (6906 > 4096). Trimm -2810 tokens.\n",
      "Sample C-2022-1_U43 exceeds max_tokens (5255 > 4096). Trimm -1159 tokens.\n",
      "Sample C-2022-1_U46 exceeds max_tokens (4743 > 4096). Trimm -647 tokens.\n",
      "Sample C-2022-1_U50 exceeds max_tokens (6124 > 4096). Trimm -2028 tokens.\n",
      "Sample C-2022-1_U52 exceeds max_tokens (7906 > 4096). Trimm -3810 tokens.\n",
      "Sample C-2022-1_U54 exceeds max_tokens (8754 > 4096). Trimm -4658 tokens.\n",
      "Sample C-2022-1_U58 exceeds max_tokens (10408 > 4096). Trimm -6312 tokens.\n",
      "Sample C-2022-1_U59 exceeds max_tokens (4793 > 4096). Trimm -697 tokens.\n",
      "Sample C-2022-1_U6 exceeds max_tokens (5672 > 4096). Trimm -1576 tokens.\n",
      "Sample C-2022-1_U65 exceeds max_tokens (8684 > 4096). Trimm -4588 tokens.\n",
      "Sample C-2022-1_U66 exceeds max_tokens (21273 > 4096). Trimm -17177 tokens.\n",
      "Sample C-2022-1_U70 exceeds max_tokens (5735 > 4096). Trimm -1639 tokens.\n",
      "Sample C-2022-1_U73 exceeds max_tokens (6543 > 4096). Trimm -2447 tokens.\n",
      "Sample C-2022-1_U77 exceeds max_tokens (7190 > 4096). Trimm -3094 tokens.\n",
      "Sample C-2022-1_U82 exceeds max_tokens (9332 > 4096). Trimm -5236 tokens.\n",
      "Sample C-2022-1_U89 exceeds max_tokens (12436 > 4096). Trimm -8340 tokens.\n"
     ]
    }
   ],
   "source": [
    "data.trim_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "56a950f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token length: 4096\n"
     ]
    }
   ],
   "source": [
    "sample = data.__getitem__(343)\n",
    "# get token_length\n",
    "text = \"\"\n",
    "for c in range(1,16):\n",
    "    cource = f\"L{c}\"\n",
    "    for q in range(1, 6):\n",
    "        question = f\"Q{q}\"\n",
    "        text += sample[cource][question]\n",
    "\n",
    "\n",
    "token_len = len(tokenizer.encode(text, add_special_tokens=False))\n",
    "print(f\"Token length: {token_len}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a00be2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userid': 'C-2022-1_U66',\n",
       " 'labels': 1,\n",
       " 'grades': 'B',\n",
       " 'L1': {'Q1': '授業方針の説明、通信手段の歴史、情報の表し方',\n",
       "  'Q2': '古代から人は様々な手段で情報を伝達していた。腕木通信は初めて知った。',\n",
       "  'Q3': '特になし',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '大学での初めての授業ですこし緊張した。情報を科学的に考えられるように頑張りたい。'},\n",
       " 'L2': {'Q1': '情報源はただ情報の源というわけではなく一定間隔で何かしらの情報を吐き出すものである。情報を伝達するために情報源符号化を行う。情報を0と1であらわし、情報をできるだけ短くするのが目的である。情報源符号化において、一意復号可能性と瞬時復号可能性を考慮する必要がある。ビット列の分解の仕方が複数存在したり、復号に時間がかからないようにするためである。よって符号の評価基準は、一意復号可能であるか、瞬時復号可能であるか、そして平均符号語長という三要素で成り立っている。これを満たすために語頭符号を活用する。語頭符号は一意復号可能性と瞬時復号可能性を満たしている。また、「任意の一意復号可能符号Cに対し、同じ符号語調を持つ語頭符号C’が存在する」という定理から、語頭符号は平均符号語長も最小にすることができる。平均符号語長は情報源と符号語を変えることで変化する。また、情報源のエントロピーは平均符号語長の最小値以下となるので、エントロピーは平均符号語長の下限となる。しかし、エントロピーが最適符号というわけではなく、平均符号語長がエントロピーと、エントロピー+1の間にあれば最適符号となる。',\n",
       "  'Q2': '情報源はただ情報の源というわけではなく一定間隔で何かしらの情報を吐き出すものである。 情報源符号化とは、情報を0と1であらわすことで、情報をできるだけ短くするのが目的。 符号の評価基準は、一意復号可能であるか、瞬時復号可能であるか、平均符号語長の三要素で成り立っている。 一意復号可能性とは、符号の長さが同じ(固定長符号)だったり、末尾の記号を統一することで情報を一意に復号できること。 瞬時復号可能性とは、符号語を読み終えた時点で先読みせずに記号を復号できること。 語頭符号とは、どの符号語も別の符号語の接頭語になっていない符号のこと。語頭符号は一意復号可能で、瞬時復号可能である。 情報源のエントロピーが平均符号語長の下限である。 最適符号はエントロピーとは限らない。',\n",
       "  'Q3': 'NaN',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '新しい単語や定理、公式が登場したのでよく理解しておきたい。'},\n",
       " 'L3': {'Q1': '情報を伝送するとき、物理的または電気的な要因でノイズが生じ、ビットが反転することがある。反転確立が小さくても、ブロックのサイズが大きいとブロック誤り率は大きくなってしまう。対策としてはまず通信路を改善する方法があげられる。しかし物理的、経済的制限により対策が困難な場合がある。そこで二つ目の対策として符号化を工夫することがあげられる。これにより自動誤り検出、自動誤り訂正が可能となる。符号語どうしがs+1以上離れているならば高々s個の誤りを自動検出できる。また符号語どうしが2t+1以上離れているならば高々t個の誤りを自動訂正できる。繰り返し符号でも誤り訂正ができる。n=2k+1からなるn次繰り返し符号では、ビットの反転がk個以下の場合は誤り訂正ができる。nを大きくすればブロック誤り率はいくらでも小さくできるが、伝送速度は低下してしまう。通信路には容量があり、符号化効率の上限である。このとき通信路符号化定理により、符号化効率はいくらでも通信路容量に近づけ、ブロック誤り率もいくらでも小さくできる。',\n",
       "  'Q2': 'ビット列における誤りの数をハミング距離といい、三次繰り返し符号において各符号語の守備範囲をハミング距離1以下にすることで高々1ビットの誤り訂正ができる。 通信路容量は通信路に固有の値で符号化効率の上限である。 通信路符号化定理により、ブロック誤り率をいくらでも小さくできるうえに、符号化効率も通信路容量に近づけれる。',\n",
       "  'Q3': '受信した側は受け取ったビット列が正しいかどうかなんてわからないのに、どうして自動で誤りを検出できるのか不思議に感じた。社員名簿の例だと受信側に似た情報が存在しているから誤りを検出劇るかもしれないが、写真などの情報を一方的に送信する場合は受け取る側に検証の余地はないのではと思った。',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': 'ブロックを冗長化したり、繰り返し符号を使うなど、様々な方法で誤りの検出や訂正ができるとしれてよかった。'},\n",
       " 'L4': {'Q1': '暗',\n",
       "  'Q2': '計算機やネットワークの発達と普及を背景に、暗号の重要性は急速に高まっている。 最古の暗号はシーザー暗号 古代ローマ時代には既に暗号が使われていた。 平文の文字の頻度には偏りがある 暗号には対象鍵暗号と非対象鍵暗号がある。 シーザー暗号、単一換字暗号は対象鍵暗号。 鍵配送問題。。。平文を暗号化する前に、受信者に鍵を伝える必要があり、復号鍵を攻撃者に知られたくないけど、そのためにはまた別の暗号が必要になるというジレンマ。 非対称鍵暗号とはコインロッカーのように暗号化と復号に異なる鍵を使う仕組みの暗号 RSA暗号とは1997年MITのRivest、Shamir、Adlemanが開発した最初の公開鍵暗号のこと。 単一換字暗号である。 RSA暗号ではmodを使う。 公開鍵(E、N)と秘密鍵Dの計算手順 1、Nを求める 2つの素数pとqを準備し、N=p*qとする 2、EとDを求める E*D=k*LCM(p-1、q-1)+1を満たす整数E、D、kを適当に選ぶ 公開鍵(E、N)は公開する 秘密鍵Dと素数p、qは公開しない RSA暗号への攻撃は時間がかかりすぎて現実的ではない',\n",
       "  'Q3': 'NaN',\n",
       "  'Q4': 'NaN',\n",
       "  'Q5': '紀元前から暗号が使われていることに驚いた。暗号を攻撃するには莫大な時間が必要で、想像以上に暗号を解読するのは不可能だと感じた。'},\n",
       " 'L5': {'Q1': 'コンピュータサイエンスは基礎科学としての計算機科学で、すべての科学技術分野の基盤となりうるものであり、物理などと同様に重要な基礎科学となりうる分野である。 コンピュータは他分野の専門家が使うだけの単なる便利な道具ではなく、他学問と同様に背景には独自の理論体系がある。 コンピュータサイエンスは、計算とうまく付き合い、制御するために、計算が何者なのか理解するための学問である。 計算機科学における問題とは、数学的に厳密に定義された関数のことで、任意の問題事例に対し、答えが一意に定まっていることが前提となる。そのため、問題は厳密に、明確に記述すべきである。計算とは、その問題の答えを求めることである。ただ、コンピュータは問題を与えられただけでは答えを計算することができないので、関数の計算手順を人間が教える必要があり、その関数の計算方法を示す手続きをアルゴリズムという。コンピュータがわかる言葉でアルゴリズムを記述したものがプログラムである。アルゴリズムは有限時間で計算を終了するものである。 アルゴリズムを工夫することで計算ステップ数を減らすことができ、アルゴリズムの実行時間を短くできる。',\n",
       "  'Q2': 'コンピューターサイエンスとは基礎科学としての計算機科学で、すべての科学技術分野の基盤となりうる学問である。 コンピュータサイエンスでは、あらゆる問題、関数の計算問題として数式化(主に文字列で表現)し、問題を解くための効率のいい計算手順を設計する。計算手順の正当性や効率化の限界は証明できる。 計算とうまく付き合い、制御するには、計算が何者なのか理解する必要がある。 計算機科学における問題とは数学的に厳密に定義された関数で、任意の問題事例に対し、答えが一意に定まっていることが前提 計算とは問題事例xが与えられたとき、その答えf(x)を求めること アルゴリズムとは問題を計算するアルゴリズムとは任意の問題事例から答えを求める手続きであり、有限時間で計算を終了するもの コンピュータがわかる言葉でアルゴリズムを記述したものがプログラム 計算ステップ数とはアルゴリズムが終了するまでに行われる基本演算の回数で、計算ステップ数が少ないほどアルゴリズムの実行時間は短くなる',\n",
       "  'Q3': '特になし',\n",
       "  'Q4': '特になし',\n",
       "  'Q5': '普段は何気なくコンピュータを利用しているが、コンピュータが問題を解くためには、実際は我々が手取り足取り計算手順を教えてあげる必要があり、アルゴリズムを構築するのは大変そうだとおもった。'},\n",
       " 'L6': {'Q1': 'ソ',\n",
       "  'Q2': 'ソ',\n",
       "  'Q3': '特になし',\n",
       "  'Q4': '特になし',\n",
       "  'Q5': 'ソートアルゴリズムについてはYouTubeなどで見たことがあったが、今回の授業でさらに理解を深めることができてよかった。'},\n",
       " 'L7': {'Q1': 'ヒ',\n",
       "  'Q2': 'ヒ',\n",
       "  'Q3': '特になし',\n",
       "  'Q4': '特になし',\n",
       "  'Q5': 'マージソートは比較的シンプルで理解しやすかった。接尾辞配列を用いてテキストを2分探索するアイディアが発表されたのは1991年とあって、比較的最近でも新たな発見がある情報科学分野はまだまだ発展の余地がありそうだとおもった。'},\n",
       " 'L8': {'Q1': 'データ',\n",
       "  'Q2': 'データ',\n",
       "  'Q3': '特になし',\n",
       "  'Q4': '特になし',\n",
       "  'Q5': 'データ分析は日常生活でも活用されており、その必要は年々高まっていると知った。具体例をあげることで、データ分析をより身近に感じた。'},\n",
       " 'L9': {'Q1': 'クラ',\n",
       "  'Q2': 'クラ',\n",
       "  'Q3': 'とくになし',\n",
       "  'Q4': 'とくになし',\n",
       "  'Q5': '自分たちが普段使っている人工知能は万能のように思っていたが、実際は特定の分野にのみ特化しているだけで、汎用AIは完成されていないと知り、AIもまだまだ発展の余地があると思った。'},\n",
       " 'L10': {'Q1': '非',\n",
       "  'Q2': '文章',\n",
       "  'Q3': '特にありません。',\n",
       "  'Q4': '特にありません。',\n",
       "  'Q5': '文章や音声、音楽を解析するだけではなく、それを利用して文章や音楽の創作も可能になったことで、コンピュータもいよいよ人間味が増してきたと感じた。'},\n",
       " 'L11': {'Q1': '統',\n",
       "  'Q2': '全',\n",
       "  'Q3': '特にありません。',\n",
       "  'Q4': '特にありません。',\n",
       "  'Q5': '今後オープンデータが広く普及し、活用されることで、日本経済や私たちの暮らしはより良いものになると感じた。'},\n",
       " 'L12': {'Q1': 'ベクトルとは、複数の数値をカタマリにしたもので、()の中にカンマで区切って書く。その順番に意味がある。 ベクトル表現されたデータの分析には、線形代数もよく使われ、行列もデータ表現に使われる。 ベクトルはデータの組み合わせなので、ベクトルでデータ分析すると、1つの組み合わせでは分からないことも、多数のデータを用意することで、データ間の関係が見えてくる データ解析における「距離」はもっと自由である。ここでの距離はデータ間の差異(似てない具合)であり、距離が小さい2データは「似ている」といえる。単位がある場合もない場合もある。 数学的には、非退化性・対称性・三角不等式の3条件を満たすd(x、y)をx、 y の「距離」と呼び、条件を満たすなら、何でも「距離」といえる。 類似度は距離の反対の概念で、大きければ大きいほど似ている(距離は小さいほど似ている)。類似度は距離ほど厳密に定義されておらず、類似度は正も負の値もとる(距離は0以上)。また、三角不等式のような条件もない モノをベクトルで表せば、様々な種類の距離や類似度が使え、距離や類似度に基づいた分析例には、相同性検索・クラスタリング、系統分類・判定・異常検知などがある。 距離は「データ解析の基本」であり、距離は1種類ではない。距離が変われば、データ解析結果は「まるっきり」変わるので、データや解析',\n",
       "  'Q2': 'ベ',\n",
       "  'Q3': '特にありません。',\n",
       "  'Q4': '特にありません。',\n",
       "  'Q5': 'データ分析ではベクトルや行列といった数学的要素が多く活用されていると分かって、数学の大切さを実感した。'},\n",
       " 'L13': {'Q1': '可視化とはデータを直感的に理解できる図にすることで、膨大なデータを把握しやすくするために必要である。可視化をするときは目的と条件によって適切なものを選択する必要がある。不適切な可視化は誤解を生んだり、不誠実な印象を招くからだ。 可視化手法は様々あり、 ・データの分布を調べたい→ヒストグラム ・データ分布の比較したい→箱ひげ図 ・数値データの比較したい→棒グラフ ・データ全体に対しての各データの割合を把握したい→パイチャート ・2種類のデータの傾向を同時に把握したい→散布図 ・2種類のデータの分布を調べたい→ヒートマップ ・データ点の間の変化の傾向を把握したい→折れ線グラフ ・データ間の関係性・接続を把握したい→無向/有向グラフ といったように用途に合わせて使い分ける。 データを可視化することで、次に行う分析の方針決定につながる。',\n",
       "  'Q2': '可',\n",
       "  'Q3': '特にありません。',\n",
       "  'Q4': '特にありません。',\n",
       "  'Q5': '普段何気なくグラフや図を使っているが、表現方法や用途についてよく考えて使う必要があるとかんじた。'},\n",
       " 'L14': {'Q1': '画像は理系・文系問わず、有用なデータである。 画像解析には画像処理、物体検出・物体認識、領域分割、3次元再構成がある。 基本的な画像解析にはフィルタ処理と2値化がある。 フィルタ処理とは出力画像の1画素の値を求めるために入力画像のある領域内の画素値を用いる濃淡変換の処理で、滑らかな出力画像の1画素の値を入力画像で同じ位置の画素の値とその周辺の画素の値から計算する。フィルタ処理をすることで、平滑化やエッジ抽出ができる。平均化フィルタや重み付き平均化フィルタといった線形フィルタを用いて、画像に含まれる不要な濃淡変動を軽減することができるが、画像全体がぼやけるという欠点もある。そこで非線形フィルタであるエッジ保存平均化フィルタを用いることで、画像がぼやけることなく平滑化できる。エッジ抽出は、微分フィルタやソーベルフィルタ、ラプラシアンフィルタを用いて画像中の明るさが急激に変化する部分(エッジ)を取り出す処理である。 2値化とは、黒(文字)と白(紙)の2値の画像に変換する処理のことで、2値化による文字領域候補の抽出により文書のデータ化ができる。 相関とは2つの量の関係性を説明する方法で、分散とどうようにデータの広がり具合の指標となる。相関は相関係数により数値で表現する(定量化する)ことができ、相関係数pが分かれば、分布の形を少し想像することができる 統計的検定は統計的に差を評価する枠組みで、帰無仮説や対立仮説を用いてデータが得られる確率を評価する。',\n",
       "  'Q2': '画像',\n",
       "  'Q3': '特にありません。',\n",
       "  'Q4': '特にありません。',\n",
       "  'Q5': '正規分布表は模試の結果などで見ることはあっても、実際に活用したことはなかったので、今回その活用例がわかって面白かった。'},\n",
       " 'L15': {'Q1': '期末テスト',\n",
       "  'Q2': 'とても不安だったけど、何とか問題を解くことができた',\n",
       "  'Q3': '所々わからない問題や不安な問題があった',\n",
       "  'Q4': '特にありません。',\n",
       "  'Q5': 'この講義で学習した知識を、今後の大学生活で活用できるよう頑張ります。'}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__getitem__(343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b57773c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f3c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_data = GradePredictionDataset(\n",
    "    dataset_path=DATA_PATH,\n",
    "    question_filter=[1],\n",
    "    concatenate=True,\n",
    "    mode = \"train\",\n",
    ")\n",
    "Q1_data.__len__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4758cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = str(Q1_data.__getitem__(idx)[\"input_text\"])\n",
    "pp.pprint(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testcase でのデータ内容を確認\n",
    "Q1_data = GradePredictionDataset(\n",
    "    dataset_path=DATA_PATH,\n",
    "    question_filter=[1],\n",
    "    concatenate=True,\n",
    "    mode=\"train\",\n",
    "    testcase=True,\n",
    ")\n",
    "Q1_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0542234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = str(Q1_data.__getitem__(idx)[\"input_text\"])\n",
    "pp.pprint(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = str(Q1_data.__getitem__(idx)[\"grades\"])\n",
    "pp.pprint(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c5716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util import load_model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import model summary\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c8840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc3618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testcase = true の場合 collate_fn の動作を確認\n",
    "input = collate_fn(Q1_data, tokenizer=tokenizer, question_filter=[1], testcase=True)\n",
    "pp.pprint(input[\"input_text\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea65c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input[\"input_ids\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bb440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode\n",
    "decoded_text = tokenizer.decode(input[\"input_ids\"][0], skip_special_tokens=True)\n",
    "pp.pprint(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util import load_model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import model summary\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c36e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.debug(\"This is a debug message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c30d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = collate_fn(Q1_data, tokenizer=tokenizer, question_filter=[1], logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input[\"input_ids\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputの内容をdecodeして確認\n",
    "decoded_input = tokenizer.decode(input[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3457164",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 生成\n",
    "model.eval()\n",
    "ids = input[\"input_ids\"][0].unsqueeze(0).to(model.device)\n",
    "attention_mask = input[\"attention_mask\"][0].unsqueeze(0).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=ids,\n",
    "        attention_mask= attention_mask,\n",
    "        max_new_tokens=5,\n",
    "        do_sample=False,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42af97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ef193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
