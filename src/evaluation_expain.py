# =====================================================================
# evaluation_expain.py (create 2025/06/20)
# description:
#   - æ–‡ç« ç”Ÿæˆã‚¿ã‚¹ã‚¯ã®è©•ä¾¡ã‚’è¡Œã†
# =====================================================================


# libraries
import json
from pathlib import Path
import logging
from typing import List, Dict

# evaluation libraries
import sacrebleu
from rouge_score import rouge_scorer
from nltk.translate.meteor_score import meteor_score  # nltk.download('wordnet') ãŒåˆå›ã®ã¿å¿…è¦
from moverscore_v2 import word_mover_score, get_idf_dict


# own libraries
from src.util import set_logger

DATA_DIR = Path("results/GradeExplanation/connect")

# =============================================================
# evaluation function
# =============================================================
def evaluate_record(record: Dict[str, List[str]]) -> Dict[str, float]:

    preds: List[str] = record["output_sentence"]
    refs:  List[str] = record["label_sentence"]

    # ---------- BLEU ----------
    bleu_score = sacrebleu.corpus_bleu(preds, [refs]).score

    # ---------- ROUGE ----------
    rouge = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
    rouge1, rouge2, rougel = 0.0, 0.0, 0.0
    for p, r in zip(preds, refs):
        scores = rouge.score(r, p)
        rouge1  += scores["rouge1"].fmeasure
        rouge2  += scores["rouge2"].fmeasure
        rougel += scores["rougeL"].fmeasure
    n = len(preds)
    rouge1, rouge2, rougel = [s / n * 100 for s in (rouge1, rouge2, rougel)]

    # ---------- METEOR ----------
    meteor = sum(meteor_score([r], p) for p, r in zip(preds, refs)) / n * 100

    # ---------- MoverScore ----------
    idf_pred = get_idf_dict(preds)
    idf_ref  = get_idf_dict(refs)
    ms_list  = word_mover_score(
        refs, preds,
        idf_ref, idf_pred,
        stop_words=[],
        n_gram=1,
        remove_subwords=True,
        batch_size=16,
    )
    moverscore = sum(ms_list) / n * 100

    return {
        "BLEU":   round(bleu_score, 2),
        "ROUGE-1": round(rouge1, 2),
        "ROUGE-2": round(rouge2, 2),
        "ROUGE-L": round(rougel, 2),
        "METEOR": round(meteor, 2),
        "MoverScore": round(moverscore, 2),
    }


# =============================================================
# main function
# =============================================================

def main() -> None:
    print("setup logger")
    logger = set_logger(logging.INFO)

    # =============================================================
    # load data
    # =============================================================
    before_file = "pred_result_before_training.json",
    after_file  = "pred_result_after_training.json",
    
    with open(DATA_DIR / before_file, encoding="utf-8") as f:
        before_data = json.load(f)
    with open(DATA_DIR / after_file, encoding="utf-8") as f:
        after_data = json.load(f)

    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ç¢ºèªã¨å¿…è¦ã«å¿œã˜ã¦ãƒ©ãƒƒãƒ—
    if isinstance(before_data, dict):
        before_data = [before_data]
    if isinstance(after_data, dict):
        after_data = [after_data]

    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼š
    # {
    #   "input_sentence": [
    #     "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1",
    #     ...
    #     "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆn",
    # ]
    #   "output_sentence": [
    #     "å‡ºåŠ›1",
    #     ...
    #     "å‡ºåŠ›n",
    #   ]
    #   "label_sentence": [
    #     "æ­£è§£1",
    #     ...
    #     "æ­£è§£n",
    #   ]
    # }

    
    # =============================================================
    # Check data
    # =============================================================

    logger.info("Check data : before_training")
    for i, data in enumerate(before_data):
        logger.info(f"Data {i+1}:")
        logger.info(f"Input: {data['input_sentence']}")
        logger.info(f"Output: {data['output_sentence']}")
        logger.info(f"Label: {data['label_sentence']}")

        if i >= 10:  # æœ€åˆã®10ä»¶ã ã‘è¡¨ç¤º
            break
    logger.info("Check data : after_training")
    for i, data in enumerate(after_data):
        logger.info(f"Data {i+1}:")
        logger.info(f"Input: {data['input_sentence']}")
        logger.info(f"Output: {data['output_sentence']}")
        logger.info(f"Label: {data['label_sentence']}")

        if i >= 10:  # æœ€åˆã®10ä»¶ã ã‘è¡¨ç¤º
            break
    # =============================================================
    # evaluation
    # metrics 1: BLEU
    # metrics 2: ROUGE
    # metrics 3: METEOR
    # metrics 4: moverscore
    # =============================================================

    def eval_and_log(dataset, tag):
        logger.info(f"âœ…  Start Evaluation ({tag}) training data ...")
        agg = {k: 0.0 for k in
               ["BLEU", "ROUGE-1", "ROUGE-2", "ROUGE-L", "METEOR", "MoverScore"]}
        for i, rec in enumerate(dataset, 1):
            scores = evaluate_record(rec)
            logger.info(f"[{tag} #{i}] {scores}")
            for k in agg:
                agg[k] += scores[k]

        m = len(dataset)
        mean_scores = {k: round(v / m, 2) for k, v in agg.items()}
        logger.info(f"ğŸ“Š  Mean metrics ({tag}) â†’ {mean_scores}")

    eval_and_log(before_data, "before")
    eval_and_log(after_data,  "after")


if __name__ == "__main__":
    main()