# =====================================================================
# train.py
# date: 2025/05/06
# description:
#   - LLMã®LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†
#   - OOMå¯¾ç­–ã¨ã—ã¦ ZeRO Data Parallelism ã‚’æ¡ç”¨
# å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ï¼›
#   - elyza/Llama-3-ELYZA-JP-8B
# =====================================================================

# -------- import libraries --------
import os
import argparse
from dotenv import load_dotenv
from transformers import Trainer, TrainingArguments, set_seed
import torch
from torchinfo import summary
from functools import partial
from peft import LoraConfig, TaskType, get_peft_model
import json

from util import load_model
from gradepred_data import GradePredictionDataset, collate_fn

# ä½¿ç”¨VRAMæ•°ã®æ¨å®š

# from accelerate import estimate_memory

# -------- environment setting --------
load_dotenv()

os.environ["TORCH_DISTRIBUTED_DEBUG"] = "DETAIL"
os.environ["TORCH_ENABLE_DISTRIBUTED"] = "1"
os.environ["TORCH_DTENSOR_SKIP_CHECK"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

BYTES_PER_PARAM = {
    torch.float32: 4,
    torch.float16: 2,
    torch.bfloat16: 2,
    torch.int8: 1,
}


def estimate_vram_cost(
    model: torch.nn.Module,
    batch_size: int,
    seq_len: int,
    dtype=torch.float16,
    optim_factor: float = 4,  # AdamW (fp32) ãªã‚‰ 4 (= 1é‡ã¿ + 1å‹¾é… + 2çŠ¶æ…‹)
    act_factor: float = 1.3,  # æ´»æ€§ã®å†ç¾ç”¨ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ (çµŒé¨“å‰‡)
) -> dict:
    """
    LLM 1 GPU ã‚ãŸã‚Šã® VRAM ä½¿ç”¨é‡ (æ¨å®š) ã‚’è¿”ã™

    Returns
    -------
    dict : {
        "params_MB": â€¦,
        "grads_MB" : â€¦,
        "optimizer_MB": â€¦,
        "activations_MB": â€¦,
        "total_MB": â€¦,
    }
    """
    bytes_per_param = BYTES_PER_PARAM[dtype]
    # â‘  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
    param_count = sum(p.numel() for p in model.parameters())
    params_MB = param_count * bytes_per_param / (1024**2)

    # â‘¡ å‹¾é…ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨åŒã‚µã‚¤ã‚ºï¼åŒ dtypeï¼‰
    grads_MB = params_MB

    # â‘¢ Optimizer çŠ¶æ…‹ (AdamW = fp32 Ã— 2 å€)
    optimizer_MB = params_MB * (optim_factor - 2)  # å‹¾é…+é‡ã¿ã¯é™¤å¤–æ¸ˆ

    # â‘£ ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ (ãŠãŠã‚ˆã batch*seq*hidden*4bytesÃ—layersÃ—ä¿‚æ•°)
    hidden = model.config.hidden_size
    layers = model.config.num_hidden_layers
    acts_numel = batch_size * seq_len * hidden * layers * act_factor
    activations_MB = acts_numel * bytes_per_param / (1024**2)

    total_MB = params_MB + grads_MB + optimizer_MB + activations_MB
    return {
        "params_MB": params_MB,
        "grads_MB": grads_MB,
        "optimizer_MB": optimizer_MB,
        "activations_MB": activations_MB,
        "total_MB": total_MB,
    }


def evaluate_generate(
    model, tokenizer, dataset, collate_fn, batch_size: int = 4, device="cuda"
):
    from torch.utils.data import DataLoader

    model.eval()
    loader = DataLoader(
        dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn
    )

    correct = 0
    total = 0

    with torch.no_grad():
        for batch in loader:
            ids = batch["input_ids"].to(device)
            mask = batch["attention_mask"].to(device)
            # gold ãƒ©ãƒ™ãƒ«ã¯ dataset å´ã‹ã‚‰ç›´æ¥å–å¾—
            golds = [s["grades"] for s in dataset[total : total + len(ids)]]

            gen = model.generate(
                input_ids=ids,
                attention_mask=mask,
                max_new_tokens=2,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

            preds = tokenizer.batch_decode(gen, skip_special_tokens=True)
            preds = [p.strip()[:1] for p in preds]  # å…ˆé ­ 1 æ–‡å­—ã‚’æŠ½å‡º
            correct += sum(p == g for p, g in zip(preds, golds))
            total += len(golds)

    acc = correct / total
    print(f"[eval] accuracy = {acc:.4f} ({correct}/{total})")
    return acc


# -------- main function --------
def main():
    import torch

    # torchã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèª
    print(f"[info] torch version: {torch.__version__}")
    print(f"[info] torch cuda version: {torch.version.cuda}")

    torch.compile = None
    if hasattr(torch, "compile"):
        torch.compile = lambda *args, **kwargs: args[0]

    parser = argparse.ArgumentParser(
        description="Fine-tune LLama with LoRA on Reflection dataset"
    )

    # ãƒ¢ãƒ‡ãƒ«ã‚„å‡ºåŠ›ã«é–¢ã™ã‚‹è¨­å®š
    parser.add_argument(
        "--base_model",
        type=str,
        default="elyza/Llama-3-ELYZA-JP-8B",
        help="Base model ID",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./outputs/lora-elyza-reflection",
        help="Directory to save LoRA-tuned checkpoints",
    )

    # LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument("--lora_r", type=int, default=8, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=16, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.1, help="LoRA dropout")

    # ãƒ‡ãƒ¼ã‚¿ãƒ»å­¦ç¿’è¨­å®š
    parser.add_argument("--max_words", type=int, default=4096, help="data max_words")
    parser.add_argument(
        "--epochs", type=int, default=3, help="Number of training epochs"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1,
        help="Global batch size (across all devices)",
    )
    parser.add_argument(
        "--micro_batch_size", type=int, default=1, help="Batch size per device"
    )

    # ãƒ­ã‚°å‡ºåŠ›è¨­å®š
    parser.add_argument(
        "--report_to",
        type=str,
        default="none",
        choices=["wandb", "tensorboard", "none"],
        help="Reporting backend for logging",
    )
    parser.add_argument(
        "--run_name",
        type=str,
        default="lora-elyza-reflection_test",
        help="Run name for experiment tracking",
    )

    args = parser.parse_args()

    # Set random seed for reproducibility
    set_seed(42)

    # DDP (Distributed Data Parallel) ã®è¨­å®š
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    # gradient_accumulation_steps = args.batch_size // args.micro_batch_size
    # if ddp := world_size != 1:
    #     gradient_accumulation_steps = gradient_accumulation_steps // world_size

    if args.batch_size % args.micro_batch_size != 0:
        raise ValueError(
            "global_batch_size must be divisible by per_device_batch_size Ã— WORLD_SIZE"
        )
    ddp = world_size != 1

    gradient_accumulation_steps = max(
        1, args.batch_size // args.micro_batch_size // world_size
    )

    if ddp:
        print(f"[info] DDP is enabled (ddp = {ddp}, world_size = {world_size})")
    else:
        print(f"[info] DDP is disabled (ddp = {ddp}, world_size = {world_size})")

    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µã®ãƒ­ãƒ¼ãƒ‰
    model, tokenizer = load_model(args.base_model, if_ZeRO=True)
    summary(model)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€
    dataset_path = "./data/"

    train_dataset = GradePredictionDataset(
        dataset_path=dataset_path,
        question_filter=[1],
        concatenate=True,
        mode="train",
    )
    eval_dataset = GradePredictionDataset(
        dataset_path=dataset_path,
        question_filter=[1],
        concatenate=True,
        mode="valid",
    )

    print("[info] len(train_dataset):", len(train_dataset))
    print("[info] len(eval_dataset):", len(eval_dataset))

    custom_collate_fn = partial(
        collate_fn, tokenizer=tokenizer, max_tokens=args.max_words
    )
    print(
        f"[info] custom_collate_fn initialized with processor: {type(tokenizer).__name__}"
    )

    # LoRAè¨­å®š
    peft_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=[
            # "q_proj",
            # "k_proj",
            # "v_proj",
            # "o_proj",  # Self-Attentionç³»
            "gate_proj",
            "up_proj",
            "down_proj",  # MLPï¼ˆFFNï¼‰ç³»
        ],
    )

    # LoRAé©ç”¨
    model.enable_input_require_grads()  #! è¿½åŠ ::å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã«å‹¾é…ã‚’æµã›ã‚‹çŠ¶æ…‹ã‚’å¼·åˆ¶ã™ã‚‹å®‰å…¨ã‚¹ã‚¤ãƒƒãƒ
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    print("âœ… LoRA has been successfully applied to the model.")
    summary(model)

    # full_finetune_modules ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†åº¦ trainable ã«ã™ã‚‹
    full_finetune_modules = ["embed_tokens", "lm_head"]

    print("ğŸ”§ Applying LoRA and enabling full finetune modules...")

    # LoRA é©ç”¨æ¸ˆã¿ï¼ˆå‰æ®µï¼‰
    for name, param in model.named_parameters():
        if any(module_name in name for module_name in full_finetune_modules):
            param.requires_grad = False  #! True -> å¤‰æ›´
            # param.data = param.data.to(torch.float32)
            param.data = param.data.to(torch.float16)

    print("âœ… LoRA has been applied.")
    print(
        f"âœ… The following modules are fully finetuned: {', '.join(full_finetune_modules)}"
    )

    model.print_trainable_parameters()
    summary(model)
    # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼‹rank 0 é™å®š summary
    model.gradient_checkpointing_enable()  #! è¿½åŠ ::model.gradient_checkpointing_enable() ã‚’ LoRA é©ç”¨å¾Œã«å‘¼ã¶ã¨ 20â€“30 % è¿½åŠ ç¯€ç´„
    # if local_rank == 0:
    #     model.print_trainable_parameters()
    #     summary(model, depth=2)

    print("Trainable parameters:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(name, param.shape, param.dtype)

    print("=" * 100)
    print("=" * 100)

    # TrainingArguments
    training_args = TrainingArguments(
        # ---
        gradient_checkpointing=True,
        # --- 
        output_dir=args.output_dir,
        per_device_train_batch_size=args.micro_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,  # args.grad_accum,
        num_train_epochs=args.epochs,
        warmup_ratio=0.03,
        logging_dir="./logs",
        logging_steps=50,
        lr_scheduler_type="cosine",
        optim="adamw_torch", # "adamw_bnb_8bit",            # Adam çŠ¶æ…‹ã‚’ 75% åœ§ç¸®  #! ZeROã ã¨ç„¡åŠ¹åŒ–ã•ã‚Œã‚‹ã£ã½ã„
        save_strategy="epoch",
        eval_strategy="epoch",
        fp16=True,
        fp16_full_eval=True,                # eval ã‚‚åŠç²¾åº¦(2023.05.18)
        #! 
        per_device_eval_batch_size=1,
        eval_accumulation_steps=1,
        #!  
        # dataloader_num_workers=4,
        remove_unused_columns=False,
        # report_to=None if args.report_to == "none" else args.report_to,
        run_name=args.run_name,
        save_total_limit=args.epochs,  # 2
        ddp_find_unused_parameters=False, #True,  #! ã‚‚ã—ã‹ã—ãŸã‚‰æ¶ˆã—ãŸæ–¹ãŒã„ã„ã‹ã‚‚
        # ddp_find_unused_parameters=False if ddp else None,
        load_best_model_at_end=False,
        # deepspeed="ds_config_zero3.json",   #! è¿½åŠ (ZeRO) -> ymalã§CMDã‹ã‚‰æŒ‡å®šã—ãŸã®ã§å†å‰Šé™¤(2025.05.18)
    )

    print("[info] Initialized TrainingArguments:")
    print(json.dumps(training_args.to_dict(), indent=2))

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=custom_collate_fn,
    )

    print("[info] Trainer instance has been created.")
    print(
        f"[info] Trainer is set with model: {type(model).__name__}, train dataset size: {len(train_dataset)}, eval dataset size: {len(eval_dataset)}"
    )

    # ãƒ¢ãƒ‡ãƒ«èª­è¾¼ç›´å¾Œ
    stats = estimate_vram_cost(
        model,
        batch_size=args.micro_batch_size,
        seq_len=args.max_words,
        dtype=torch.float16,
    )
    print("[memory-estimate]")
    for k, v in stats.items():
        print(f"{k:15}: {v:8.1f} MB")

    # ä¾‹: A6000-ada (48 GB) ãªã‚‰ä½™è£• 48 GB (= 49,152 MB) ãŒä¸Šé™
    if stats["total_MB"] > 48 * 1024:
        print(
            "âš ï¸ Estimated VRAM exceeded. Please consider switching to Model Parallel/FSDP etc."
        )

    print("ğŸ”„ Starting training...")
    trainer.train()
    print("âœ… Model training has been completed successfully!")

    ## rank 0ã®ã¿ãŒä¿å­˜å‡¦ç†
    # if is_main_process():
    #    print("âœ… Training finished. Starting model saving...")
    #    torch.distributed.barrier()  # å…¨rankã®åŒæœŸã‚’æ˜ç¤ºçš„ã«ã¨ã‚‹
    #    merge_lora_and_save(model, processor, args.merged_output_dir)
    #    print("âœ… Model saved successfully.")

    print("ğŸ‰ All steps completed successfully!")

    # evaluate
    # --- train() ã®ç›´å¾Œã«å‘¼ã¶ ---
    print("ğŸ”  Running evaluation on validation split â€¦")
    evaluate_generate(
        model=model,
        tokenizer=tokenizer,
        dataset=eval_dataset,
        collate_fn=custom_collate_fn,
        device=model.device,
        batch_size=args.micro_batch_size,
    )
    print("âœ… Evaluation completed successfully!")


if __name__ == "__main__":
    main()
