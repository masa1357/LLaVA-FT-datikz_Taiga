# =====================================================================
# train.py
# date: 2025/06/04
# description:
#   - LLMã®LoRAãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†
#   - OOMå¯¾ç­–ã¨ã—ã¦ ZeRO Data Parallelism ã‚’æ¡ç”¨
# å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ï¼›
#   - elyza/Llama-3-ELYZA-JP-8B
# =====================================================================

# ================ æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ================
import os
import sys
import re
import argparse
from logging import (
    getLogger,
    StreamHandler,
    Formatter,
    DEBUG,
    INFO,
    WARNING,
    ERROR,
    CRITICAL,
    Logger,
)
from functools import partial
from typing import Dict, List

# ================ ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ ================
import numpy as np
import torch
from torch.utils.data import DataLoader
from torchinfo import summary

from dotenv import load_dotenv  # .env èª­ã¿è¾¼ã¿
import sacrebleu  # BLEU
import evaluate  # ROUGE / BERTScore / MoverScore ãƒ©ãƒƒãƒ‘
from moverscore_v2 import word_mover_score, get_idf_dict

from transformers import Trainer, TrainingArguments, set_seed, GenerationConfig
from peft import LoraConfig, get_peft_model, TaskType
from transformers.trainer_utils import EvalPrediction, PredictionOutput
from sklearn.metrics import confusion_matrix

# ================ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰ ================
from util import load_model, set_seed, set_logger
from gradepred_data import GradePredictionDataset, GradePredictionCollator
from accelerate import Accelerator

# -------- environment setting --------
load_dotenv()

os.environ["TORCH_DISTRIBUTED_DEBUG"] = "DETAIL"
os.environ["TORCH_ENABLE_DISTRIBUTED"] = "1"
os.environ["TORCH_DTENSOR_SKIP_CHECK"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["ACCELERATE_DISABLE_FREE_MEMORY"] = (
    "1"  #  AssertionError: DeepSpeed backend not set, please initialize it using init_process_group()ã¸ã®å¯¾ç­–
)

BYTES_PER_PARAM = {
    torch.float32: 4,
    torch.float16: 2,
    torch.bfloat16: 2,
    torch.int8: 1,
}

if not hasattr(np, "float"):
    np.float = float


def evaluate(
    pred_result,
    eval_dataset,
    tokenizer,
    show_samples: int = 5,
    logger: Logger = getLogger("EvaluationLogger"),
) -> Dict[str, float]:

    # logger.info(f"pred_result elements\t:{pred_result._asdict().keys()}")
    logger.info("start evaluate!")
    if isinstance(pred_result, dict):
        logger.info(f"pred_result keys\t:{list(pred_result.keys())}")
        input_text = pred_result["input_sentence"]
        pred_text = pred_result["output_sentence"]
        label_text = pred_result["label_sentence"]
        # input_text = pred_result["input_sentence"]
    else:  # NamedTuple
        logger.info(f"pred_result elements\t:{pred_result._asdict().keys()}")
        input_text = pred_result.input_sentence
        pred_text = pred_result.output_sentence
        label_text = pred_result.label_sentence
        # input_text = pred_result.input_sentence

    logger.info(f"#pred={len(pred_text)}, #labels={len(label_text)}")

    # pred_logits = pred_result.predictions  # (bs, seq, vocab) ã‹ np.object_
    # logger.info(f"pred_logits shape\t: {pred_logits.shape}")
    # logger.debug(f"pred_logits \t: \n{pred_logits}")
    # if pred_logits.ndim == 3:  # logits ãƒ‘ã‚¿ãƒ¼ãƒ³
    #     pred_ids = pred_logits.argmax(-1)  # (bs, seq)
    # else:  # æ—¢ã« ID ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆ
    #     logger.info("pred_logits is already in ID format, skipping argmax operation.")
    #     pred_ids = pred_logits

    # # -100 â†’ pad ã«ç½®æ›ï¼ˆlabels ã‚‚åŒæ§˜ã«ï¼‰
    # pred_ids = pred_ids.tolist()  # list[list[int]]
    # label_ids = pred_result.label_ids
    # logger.info(f"label_ids shape\t: {label_ids.shape}")
    # logger.debug(f"label_ids \t: \n{label_ids}")
    # label_ids[label_ids == -100] = tokenizer.pad_token_id
    # label_ids = label_ids.tolist()

    # pred_text = pred_result.output_sentence
    # grades = [row["grades"] for row in eval_dataset]
    # labels_text = [f" ã“ã®å­¦ç”Ÿã®æˆç¸¾ã¯ã€{g}ã§ã™ã€‚" for g in grades]

    # pred_text = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    # labels_text = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    #! ä¸€æ™‚çš„ãªå‡¦ç½®
    labels_text = label_text

    if show_samples > 0:
        logger.info("âœ…ï¸ Visualize sample answers")
        for i in range(show_samples):
            msg = "\n".join(
                [
                    "========================",
                    f"Sample {i}:",
                    f"Raw Questionnaires\t: \n{eval_dataset[i]['input_text']}",
                    f"Predict\t: {pred_text[i]}",
                    f"Target\t: {labels_text[i]}",
                    "========================",
                ]
            )
            logger.info(msg)
            msg = ""

    # Metrics #1    : BLEU
    bleu = sacrebleu.corpus_bleu(pred_text, [labels_text]).score

    # Metrics #2    : MoverScore
    # idf_p = get_idf_dict(pred_text)
    # idf_r = get_idf_dict(labels_text)
    # ms = word_mover_score(
    #     labels_text,
    #     pred_text,
    #     idf_r,
    #     idf_p,
    #     stop_words=[],
    #     n_gram=1,
    #     remove_subwords=True,
    #     batch_size=16,
    #     device = torch.device("cpu")
    # )
    # moverscore = float(np.mean(ms))
    moverscore = 1.0  # ä»®ç½®ãå€¤ï¼ˆå®Ÿéš›ã®è¨ˆç®—ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰

    # Metrics #3    : Accuracy
    # -> targetå†…ã«ã¯[A,B,C,D,F]ã®ã„ãšã‚Œã‹ã‚’å«ã‚€æ–‡å­—åˆ—ãŒå…¥ã£ã¦ã„ã‚‹
    # predictionã¨targetã‹ã‚‰æœ€åˆã«å‡ºç¾ã™ã‚‹["A", "B", "C", "D", "F"]ã‚’æŠ½å‡ºã—ã¦æ¯”è¼ƒã™ã‚‹
    def extract_grade(text: str) -> str:
        # 1. æ–‡å­—åˆ—ã‹ã‚‰[/INST]ä»¥å‰ã®éƒ¨åˆ†ã‚’å‰Šé™¤
        text = text.split("[/INST]")[-1]

        # 2. æ–‡å­—åˆ—ã‹ã‚‰æˆç¸¾ã‚’æŠ½å‡º
        # ã€Œæˆç¸¾ã¯ã€Xã§ã™ã€ã® X ã‚’æ­£è¦è¡¨ç¾ã§æŠœã
        m = re.search(r"æˆç¸¾ã¯ã€([A-D]|F)ã§ã™", text)
        if m:
            return m.group(1)
        else:
            for grade in ["A", "B", "C", "D", "F"]:
                if grade in text:
                    return grade
        return "F"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ F

    pred_grades = [extract_grade(text) for text in pred_text]
    label_grades = [extract_grade(text) for text in labels_text]
    accuracy = np.mean(np.array(pred_grades) == np.array(label_grades)) * 100
    cf = confusion_matrix(
        label_grades, pred_grades, labels=["A", "B", "C", "D", "F"]
    )
    

    # äºˆæ¸¬ã«æˆåŠŸã—ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ã‚’ã„ãã¤ã‹è¡¨ç¤º
    if show_samples > 0:
        logger.info("â­• Visualize successful predictions")
        for i in range(len(pred_text)):
            if pred_grades[i] == label_grades[i]:
                msg = "\n".join(
                    [
                        "========================",
                        f"Sample {i}:",
                        f"Predict\t: {pred_text[i]} (Grade: {pred_grades[i]})",
                        f"Target\t: {labels_text[i]} (Grade: {label_grades[i]})",
                        "========================",
                    ]
                )
                logger.info(msg)

    # äºˆæ¸¬ã«å¤±æ•—ã—ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ã‚’ã„ãã¤ã‹è¡¨ç¤º
    if show_samples > 0:
        logger.info("âŒ Visualize failed predictions")
        for i in range(len(pred_text)):
            if pred_grades[i] != label_grades[i]:
                msg = "\n".join(
                    [
                        "========================",
                        f"Sample {i}:",
                        f"Predict\t: {pred_text[i]} (Grade: {pred_grades[i]})",
                        f"Target\t: {labels_text[i]} (Grade: {label_grades[i]})",
                        "========================",
                    ]
                )
                logger.info(msg)

    return {
        "bleu": round(bleu, 4),
        "moverscore": round(moverscore, 4),
        "accuracy": round(accuracy, 4),
        "confusion_matrix": cf
    }


def custom_compute_metrics(res: EvalPrediction) -> Dict:

    return {}


# DummyFile: ä½•ã‚‚æ›¸ãè¾¼ã¾ãªã„ãƒ€ãƒŸãƒ¼ã‚¯ãƒ©ã‚¹
class DummyFile:
    def write(self, x):
        pass  # ä½•ã‚‚ã—ãªã„

    def flush(self):
        pass  # ä½•ã‚‚ã—ãªã„


def main():
    # ? loggerè¨­å®š
    print("set logger")
    logger = set_logger(level=DEBUG)

    acc = Accelerator()
    if not acc.is_main_process:
        logger.setLevel(CRITICAL)
        # ãƒ©ãƒ³ã‚¯0ä»¥å¤–ã®ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã€sys.stdoutã‚’DummyFileã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
        sys.stdout = DummyFile()
        # sys.stderr = DummyFile()
    logger.info(f"logger set complite")

    # ================================================================
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—
    # ================================================================

    parser = argparse.ArgumentParser(
        description="Fine-tune LLama with LoRA on Reflection dataset"
    )

    # ãƒ¢ãƒ‡ãƒ«ã‚„å‡ºåŠ›ã«é–¢ã™ã‚‹è¨­å®š
    parser.add_argument(
        "--base_model",
        type=str,
        default="elyza/Llama-3-ELYZA-JP-8B",
        help="Base model ID",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./outputs/lora-elyza-reflection",
        help="Directory to save LoRA-tuned checkpoints",
    )

    # LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument("--lora_r", type=int, default=8, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=16, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.1, help="LoRA dropout")

    # ãƒ‡ãƒ¼ã‚¿ãƒ»å­¦ç¿’è¨­å®š
    parser.add_argument("--max_words", type=int, default=4096, help="data max_words")
    parser.add_argument(
        "--epochs", type=int, default=3, help="Number of training epochs"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1,
        help="Global batch size (across all devices)",
    )
    parser.add_argument(
        "--micro_batch_size", type=int, default=1, help="Batch size per device"
    )

    # ãƒ­ã‚°å‡ºåŠ›è¨­å®š
    parser.add_argument(
        "--report_to",
        type=str,
        default="none",
        choices=["wandb", "tensorboard", "none"],
        help="Reporting backend for logging",
    )
    parser.add_argument(
        "--run_name",
        type=str,
        default="lora-elyza-reflection_test",
        help="Run name for experiment tracking",
    )
    parser.add_argument(
        "--logfile",
        type=str,
        default="NA",
        help="File name for logging (default: NA, no file logging)",
    )

    args = parser.parse_args()
    set_seed(42)

    # ================================================================
    # DDP (Distributed Data Parallel) ã®è¨­å®š
    # ================================================================

    world_size = int(os.environ.get("WORLD_SIZE", 1))
    # gradient_accumulation_steps = args.batch_size // args.micro_batch_size
    # if ddp := world_size != 1:
    #     gradient_accumulation_steps = gradient_accumulation_steps // world_size
    if args.batch_size % args.micro_batch_size != 0:
        raise ValueError(
            "global_batch_size must be divisible by per_device_batch_size Ã— WORLD_SIZE"
        )

    ddp = world_size != 1
    gradient_accumulation_steps = max(
        1, args.batch_size // args.micro_batch_size // world_size
    )

    if ddp:
        logger.info(f"DDP is enabled (ddp = {ddp}, world_size = {world_size})")
    else:
        logger.info(f"DDP is disabled (ddp = {ddp}, world_size = {world_size})")

    # ================================================================
    # ãƒ¢ãƒ‡ãƒ«ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰
    # ================================================================

    model, tokenizer = load_model(args.base_model, if_ZeRO=True)
    summary(model)

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€
    dataset_path = "./data/"

    all_extend = False  # å…¨è¡Œå±•é–‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
    if all_extend:
        # å…¨è¡Œå±•é–‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³
        train_dataset = GradePredictionDataset(
            dataset_path=dataset_path,
            concatenate=False,
            mode="train",
            division=True,  # å…¨è¡Œå±•é–‹
            # add_extended=True,  #? è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡
        )
        eval_dataset = GradePredictionDataset(
            dataset_path=dataset_path,
            concatenate=False,
            mode="valid",
            division=True,  # å…¨è¡Œå±•é–‹
        )
    else:
        train_dataset = GradePredictionDataset(
            dataset_path=dataset_path,
            question_filter=[1],
            concatenate=True,
            mode="train",
            )
        eval_dataset = GradePredictionDataset(
            dataset_path=dataset_path,
            question_filter=[1],
            concatenate=True,
            mode="valid",
        )

    logger.info(f"len(train_dataset): {len(train_dataset)}")
    logger.info(f"len(eval_dataset): {len(eval_dataset)}")

    train_logger = set_logger(name="CollateTrain", level=INFO)
    eval_logger = set_logger(name="CollateEval", level=INFO)

    if all_extend:
        logger.info("Using all-extended version of the dataset (all_extend=True).")
        train_collator = GradePredictionCollator(
            tokenizer,
            max_tokens=args.max_words,
            include_target=True,
            logger=train_logger,
        )
        eval_collator = GradePredictionCollator(
            tokenizer,
            max_tokens=args.max_words,
            include_target=False,
            logger=eval_logger,
        )
    else:
        logger.info("Using standard version of the dataset (all_extend=False).")
        train_collator = GradePredictionCollator(
            tokenizer,
            max_tokens=args.max_words,
            include_target=True,
            logger=train_logger,
            question_filter=[1],
        )
        eval_collator = GradePredictionCollator(
            tokenizer,
            max_tokens=args.max_words,
            include_target=False,
            logger=eval_logger,
            question_filter=[1],
        )

    logger.info(
        f"custom_collate_fn initialized with processor: {type(tokenizer).__name__}"
    )

    # collatorã®å‹•ä½œã‚’ç¢ºèª
    example_loader = DataLoader(
        train_dataset, collate_fn=train_collator, batch_size=4, shuffle=True
    )
    batch = next(iter(example_loader))
    logger.info(
        f"Batch keys: {batch.keys()}"
    )  # -> Batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])
    for k, v in batch.items():
        logger.info(
            f"Batch key: {k}, value shape: {v.shape if isinstance(v, torch.Tensor) else type(v)}"
        )
    # batchå†…ã®input_idsã‚’ã§ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç¢ºèª
    input_text = tokenizer.batch_decode(batch["input_ids"], skip_special_tokens=True)

    logger.info(f"Input prompt: \n{input_text}")
    logger.debug(f"batch:\n{batch}")

    # ================================================================
    # è¨“ç·´å‰æ¨è«–
    # !è¨“ç·´å‰ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã¨ï¼Œè¨“ç·´ã§ãã„ãƒã‚°ãŒç™ºç”Ÿ
    # -> è¨“ç·´ã®è¨­å®šå‰ã«æ¨è«–ã‚’è¡Œã„ï¼Œãƒ¢ãƒ‡ãƒ«ã‚’å†åº¦ãƒ­ãƒ¼ãƒ‰ã—ç›´ã™ã“ã¨ã§å›é¿
    # ================================================================
    msg = "\n".join(
        [
            "=================================================",
            "=================================================",
            "ğŸ”„ Start Evaluation before training...",
        ]
    )
    logger.info(msg)

    pred_text = []
    input_text = []
    label_text = []
    eval_loader = DataLoader(
        eval_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=eval_collator,
    )

    model, eval_loader = acc.prepare(model, eval_loader)  # DDPå¯¾å¿œåŒ–

    # if acc.is_main_process:  # rank 0 ã ã‘è¡¨ç¤º
    #     st = acc.state
    #     acc.print(
    #         f"Accelerate initialized â‡’ "
    #         f"world_size={st.num_processes}, "
    #         f"local_rank={st.local_process_index}, "
    #         f"device={st.device}"
    #     )
    model.eval()  # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š

    # modelã®deviceã‚’è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    for batch in eval_loader:
        batch = {k: v.to(model.device) for k, v in batch.items()}
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]

        with torch.no_grad():
            # ç”Ÿæˆ
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=128,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

        decoded_inputs = tokenizer.batch_decode(input_ids, skip_special_tokens=True)
        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)

        target_ids = batch["target_ids"]
        # decode
        tgt_str = tokenizer.batch_decode(target_ids, skip_special_tokens=True)

        input_text.extend(decoded_inputs)
        pred_text.extend(decoded_outputs)
        label_text.extend(tgt_str)

    # gather_for_metrics ã§å…¨ rank åˆ†ã‚’ rank0 ã«é›†ç´„
    input_text = acc.gather_for_metrics(input_text)
    pred_text = acc.gather_for_metrics(pred_text)
    label_text = acc.gather_for_metrics(label_text)

    # å…ˆé ­åˆ‡ã‚Šæ¨ã¦
    input_text = input_text[0 : len(eval_dataset)]
    pred_text = pred_text[0 : len(eval_dataset)]
    label_text = label_text[0 : len(eval_dataset)]

    pred_result = {
        "input_sentence": input_text,
        "output_sentence": pred_text,
        "label_sentence": label_text,
    }

    metrics = evaluate(
        pred_result, eval_dataset, tokenizer, show_samples=5, logger=logger
    )
    logger.info("âœ… Evaluation before training completed successfully!")
    logger.info(
        f"Metrics\t:\nMoverScore\t: {metrics['moverscore']}\nAccuracy\t: {metrics['accuracy']}\n"
    )
    msg = "\n".join(
        [
            "=================================================",
            "confusion_matrix:",
            f"{metrics['confusion_matrix']}",
            "=================================================",
        ]
    )
    logger.info(msg)
    logger.info(
        "\n====================================\n===================================="
    )

    # ãƒ¢ãƒ‡ãƒ«ã®å‰Šé™¤ï¼ˆä¸€æ™‚ä¿ç•™ï¼‰
    del model, tokenizer, eval_loader
    torch.cuda.empty_cache()  # GPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
    logger.info("ğŸ”„ Reloading model and tokenizer...")

    # å†åº¦ãƒ­ãƒ¼ãƒ‰
    model, tokenizer = load_model(args.base_model, if_ZeRO=True)

    logger.info("âœ… Model and tokenizer reloaded successfully!")

    # ================================================================
    # è¨“ç·´ç”¨ã®è¨­å®š
    # ================================================================

    # â‘  LoRAè¨­å®š
    peft_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",  # Self-Attentionç³»
            "gate_proj",
            "up_proj",
            "down_proj",  # MLPï¼ˆFFNï¼‰ç³»
        ],
    )

    # LoRAé©ç”¨
    model.enable_input_require_grads()  #! è¿½åŠ ::å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã«å‹¾é…ã‚’æµã›ã‚‹çŠ¶æ…‹ã‚’å¼·åˆ¶ã™ã‚‹å®‰å…¨ã‚¹ã‚¤ãƒƒãƒ
    model = get_peft_model(model, peft_config)

    logger.info(f"Trainable parameters:")
    model.print_trainable_parameters()
    logger.info("âœ… LoRA has been successfully applied to the model.")
    logger.debug(summary(model))

    # full_finetune_modules ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†åº¦ trainable ã«ã™ã‚‹
    full_finetune_modules = ["embed_tokens", "lm_head"]

    logger.info("ğŸ”§ Applying LoRA and enabling full finetune modules...")

    # LoRA é©ç”¨æ¸ˆã¿ï¼ˆå‰æ®µï¼‰ -> ZeRO 3 ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ã‚ã‚Š
    # for name, param in model.named_parameters():
    #     if any(module_name in name for module_name in full_finetune_modules):
    #         param.requires_grad = False
    #         param.data = param.data.to(torch.float16)

    logger.info("âœ… LoRA has been applied.")
    logger.info(
        f"âœ… The following modules are fully finetuned: {', '.join(full_finetune_modules)}"
    )

    model.gradient_checkpointing_enable()
    # if local_rank == 0:
    #     model.logger.info_trainable_parameters()
    #     summary(model, depth=2)

    logger.debug("Trainable parameters:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            logger.debug(f" - {name}: {param.shape}, dtype: {param.dtype}")

    logger.info("=" * 100)
    logger.info("=" * 100)

    # â‘¡TrainingArguments
    training_args = TrainingArguments(
        gradient_checkpointing=True,
        output_dir=args.output_dir,
        per_device_train_batch_size=args.micro_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,  # args.grad_accum,
        num_train_epochs=args.epochs,
        warmup_ratio=0.03,
        logging_dir="./logs",
        logging_steps=50,
        lr_scheduler_type="cosine",
        optim="adamw_torch",
        save_strategy="epoch",
        eval_strategy="epoch",
        fp16=True,
        fp16_full_eval=True,
        per_device_eval_batch_size=1,
        eval_accumulation_steps=1,
        remove_unused_columns=False,
        run_name=args.run_name,
        save_total_limit=args.epochs,
        ddp_find_unused_parameters=False,
        load_best_model_at_end=False,
        label_names=["labels"],  # PEFTç’°å¢ƒä¸‹ã§ã¯æ˜ç¤ºã—ãŸã»ã†ãŒè‰¯ã„ã‚‰ã—ã„ï¼Ÿ
        learning_rate=2e-6,
    )

    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        data_collator=train_collator,
        # compute_metrics=custom_compute_metrics,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        # predict_with_generate=True,  # æ¨è«–æ™‚ã« generate ã‚’ä½¿ç”¨
    )

    logger.info("Trainer instance has been created.")
    logger.info(
        f"Trainer is set with model: {type(model).__name__}, train dataset size: {len(train_dataset)}, eval dataset size: {len(eval_dataset)}"
    )

    # ================================================================
    # è¨“ç·´
    # ================================================================
    logger.info("=================================================")
    logger.info("=================================================")

    # trainer.accelerator.end_training()  # â˜… inference Accelerator ã‚’é–‰ã˜ã‚‹
    # trainer._created_accelerator = False  # â˜… â€œä½œæˆæ¸ˆã¿â€ ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
    model.train()
    logger.info("ğŸ”„ Start training...")
    trainer.data_collator = train_collator
    trainer.train()
    logger.info("âœ… Model training has been completed successfully!")

    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    file_path = args.output_dir + args.logfile
    if args.logfile != "NA":
        logger.info(f"Saving model to {file_path}")
        model.save_pretrained(file_path)
        tokenizer.save_pretrained(file_path)
    else:
        logger.info("No logfile specified, skipping model save.")

    # ================================================================
    # è¨“ç·´å¾Œæ¨è«–
    # ================================================================
    logger.info("=================================================")
    logger.info("=================================================")

    logger.info("ğŸ”„ Start Evaluation after training...")
    trainer.data_collator = eval_collator

    # ---
    model.eval()
    pred_text = []
    input_text = []
    label_text = []
    loader = DataLoader(
        eval_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=eval_collator,
    )
    for batch in loader:
        batch = {k: v.to(model.device) for k, v in batch.items()}
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]

        with torch.no_grad():
            # ç”Ÿæˆ
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=128,
                do_sample=True,  # æ¨è«–æ™‚ã¯é€šå¸¸Greedy
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

        decoded_inputs = tokenizer.batch_decode(input_ids, skip_special_tokens=True)
        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        target_ids = batch["target_ids"]
        # decode
        tgt_str = tokenizer.batch_decode(target_ids, skip_special_tokens=True)

        input_text.extend(decoded_inputs)
        pred_text.extend(decoded_outputs)
        label_text.extend(tgt_str)

    # gc = GenerationConfig(
    #     max_new_tokens=64,
    #     do_sample=False,  # è©•ä¾¡ã§ã¯é€šå¸¸ OFF
    #     top_p=0.9,  # å¿…è¦ã«å¿œã˜ã¦
    #     temperature=0.7,
    #     pad_token_id=tokenizer.pad_token_id,
    #     eos_token_id=tokenizer.eos_token_id,
    #     # åˆ†æ•£æ¨è«–ã®é•·ã•ãšã‚Œé˜²æ­¢
    #     # synced_gpus=True,     # transformers>=4.34 ãªã‚‰æœ‰åŠ¹
    # )

    # trainer.data_collator = eval_collator
    # pred_output = trainer.predict(
    #     eval_dataset, generation_config=gc, predict_with_generate=True
    # )  # DeepSpeed + ZeRO3 å¯¾å¿œæ¸ˆã¿

    # # ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’å®‰å…¨ã«æ•´å½¢
    # pred_ids = pred_output.predictions.tolist()

    # clean_pred_ids = []
    # for seq in pred_ids:
    #     if gc.eos_token_id in seq:
    #         seq = seq[: seq.index(gc.eos_token_id)]
    #     clean_pred_ids.append([tok for tok in seq if tok != gc.pad_token_id])

    # pred_text = tokenizer.batch_decode(
    #     clean_pred_ids,
    #     skip_special_tokens=True,
    #     clean_up_tokenization_spaces=True,  # â˜…
    # )

    # # eval_datasetã‹ã‚‰å…¥åŠ›æ–‡ã‚’æŠ½å‡º
    # input_ids_list = [
    #     example["input_ids"] for example in eval_dataset
    # ]  # list[list[int]]
    # input_text = tokenizer.batch_decode(
    #     input_ids_list,
    #     skip_special_tokens=True,
    # )

    # del pred_output, pred_ids
    # torch.cuda.empty_cache()

    # çµæœã‚’æ ¼ç´ï¼ˆTrainer.predictã®å½¢å¼ã«å¯„ã›ãŸã„å ´åˆï¼‰
    pred_result = {
        "input_sentence": input_text,
        "output_sentence": pred_text,
        "label_sentence": label_text,
    }

    # logger.debug(f"pred_result elements\t:{pred_result._asdict().keys()}")
    # pred_text = tokenizer.batch_decode(
    #     pred_result.predictions, skip_special_tokens=True
    # )
    # labels_text = tokenizer.batch_decode(
    #     pred_result.label_ids, skip_special_tokens=True
    # )

    # logger.info("âœ…ï¸ Visualize sample answers")
    # for i in range(5):
    #     msg = "\n".join(
    #         [
    #             "========================",
    #             f"Sample {i}:",
    #             f"Predict\t: {pred_text[i]}",
    #             f"Target\t: {label_text[i]}",
    #             "========================",
    #         ]
    #     )
    #     logger.info(msg)

    metrics = evaluate(
        pred_result, eval_dataset, tokenizer, show_samples=5, logger=logger
    )
    logger.info("âœ… Evaluation after training completed successfully!")
    logger.info(
        f"Metrics\t:\nMoverScore\t: {metrics['moverscore']}\nAccuracy\t: {metrics['accuracy']}\n"
    )
    msg = "\n".join(
        [
            "=================================================",
            "confusion_matrix:",
            f"{metrics['confusion_matrix']}",
            "=================================================",
        ]
    )
    logger.info(msg)


if __name__ == "__main__":
    main()
