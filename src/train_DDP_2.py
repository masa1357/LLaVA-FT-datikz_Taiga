



# -------- environment setting --------
load_dotenv()

os.environ["TORCH_DISTRIBUTED_DEBUG"] = "DETAIL"
os.environ["TORCH_ENABLE_DISTRIBUTED"] = "1"
os.environ["TORCH_DTENSOR_SKIP_CHECK"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

BYTES_PER_PARAM = {
    torch.float32: 4,
    torch.float16: 2,
    torch.bfloat16: 2,
    torch.int8: 1,
}

def train():
    pass

def evaluate():
    pass

def custom_compute_metrics(res: EvalPrediction) -> Dict:
    pred = res.predictions.argmax(axis=1)
    target = res.label_ids
    pass


def main():
    # torchã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèª
    print(f"[info] torch version: {torch.__version__}")
    print(f"[info] torch cuda version: {torch.version.cuda}")

    #================================================================
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—
    #================================================================

    parser = argparse.ArgumentParser(
        description="Fine-tune LLama with LoRA on Reflection dataset"
    )

    # ãƒ¢ãƒ‡ãƒ«ã‚„å‡ºåŠ›ã«é–¢ã™ã‚‹è¨­å®š
    parser.add_argument(
        "--base_model",
        type=str,
        default="elyza/Llama-3-ELYZA-JP-8B",
        help="Base model ID",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./outputs/lora-elyza-reflection",
        help="Directory to save LoRA-tuned checkpoints",
    )

    # LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument("--lora_r", type=int, default=8, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=16, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.1, help="LoRA dropout")

    # ãƒ‡ãƒ¼ã‚¿ãƒ»å­¦ç¿’è¨­å®š
    parser.add_argument("--max_words", type=int, default=4096, help="data max_words")
    parser.add_argument(
        "--epochs", type=int, default=3, help="Number of training epochs"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1,
        help="Global batch size (across all devices)",
    )
    parser.add_argument(
        "--micro_batch_size", type=int, default=1, help="Batch size per device"
    )

    # ãƒ­ã‚°å‡ºåŠ›è¨­å®š
    parser.add_argument(
        "--report_to",
        type=str,
        default="none",
        choices=["wandb", "tensorboard", "none"],
        help="Reporting backend for logging",
    )
    parser.add_argument(
        "--run_name",
        type=str,
        default="lora-elyza-reflection_test",
        help="Run name for experiment tracking",
    )

    args = parser.parse_args()

    # Set random seed for reproducibility
    set_seed(42)

    #================================================================
    # ãƒ¢ãƒ‡ãƒ«ï¼Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ­ãƒ¼ãƒ‰
    #================================================================

    model, tokenizer = load_model(args.base_model, if_ZeRO=True)
    summary(model)

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€
    dataset_path = "./data/"

    train_dataset = GradePredictionDataset(
        dataset_path=dataset_path,
        question_filter=[1],
        concatenate=True,
        mode="train",
        testcase=True,
    )
    eval_dataset = GradePredictionDataset(
        dataset_path=dataset_path,
        question_filter=[1],
        concatenate=True,
        mode="valid",
        testcase=True,
    )

    print("[info] len(train_dataset):", len(train_dataset))
    print("[info] len(eval_dataset):", len(eval_dataset))

    # ç‹¬è‡ªcollatorã®å®šç¾©
    custom_collate_fn = partial(
        collate_fn, tokenizer=tokenizer, max_tokens=args.max_words, testcase=True, question_filter=[1]
    )
    print(
        f"[info] custom_collate_fn initialized with processor: {type(tokenizer).__name__}"
    )

    # collatorã®å‹•ä½œã‚’ç¢ºèª
    example_loader = DataLoader(train_dataset, collate_fn=custom_collate_fn, batch_size=4, shuffle=True)
    batch = next(iter(loader))
    for k,v in batch.items():
        print(k, v.shape)
    print(batch)


    #================================================================
    # è¨“ç·´ç”¨ã®è¨­å®š
    #================================================================

    # â‘  LoRAè¨­å®š
    peft_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=[
            # "q_proj",
            # "k_proj",
            # "v_proj",
            # "o_proj",       # Self-Attentionç³»
            "gate_proj",
            "up_proj",
            "down_proj",  # MLPï¼ˆFFNï¼‰ç³»
        ],
    )

    # LoRAé©ç”¨
    model.enable_input_require_grads()  #! è¿½åŠ ::å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã«å‹¾é…ã‚’æµã›ã‚‹çŠ¶æ…‹ã‚’å¼·åˆ¶ã™ã‚‹å®‰å…¨ã‚¹ã‚¤ãƒƒãƒ
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    print("âœ… LoRA has been successfully applied to the model.")

    # full_finetune_modules ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†åº¦ trainable ã«ã™ã‚‹
    full_finetune_modules = ["embed_tokens", "lm_head"]

    print("ğŸ”§ Applying LoRA and enabling full finetune modules...")

    # LoRA é©ç”¨æ¸ˆã¿ï¼ˆå‰æ®µï¼‰
    for name, param in model.named_parameters():
        if any(module_name in name for module_name in full_finetune_modules):
            param.requires_grad = False  #! True -> å¤‰æ›´
            # param.data = param.data.to(torch.float32)
            param.data = param.data.to(torch.float16)

    print("âœ… LoRA has been applied.")
    print(
        f"âœ… The following modules are fully finetuned: {', '.join(full_finetune_modules)}"
    )

    model.gradient_checkpointing_enable()  #! è¿½åŠ ::model.gradient_checkpointing_enable() ã‚’ LoRA é©ç”¨å¾Œã«å‘¼ã¶ã¨ 20â€“30 % è¿½åŠ ç¯€ç´„
    # if local_rank == 0:
    #     model.print_trainable_parameters()
    #     summary(model, depth=2)

    print("Trainable parameters:")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(name, param.shape, param.dtype)

    print("=" * 100)
    print("=" * 100)

    # â‘¡TrainingArguments
    training_args = TrainingArguments(
        gradient_checkpointing      = True,
        output_dir                  = args.output_dir,
        per_device_train_batch_size = args.micro_batch_size,
        gradient_accumulation_steps = gradient_accumulation_steps,  # args.grad_accum,
        num_train_epochs            = args.epochs,
        warmup_ratio                = 0.03,
        logging_dir                 = "./logs",
        logging_steps               = 50,
        lr_scheduler_type           = "cosine",
        optim                       = "adamw_torch",
        save_strategy               = "epoch",
        eval_strategy               = "epoch",
        fp16                        = True,
        fp16_full_eval              = True,
        per_device_eval_batch_size  = 1,
        eval_accumulation_steps     = 1,
        remove_unused_columns       = False,
        run_name                    = args.run_name,
        save_total_limit            = args.epochs,
        ddp_find_unused_parameters  = False,
        load_best_model_at_end      = False,
        )


    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        data_collator=custom_collate_fn,
        compute_metrics=custom_compute_metrics,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )




if __name__ == "__main__":
    main()
