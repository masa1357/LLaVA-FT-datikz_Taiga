import os
from transformers import LlavaForConditionalGeneration, AutoProcessor
import torch
from peft import PeftModel
from transformers import PreTrainedModel

#def load_model(base_model: str, use_fast: bool = True, dtype=torch.float16, device_map="auto"):
#    """
#    Parameters:
#        base_model (str): ãƒ¢ãƒ‡ãƒ«å or ãƒ‘ã‚¹
#        use_fast (bool): Fast image processor ã‚’ä½¿ã†ã‹ã©ã†ã‹
#        dtype (torch.dtype): ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®ãƒ‡ãƒ¼ã‚¿å‹ï¼ˆä¾‹ï¼štorch.float16ï¼‰
#        device_map (str or dict): ãƒ‡ãƒã‚¤ã‚¹ã®å‰²ã‚Šå½“ã¦æ–¹å¼ï¼ˆä¾‹ï¼š"auto"ï¼‰
#
#    Returns:
#        model (LlavaForConditionalGeneration): èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
#        processor (AutoProcessor): èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ—ãƒ­ã‚»ãƒƒã‚µ
#    """
#    print(f"[info] Loading model from '{base_model}'...")
#    model = LlavaForConditionalGeneration.from_pretrained(
#        base_model,
#        torch_dtype=dtype,
#        device_map=device_map
#    )
#
#    print(f"[info] Loading processor (use_fast={use_fast})...")
#    processor = AutoProcessor.from_pretrained(base_model, use_fast=use_fast)
#
#    print("âœ… Model and processor loaded successfully!")
#    return model, processor

def load_model(base_model: str, use_fast: bool = True, dtype = torch.float16):
    """
    ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’èª­ã¿è¾¼ã‚€ï¼ˆDDPå¯¾å¿œï¼‰

    Parameters:
        base_model (str): ãƒ¢ãƒ‡ãƒ«åã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹
        use_fast (bool): Fast tokenizer/image processor ã‚’ä½¿ã†ã‹ã©ã†ã‹
        dtype: ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®ãƒ‡ãƒ¼ã‚¿å‹ï¼ˆä¾‹ï¼štorch.float16ï¼‰

    Returns:
        model (LlavaForConditionalGeneration): èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
        processor (AutoProcessor): èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ—ãƒ­ã‚»ãƒƒã‚µ
    """
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    device = f"cuda:{local_rank}"

    print(f"[info] Loading model from '{base_model}' to device {device}...")
    model = LlavaForConditionalGeneration.from_pretrained(
        base_model,
        torch_dtype=dtype,
        device_map={"": device}
    )

    print(f"[info] Loading processor (use_fast={use_fast})...")
    processor = AutoProcessor.from_pretrained(base_model, use_fast=use_fast)

    print("âœ… Model and processor loaded successfully!")
    return model, processor


def merge_and_save(base_model_name, lora_checkpoint, save_path):
    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨Processorã®èª­ã¿è¾¼ã¿
    base_model, processor = load_model(base_model_name)

    # LoRAé‡ã¿ã®èª­ã¿è¾¼ã¿ã¨ãƒãƒ¼ã‚¸
    print("ğŸ”„ Loading and merging LoRA into base model...")
    model = PeftModel.from_pretrained(base_model, lora_checkpoint)
    model = model.merge_and_unload()

    # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(save_path, exist_ok=True)

    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µã®ä¿å­˜
    print("ğŸ’¾ Saving merged model and processor to:", save_path)
    model.save_pretrained(save_path, safe_serialization=True)
    processor.save_pretrained(save_path)
    print("âœ… Merged model and processor saved successfully!")

#def merge_lora_and_save(model, processor, save_dir):
#    if isinstance(model, PeftModel):
#        print("ğŸ”„ Merging LoRA weights into base model...")
#        model = model.merge_and_unload()  # LoRAã‚’ãƒãƒ¼ã‚¸ã—ã¦æˆ»ã™
#        base_model = model.base_model#.model  # å®Ÿä½“ã‚’å–ã‚Šå‡ºã™
#    else:
#        base_model = model
#
#    os.makedirs(save_dir, exist_ok=True)
#    print(f"âœ… Saving full model to: {save_dir}")
#    base_model.save_pretrained(save_dir, safe_serialization=True)
#    processor.save_pretrained(save_dir)
#    print("âœ… Full model and processor saved.")

#def merge_lora_and_save(model, processor, save_dir):
#    # DDPã§å‹•ä½œã—ã¦ã„ã‚‹å ´åˆã€rank 0 ã®ã¿ã§å®Ÿè¡Œ
#    if torch.distributed.is_initialized() and torch.distributed.get_rank() != 0:
#        return
#
#    if isinstance(model, PeftModel):
#        print("ğŸ”„ Merging LoRA weights into base model...")
#
#        model = model.to("cuda")
#        model.eval()
#
#        # LoRAã®ãƒãƒ¼ã‚¸
#        model = model.merge_and_unload()
#
#        # å…ƒã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
#        base_model = model.base_model.model if hasattr(model.base_model, "model") else model.base_model
#    else:
#        base_model = model
#
#    # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
#    os.makedirs(save_dir, exist_ok=True)
#    print(f"ğŸ’¾ Saving full model to: {save_dir}")
#
#    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ä¿å­˜
#    base_model.save_pretrained(save_dir, safe_serialization=True)
#    processor.save_pretrained(save_dir)
#
#    print("âœ… Full model and processor saved.")


def merge_lora_and_save(model, processor, save_dir):
    # DDPã§å‹•ä½œã—ã¦ã„ã‚‹å ´åˆã€rank 0 ã®ã¿ã§å®Ÿè¡Œ
    if torch.distributed.is_initialized() and torch.distributed.get_rank() != 0:
        return

    if isinstance(model, PeftModel):
        print("ğŸ”„ Merging LoRA weights into base model...")

        model = model.to("cuda")
        model.eval()

        model = model.merge_and_unload()
        base_model = model.base_model.model if hasattr(model.base_model, "model") else model.base_model
    else:
        base_model = model
    
    print(f"[info] merge_lora_and_save  >> base_model type: {type(base_model)}")

    os.makedirs(save_dir, exist_ok=True)
    print(f"ğŸ’¾ Saving full model to: {save_dir}")

    # --- ãƒ¢ãƒ‡ãƒ«ä¿å­˜ ---
    if isinstance(base_model, PreTrainedModel):
        base_model.save_pretrained(save_dir, safe_serialization=True)
    else:
        torch.save(base_model.state_dict(), os.path.join(save_dir, "pytorch_model.bin"))

    # --- Processorä¿å­˜ ---
    processor.save_pretrained(save_dir)

    print("âœ… Full model and processor saved.")


def generate_captions(model, processor, image, device, max_new_tokens=128):
    chat = [{
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": "Please describe this image."}
        ]
    }]

    inputs = processor.apply_chat_template(
        chat,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        padding=True,
        return_tensors="pt"
    ).to(model.device, torch.float16)
    #inputs["images"] = images.to(device, dtype=torch.float16)

    #for k in ["input_ids", "attention_mask"]:
    #    inputs[k] = inputs[k].to(device)

    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)
    return processor.tokenizer.batch_decode(outputs, skip_special_tokens=True)